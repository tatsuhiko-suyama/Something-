{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNG2ACBP0bSLL71Msyulqi4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatsuhiko-suyama/Something-/blob/main/4_16_v2_%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%82%B5%E3%82%A4%E3%82%BA%EF%BC%93%E4%BB%A5%E4%B8%8A%E3%82%92%E6%8E%A2%E3%81%99%E6%97%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9BEQR5ru7w_",
        "outputId": "4cc34238-cc7f-4d80-fa25-53535201f5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "--- Starting Full Range Analysis [0.0, 1.5] (using ActiveSet, mu_tilde=0.02) ---\n",
            "--- (CSV will be saved for alpha approx every 0.1) ---\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Starting Full Results Analysis over Alpha Range (using Active Set Method) ---\n",
            "Analyzing alpha = 1.500000 (151/151) ... \n",
            "--- Finished Full Results Analysis ---\n",
            "\n",
            "--- Analysis Results Summary (DataFrame - First 5 & Last 5 rows) ---\n",
            "         alpha     H_star active_set     w_fw_0     w_fw_1     w_fw_2       pi_0       pi_1        pi_2       pi_3       pi_4   grad_H_0   grad_H_1   grad_H_2   lambda_0   lambda_1   lambda_2\n",
            "0   0.0000e+00 1.0393e-03       (1,) 8.7835e-06 9.9998e-01 8.7835e-06 8.1006e-02 1.0352e-01 -4.9906e-02 1.0154e-01 1.2416e-01 5.6578e-04 6.3926e-04 5.9951e-04 0.0000e+00 1.0393e-01 0.0000e+00\n",
            "1   1.0000e-02 1.0370e-03       (1,) 8.9124e-06 9.9998e-01 8.9124e-06 8.0927e-02 1.0343e-01 -4.9868e-02 1.0145e-01 1.2409e-01 5.6427e-04 6.3697e-04 5.9806e-04 0.0000e+00 1.0370e-01 0.0000e+00\n",
            "2   2.0000e-02 1.0347e-03       (1,) 9.0442e-06 9.9998e-01 9.0442e-06 8.0847e-02 1.0333e-01 -4.9830e-02 1.0137e-01 1.2402e-01 5.6276e-04 6.3468e-04 5.9661e-04 0.0000e+00 1.0347e-01 0.0000e+00\n",
            "3   3.0000e-02 1.0324e-03       (1,) 9.1789e-06 9.9998e-01 9.1789e-06 8.0768e-02 1.0324e-01 -4.9792e-02 1.0129e-01 1.2395e-01 5.6126e-04 6.3239e-04 5.9516e-04 0.0000e+00 1.0324e-01 0.0000e+00\n",
            "4   4.0000e-02 1.0301e-03       (1,) 9.3167e-06 9.9998e-01 9.3167e-06 8.0689e-02 1.0314e-01 -4.9754e-02 1.0121e-01 1.2388e-01 5.5976e-04 6.3012e-04 5.9372e-04 0.0000e+00 1.0301e-01 0.0000e+00\n",
            "5   5.0000e-02 1.0278e-03       (1,) 9.4576e-06 9.9998e-01 9.4576e-06 8.0610e-02 1.0305e-01 -4.9717e-02 1.0113e-01 1.2382e-01 5.5827e-04 6.2784e-04 5.9228e-04 0.0000e+00 1.0278e-01 0.0000e+00\n",
            "6   6.0000e-02 1.0256e-03       (1,) 9.6017e-06 9.9998e-01 9.6017e-06 8.0531e-02 1.0295e-01 -4.9679e-02 1.0105e-01 1.2375e-01 5.5678e-04 6.2558e-04 5.9085e-04 0.0000e+00 1.0256e-01 0.0000e+00\n",
            "7   7.0000e-02 1.0233e-03       (1,) 9.7492e-06 9.9998e-01 9.7492e-06 8.0453e-02 1.0286e-01 -4.9641e-02 1.0097e-01 1.2368e-01 5.5530e-04 6.2332e-04 5.8942e-04 0.0000e+00 1.0233e-01 0.0000e+00\n",
            "8   8.0000e-02 1.0211e-03       (1,) 9.9000e-06 9.9998e-01 9.9000e-06 8.0374e-02 1.0277e-01 -4.9604e-02 1.0089e-01 1.2361e-01 5.5382e-04 6.2107e-04 5.8799e-04 0.0000e+00 1.0211e-01 0.0000e+00\n",
            "9   9.0000e-02 1.0188e-03       (1,) 1.0054e-05 9.9998e-01 1.0054e-05 8.0296e-02 1.0267e-01 -4.9566e-02 1.0081e-01 1.2355e-01 5.5235e-04 6.1882e-04 5.8658e-04 0.0000e+00 1.0188e-01 0.0000e+00\n",
            "10  1.0000e-01 1.0166e-03       (1,) 1.0293e-05 9.9998e-01 1.0293e-05 8.0218e-02 1.0258e-01 -4.9529e-02 1.0073e-01 1.2348e-01 5.5088e-04 6.1658e-04 5.8516e-04 0.0000e+00 1.0166e-01 0.0000e+00\n",
            "11  1.1000e-01 1.0143e-03       (1,) 1.0457e-05 9.9998e-01 1.0457e-05 8.0140e-02 1.0249e-01 -4.9491e-02 1.0065e-01 1.2341e-01 5.4942e-04 6.1435e-04 5.8375e-04 0.0000e+00 1.0143e-01 0.0000e+00\n",
            "12  1.2000e-01 1.0121e-03       (1,) 1.0624e-05 9.9998e-01 1.0624e-05 8.0062e-02 1.0239e-01 -4.9454e-02 1.0057e-01 1.2334e-01 5.4797e-04 6.1212e-04 5.8234e-04 0.0000e+00 1.0121e-01 0.0000e+00\n",
            "13  1.3000e-01 1.0099e-03       (1,) 1.0796e-05 9.9998e-01 1.0796e-05 7.9984e-02 1.0230e-01 -4.9417e-02 1.0049e-01 1.2327e-01 5.4652e-04 6.0990e-04 5.8094e-04 0.0000e+00 1.0099e-01 0.0000e+00\n",
            "14  1.4000e-01 1.0077e-03       (1,) 1.0972e-05 9.9998e-01 1.0972e-05 7.9906e-02 1.0221e-01 -4.9379e-02 1.0041e-01 1.2321e-01 5.4507e-04 6.0768e-04 5.7954e-04 0.0000e+00 1.0077e-01 0.0000e+00\n",
            "15  1.5000e-01 1.0055e-03       (1,) 1.1152e-05 9.9998e-01 1.1152e-05 7.9829e-02 1.0211e-01 -4.9342e-02 1.0033e-01 1.2314e-01 5.4363e-04 6.0547e-04 5.7815e-04 0.0000e+00 1.0055e-01 0.0000e+00\n",
            "16  1.6000e-01 1.0033e-03       (1,) 1.1337e-05 9.9998e-01 1.1337e-05 7.9752e-02 1.0202e-01 -4.9305e-02 1.0025e-01 1.2307e-01 5.4219e-04 6.0327e-04 5.7676e-04 0.0000e+00 1.0033e-01 0.0000e+00\n",
            "17  1.7000e-01 1.0011e-03       (1,) 1.1623e-05 9.9998e-01 1.1623e-05 7.9675e-02 1.0193e-01 -4.9268e-02 1.0018e-01 1.2300e-01 5.4076e-04 6.0107e-04 5.7537e-04 0.0000e+00 1.0011e-01 0.0000e+00\n",
            "18  1.8000e-01 9.9888e-04       (1,) 1.1819e-05 9.9998e-01 1.1819e-05 7.9598e-02 1.0184e-01 -4.9231e-02 1.0010e-01 1.2294e-01 5.3934e-04 5.9888e-04 5.7399e-04 0.0000e+00 9.9888e-02 0.0000e+00\n",
            "19  1.9000e-01 9.9670e-04       (1,) 1.2021e-05 9.9998e-01 1.2021e-05 7.9521e-02 1.0174e-01 -4.9194e-02 1.0002e-01 1.2287e-01 5.3792e-04 5.9669e-04 5.7261e-04 0.0000e+00 9.9670e-02 0.0000e+00\n",
            "20  2.0000e-01 9.9452e-04       (1,) 1.2227e-05 9.9998e-01 1.2227e-05 7.9444e-02 1.0165e-01 -4.9157e-02 9.9939e-02 1.2280e-01 5.3650e-04 5.9452e-04 5.7124e-04 0.0000e+00 9.9452e-02 0.0000e+00\n",
            "21  2.1000e-01 9.9234e-04       (1,) 1.2440e-05 9.9998e-01 1.2440e-05 7.9367e-02 1.0156e-01 -4.9120e-02 9.9861e-02 1.2274e-01 5.3509e-04 5.9234e-04 5.6987e-04 0.0000e+00 9.9234e-02 0.0000e+00\n",
            "22  2.2000e-01 9.9017e-04       (1,) 1.2768e-05 9.9997e-01 1.2768e-05 7.9291e-02 1.0147e-01 -4.9083e-02 9.9782e-02 1.2267e-01 5.3369e-04 5.9017e-04 5.6851e-04 0.0000e+00 9.9017e-02 0.0000e+00\n",
            "23  2.3000e-01 9.8801e-04       (1,) 1.2995e-05 9.9997e-01 1.2995e-05 7.9215e-02 1.0138e-01 -4.9046e-02 9.9704e-02 1.2260e-01 5.3229e-04 5.8801e-04 5.6715e-04 0.0000e+00 9.8801e-02 0.0000e+00\n",
            "24  2.4000e-01 9.8586e-04       (1,) 1.3228e-05 9.9997e-01 1.3228e-05 7.9138e-02 1.0128e-01 -4.9010e-02 9.9626e-02 1.2253e-01 5.3089e-04 5.8586e-04 5.6579e-04 0.0000e+00 9.8586e-02 0.0000e+00\n",
            "25  2.5000e-01 9.8371e-04       (1,) 1.3588e-05 9.9997e-01 1.3588e-05 7.9062e-02 1.0119e-01 -4.8973e-02 9.9548e-02 1.2247e-01 5.2950e-04 5.8371e-04 5.6444e-04 0.0000e+00 9.8371e-02 0.0000e+00\n",
            "26  2.6000e-01 9.8156e-04       (1,) 1.3837e-05 9.9997e-01 1.3837e-05 7.8987e-02 1.0110e-01 -4.8936e-02 9.9470e-02 1.2240e-01 5.2812e-04 5.8156e-04 5.6309e-04 0.0000e+00 9.8156e-02 0.0000e+00\n",
            "27  2.7000e-01 9.7943e-04       (1,) 1.4093e-05 9.9997e-01 1.4093e-05 7.8911e-02 1.0101e-01 -4.8900e-02 9.9392e-02 1.2233e-01 5.2674e-04 5.7943e-04 5.6174e-04 0.0000e+00 9.7943e-02 0.0000e+00\n",
            "28  2.8000e-01 9.7729e-04       (1,) 1.4490e-05 9.9997e-01 1.4490e-05 7.8835e-02 1.0092e-01 -4.8863e-02 9.9314e-02 1.2227e-01 5.2536e-04 5.7729e-04 5.6040e-04 0.0000e+00 9.7729e-02 0.0000e+00\n",
            "29  2.9000e-01 9.7517e-04       (1,) 1.4764e-05 9.9997e-01 1.4764e-05 7.8760e-02 1.0083e-01 -4.8827e-02 9.9237e-02 1.2220e-01 5.2399e-04 5.7517e-04 5.5907e-04 0.0000e+00 9.7517e-02 0.0000e+00\n",
            "30  3.0000e-01 9.7305e-04       (1,) 1.5189e-05 9.9997e-01 1.5189e-05 7.8684e-02 1.0074e-01 -4.8790e-02 9.9159e-02 1.2213e-01 5.2262e-04 5.7305e-04 5.5774e-04 0.0000e+00 9.7305e-02 0.0000e+00\n",
            "31  3.1000e-01 9.7093e-04       (1,) 1.5484e-05 9.9997e-01 1.5484e-05 7.8609e-02 1.0065e-01 -4.8754e-02 9.9082e-02 1.2207e-01 5.2126e-04 5.7093e-04 5.5641e-04 0.0000e+00 9.7093e-02 0.0000e+00\n",
            "32  3.2000e-01 9.6882e-04       (1,) 1.5941e-05 9.9997e-01 1.5941e-05 7.8534e-02 1.0056e-01 -4.8718e-02 9.9004e-02 1.2200e-01 5.1990e-04 5.6882e-04 5.5508e-04 0.0000e+00 9.6882e-02 0.0000e+00\n",
            "33  3.3000e-01 9.6672e-04       (1,) 1.6258e-05 9.9997e-01 1.6258e-05 7.8459e-02 1.0047e-01 -4.8681e-02 9.8927e-02 1.2193e-01 5.1855e-04 5.6672e-04 5.5376e-04 0.0000e+00 9.6672e-02 0.0000e+00\n",
            "34  3.4000e-01 9.6462e-04       (1,) 1.6750e-05 9.9997e-01 1.6750e-05 7.8384e-02 1.0038e-01 -4.8645e-02 9.8850e-02 1.2187e-01 5.1721e-04 5.6462e-04 5.5245e-04 0.0000e+00 9.6462e-02 0.0000e+00\n",
            "35  3.5000e-01 9.6253e-04       (1,) 1.7091e-05 9.9997e-01 1.7091e-05 7.8310e-02 1.0029e-01 -4.8609e-02 9.8773e-02 1.2180e-01 5.1586e-04 5.6253e-04 5.5113e-04 0.0000e+00 9.6253e-02 0.0000e+00\n",
            "36  3.6000e-01 9.6220e-04       (2,) 1.7623e-05 9.9996e-01 1.7623e-05 7.8287e-02 1.0028e-01 -4.8625e-02 9.8795e-02 1.2191e-01 5.1547e-04 5.6147e-04 5.5083e-04 0.0000e+00 0.0000e+00 9.6220e-02\n",
            "37  3.7000e-01 9.6217e-04       (2,) 1.7991e-05 9.9996e-01 1.7991e-05 7.8310e-02 1.0030e-01 -4.8636e-02 9.8819e-02 1.2193e-01 5.1523e-04 5.6058e-04 5.5070e-04 0.0000e+00 0.0000e+00 9.6217e-02\n",
            "38  3.8000e-01 9.6215e-04       (2,) 1.8565e-05 9.9996e-01 1.8565e-05 7.8333e-02 1.0033e-01 -4.8647e-02 9.8842e-02 1.2195e-01 5.1500e-04 5.5970e-04 5.5058e-04 0.0000e+00 0.0000e+00 9.6215e-02\n",
            "39  3.9000e-01 9.6213e-04       (2,) 1.8963e-05 9.9996e-01 1.8963e-05 7.8356e-02 1.0036e-01 -4.8658e-02 9.8865e-02 1.2197e-01 5.1477e-04 5.5881e-04 5.5045e-04 0.0000e+00 0.0000e+00 9.6213e-02\n",
            "40  4.0000e-01 9.6210e-04       (2,) 1.9585e-05 9.9996e-01 1.9585e-05 7.8379e-02 1.0039e-01 -4.8669e-02 9.8889e-02 1.2199e-01 5.1454e-04 5.5792e-04 5.5032e-04 0.0000e+00 0.0000e+00 9.6210e-02\n",
            "41  4.1000e-01 9.6208e-04       (2,) 2.0016e-05 9.9996e-01 2.0016e-05 7.8402e-02 1.0042e-01 -4.8680e-02 9.8912e-02 1.2201e-01 5.1431e-04 5.5703e-04 5.5020e-04 0.0000e+00 0.0000e+00 9.6208e-02\n",
            "42  4.2000e-01 9.6205e-04       (2,) 2.0691e-05 9.9996e-01 2.0691e-05 7.8425e-02 1.0044e-01 -4.8691e-02 9.8936e-02 1.2203e-01 5.1408e-04 5.5614e-04 5.5007e-04 0.0000e+00 0.0000e+00 9.6205e-02\n",
            "43  4.3000e-01 9.6203e-04       (2,) 2.1400e-05 9.9996e-01 2.1400e-05 7.8449e-02 1.0047e-01 -4.8702e-02 9.8959e-02 1.2205e-01 5.1386e-04 5.5525e-04 5.4994e-04 0.0000e+00 0.0000e+00 9.6203e-02\n",
            "44  4.4000e-01 9.6200e-04       (2,) 2.1894e-05 9.9996e-01 2.1894e-05 7.8472e-02 1.0050e-01 -4.8713e-02 9.8983e-02 1.2207e-01 5.1363e-04 5.5436e-04 5.4982e-04 0.0000e+00 0.0000e+00 9.6200e-02\n",
            "45  4.5000e-01 9.6198e-04       (2,) 2.2666e-05 9.9995e-01 2.2666e-05 7.8495e-02 1.0053e-01 -4.8724e-02 9.9006e-02 1.2209e-01 5.1340e-04 5.5346e-04 5.4969e-04 0.0000e+00 0.0000e+00 9.6198e-02\n",
            "46  4.6000e-01 9.6196e-04       (2,) 2.3481e-05 9.9995e-01 2.3481e-05 7.8518e-02 1.0055e-01 -4.8735e-02 9.9030e-02 1.2211e-01 5.1318e-04 5.5256e-04 5.4957e-04 0.0000e+00 0.0000e+00 9.6196e-02\n",
            "47  4.7000e-01 9.6193e-04       (2,) 2.4340e-05 9.9995e-01 2.4340e-05 7.8541e-02 1.0058e-01 -4.8746e-02 9.9053e-02 1.2213e-01 5.1295e-04 5.5167e-04 5.4944e-04 0.0000e+00 0.0000e+00 9.6193e-02\n",
            "48  4.8000e-01 9.6191e-04       (2,) 2.5247e-05 9.9995e-01 2.5247e-05 7.8565e-02 1.0061e-01 -4.8757e-02 9.9077e-02 1.2215e-01 5.1273e-04 5.5077e-04 5.4932e-04 0.0000e+00 0.0000e+00 9.6191e-02\n",
            "49  4.9000e-01 9.6188e-04       (2,) 2.6205e-05 9.9995e-01 2.6205e-05 7.8588e-02 1.0064e-01 -4.8768e-02 9.9100e-02 1.2216e-01 5.1251e-04 5.4987e-04 5.4919e-04 0.0000e+00 0.0000e+00 9.6188e-02\n",
            "50  5.0000e-01 9.6189e-04       (2,) 1.9585e-05 5.0260e-01 4.9738e-01 7.8611e-02 1.0067e-01 -4.8779e-02 9.9124e-02 1.2218e-01 5.1909e-04 5.5543e-04 5.5543e-04 0.0000e+00 0.0000e+00 9.6189e-02\n",
            "51  5.1000e-01 9.6260e-04       (2,) 2.7220e-05 2.7220e-05 9.9995e-01 7.8634e-02 1.0069e-01 -4.8790e-02 9.9148e-02 1.2220e-01 5.2667e-04 5.6195e-04 5.6260e-04 0.0000e+00 0.0000e+00 9.6260e-02\n",
            "52  5.2000e-01 9.6334e-04       (2,) 2.6537e-05 2.6537e-05 9.9995e-01 7.8658e-02 1.0072e-01 -4.8801e-02 9.9171e-02 1.2222e-01 5.2737e-04 5.6194e-04 5.6334e-04 0.0000e+00 0.0000e+00 9.6334e-02\n",
            "53  5.3000e-01 9.6408e-04       (2,) 2.5880e-05 2.5880e-05 9.9995e-01 7.8681e-02 1.0075e-01 -4.8812e-02 9.9195e-02 1.2224e-01 5.2807e-04 5.6193e-04 5.6408e-04 0.0000e+00 0.0000e+00 9.6408e-02\n",
            "54  5.4000e-01 9.6483e-04       (2,) 2.5560e-05 2.5560e-05 9.9995e-01 7.8704e-02 1.0078e-01 -4.8823e-02 9.9218e-02 1.2226e-01 5.2877e-04 5.6193e-04 5.6482e-04 0.0000e+00 0.0000e+00 9.6483e-02\n",
            "55  5.5000e-01 9.6557e-04       (2,) 2.4939e-05 2.4939e-05 9.9995e-01 7.8728e-02 1.0080e-01 -4.8834e-02 9.9242e-02 1.2228e-01 5.2947e-04 5.6192e-04 5.6557e-04 0.0000e+00 0.0000e+00 9.6557e-02\n",
            "56  5.6000e-01 9.6631e-04       (2,) 2.4637e-05 2.4637e-05 9.9995e-01 7.8751e-02 1.0083e-01 -4.8845e-02 9.9266e-02 1.2230e-01 5.3018e-04 5.6191e-04 5.6631e-04 0.0000e+00 0.0000e+00 9.6631e-02\n",
            "57  5.7000e-01 9.6706e-04       (2,) 2.4048e-05 2.4048e-05 9.9995e-01 7.8774e-02 1.0086e-01 -4.8856e-02 9.9289e-02 1.2232e-01 5.3088e-04 5.6191e-04 5.6706e-04 0.0000e+00 0.0000e+00 9.6706e-02\n",
            "58  5.8000e-01 9.6780e-04       (2,) 2.3481e-05 2.3481e-05 9.9995e-01 7.8798e-02 1.0089e-01 -4.8867e-02 9.9313e-02 1.2234e-01 5.3158e-04 5.6191e-04 5.6780e-04 0.0000e+00 0.0000e+00 9.6780e-02\n",
            "59  5.9000e-01 9.6855e-04       (2,) 2.3205e-05 2.3205e-05 9.9995e-01 7.8821e-02 1.0092e-01 -4.8878e-02 9.9337e-02 1.2236e-01 5.3229e-04 5.6190e-04 5.6855e-04 0.0000e+00 0.0000e+00 9.6855e-02\n",
            "60  6.0000e-01 9.6929e-04       (2,) 2.2666e-05 2.2666e-05 9.9995e-01 7.8845e-02 1.0094e-01 -4.8889e-02 9.9360e-02 1.2238e-01 5.3299e-04 5.6190e-04 5.6929e-04 0.0000e+00 0.0000e+00 9.6929e-02\n",
            "61  6.1000e-01 9.7004e-04       (2,) 2.2404e-05 2.2404e-05 9.9996e-01 7.8868e-02 1.0097e-01 -4.8900e-02 9.9384e-02 1.2240e-01 5.3370e-04 5.6190e-04 5.7004e-04 0.0000e+00 0.0000e+00 9.7004e-02\n",
            "62  6.2000e-01 9.7079e-04       (2,) 2.1894e-05 2.1894e-05 9.9996e-01 7.8891e-02 1.0100e-01 -4.8911e-02 9.9408e-02 1.2242e-01 5.3441e-04 5.6190e-04 5.7079e-04 0.0000e+00 0.0000e+00 9.7079e-02\n",
            "63  6.3000e-01 9.7154e-04       (2,) 2.1645e-05 2.1645e-05 9.9996e-01 7.8915e-02 1.0103e-01 -4.8922e-02 9.9431e-02 1.2244e-01 5.3512e-04 5.6190e-04 5.7154e-04 0.0000e+00 0.0000e+00 9.7154e-02\n",
            "64  6.4000e-01 9.7229e-04       (2,) 2.1160e-05 2.1160e-05 9.9996e-01 7.8938e-02 1.0106e-01 -4.8934e-02 9.9455e-02 1.2246e-01 5.3582e-04 5.6191e-04 5.7229e-04 0.0000e+00 0.0000e+00 9.7229e-02\n",
            "65  6.5000e-01 9.7304e-04       (2,) 2.0924e-05 2.0924e-05 9.9996e-01 7.8962e-02 1.0108e-01 -4.8945e-02 9.9479e-02 1.2248e-01 5.3653e-04 5.6191e-04 5.7303e-04 0.0000e+00 0.0000e+00 9.7304e-02\n",
            "66  6.6000e-01 9.7379e-04       (2,) 2.0462e-05 2.0462e-05 9.9996e-01 7.8985e-02 1.0111e-01 -4.8956e-02 9.9502e-02 1.2250e-01 5.3724e-04 5.6191e-04 5.7379e-04 0.0000e+00 0.0000e+00 9.7379e-02\n",
            "67  6.7000e-01 9.7454e-04       (2,) 2.0238e-05 2.0238e-05 9.9996e-01 7.9009e-02 1.0114e-01 -4.8967e-02 9.9526e-02 1.2252e-01 5.3795e-04 5.6192e-04 5.7454e-04 0.0000e+00 0.0000e+00 9.7454e-02\n",
            "68  6.8000e-01 9.7529e-04       (2,) 1.9799e-05 1.9799e-05 9.9996e-01 7.9032e-02 1.0117e-01 -4.8978e-02 9.9550e-02 1.2254e-01 5.3867e-04 5.6192e-04 5.7529e-04 0.0000e+00 0.0000e+00 9.7529e-02\n",
            "69  6.9000e-01 9.7604e-04       (2,) 1.9585e-05 1.9585e-05 9.9996e-01 7.9056e-02 1.0120e-01 -4.8989e-02 9.9574e-02 1.2256e-01 5.3938e-04 5.6193e-04 5.7604e-04 0.0000e+00 0.0000e+00 9.7604e-02\n",
            "70  7.0000e-01 9.7679e-04       (2,) 1.9374e-05 1.9374e-05 9.9996e-01 7.9079e-02 1.0122e-01 -4.9000e-02 9.9597e-02 1.2258e-01 5.4009e-04 5.6194e-04 5.7679e-04 0.0000e+00 0.0000e+00 9.7679e-02\n",
            "71  7.1000e-01 9.7755e-04       (2,) 1.8963e-05 1.8963e-05 9.9996e-01 7.9103e-02 1.0125e-01 -4.9011e-02 9.9621e-02 1.2260e-01 5.4080e-04 5.6195e-04 5.7755e-04 0.0000e+00 0.0000e+00 9.7755e-02\n",
            "72  7.2000e-01 9.7830e-04       (2,) 1.8762e-05 1.8762e-05 9.9996e-01 7.9126e-02 1.0128e-01 -4.9023e-02 9.9645e-02 1.2262e-01 5.4152e-04 5.6196e-04 5.7830e-04 0.0000e+00 0.0000e+00 9.7830e-02\n",
            "73  7.3000e-01 9.7906e-04       (2,) 1.8371e-05 1.8371e-05 9.9996e-01 7.9150e-02 1.0131e-01 -4.9034e-02 9.9669e-02 1.2263e-01 5.4223e-04 5.6197e-04 5.7906e-04 0.0000e+00 0.0000e+00 9.7906e-02\n",
            "74  7.4000e-01 9.7981e-04       (2,) 1.8179e-05 1.8179e-05 9.9996e-01 7.9174e-02 1.0134e-01 -4.9045e-02 9.9693e-02 1.2265e-01 5.4295e-04 5.6198e-04 5.7981e-04 0.0000e+00 0.0000e+00 9.7981e-02\n",
            "75  7.5000e-01 9.8057e-04       (2,) 1.7991e-05 1.7991e-05 9.9996e-01 7.9197e-02 1.0137e-01 -4.9056e-02 9.9717e-02 1.2267e-01 5.4367e-04 5.6199e-04 5.8057e-04 0.0000e+00 0.0000e+00 9.8057e-02\n",
            "76  7.6000e-01 9.8133e-04       (2,) 1.7623e-05 1.7623e-05 9.9996e-01 7.9221e-02 1.0139e-01 -4.9067e-02 9.9740e-02 1.2269e-01 5.4438e-04 5.6201e-04 5.8133e-04 0.0000e+00 0.0000e+00 9.8133e-02\n",
            "77  7.7000e-01 9.8208e-04       (2,) 1.7443e-05 1.7443e-05 9.9997e-01 7.9244e-02 1.0142e-01 -4.9078e-02 9.9764e-02 1.2271e-01 5.4510e-04 5.6202e-04 5.8208e-04 0.0000e+00 0.0000e+00 9.8208e-02\n",
            "78  7.8000e-01 9.8284e-04       (2,) 1.7266e-05 1.7266e-05 9.9997e-01 7.9268e-02 1.0145e-01 -4.9089e-02 9.9788e-02 1.2273e-01 5.4582e-04 5.6204e-04 5.8284e-04 0.0000e+00 0.0000e+00 9.8284e-02\n",
            "79  7.9000e-01 9.8360e-04       (2,) 1.6920e-05 1.6920e-05 9.9997e-01 7.9292e-02 1.0148e-01 -4.9101e-02 9.9812e-02 1.2275e-01 5.4654e-04 5.6205e-04 5.8360e-04 0.0000e+00 0.0000e+00 9.8360e-02\n",
            "80  8.0000e-01 9.8436e-04       (2,) 1.6750e-05 1.6750e-05 9.9997e-01 7.9315e-02 1.0151e-01 -4.9112e-02 9.9836e-02 1.2277e-01 5.4726e-04 5.6207e-04 5.8436e-04 0.0000e+00 0.0000e+00 9.8436e-02\n",
            "81  8.1000e-01 9.8512e-04       (2,) 1.6584e-05 1.6584e-05 9.9997e-01 7.9339e-02 1.0153e-01 -4.9123e-02 9.9860e-02 1.2279e-01 5.4798e-04 5.6209e-04 5.8512e-04 0.0000e+00 0.0000e+00 9.8512e-02\n",
            "82  8.2000e-01 9.8588e-04       (2,) 1.6258e-05 1.6258e-05 9.9997e-01 7.9363e-02 1.0156e-01 -4.9134e-02 9.9884e-02 1.2281e-01 5.4870e-04 5.6211e-04 5.8588e-04 0.0000e+00 0.0000e+00 9.8588e-02\n",
            "83  8.3000e-01 9.8664e-04       (2,) 1.6098e-05 1.6098e-05 9.9997e-01 7.9386e-02 1.0159e-01 -4.9145e-02 9.9908e-02 1.2283e-01 5.4942e-04 5.6213e-04 5.8664e-04 0.0000e+00 0.0000e+00 9.8664e-02\n",
            "84  8.4000e-01 9.8741e-04       (2,) 1.5941e-05 1.5941e-05 9.9997e-01 7.9410e-02 1.0162e-01 -4.9156e-02 9.9932e-02 1.2285e-01 5.5015e-04 5.6215e-04 5.8741e-04 0.0000e+00 0.0000e+00 9.8741e-02\n",
            "85  8.5000e-01 9.8817e-04       (2,) 1.5787e-05 1.5787e-05 9.9997e-01 7.9434e-02 1.0165e-01 -4.9168e-02 9.9955e-02 1.2287e-01 5.5087e-04 5.6217e-04 5.8817e-04 0.0000e+00 0.0000e+00 9.8817e-02\n",
            "86  8.6000e-01 9.8893e-04       (2,) 1.5484e-05 1.5484e-05 9.9997e-01 7.9458e-02 1.0168e-01 -4.9179e-02 9.9979e-02 1.2289e-01 5.5160e-04 5.6220e-04 5.8893e-04 0.0000e+00 0.0000e+00 9.8893e-02\n",
            "87  8.7000e-01 9.8970e-04       (2,) 1.5336e-05 1.5336e-05 9.9997e-01 7.9481e-02 1.0170e-01 -4.9190e-02 1.0000e-01 1.2291e-01 5.5232e-04 5.6222e-04 5.8970e-04 0.0000e+00 0.0000e+00 9.8970e-02\n",
            "88  8.8000e-01 9.9046e-04       (2,) 1.5189e-05 1.5189e-05 9.9997e-01 7.9505e-02 1.0173e-01 -4.9201e-02 1.0003e-01 1.2293e-01 5.5305e-04 5.6224e-04 5.9046e-04 0.0000e+00 0.0000e+00 9.9046e-02\n",
            "89  8.9000e-01 9.9123e-04       (2,) 1.5046e-05 1.5046e-05 9.9997e-01 7.9529e-02 1.0176e-01 -4.9213e-02 1.0005e-01 1.2295e-01 5.5377e-04 5.6227e-04 5.9123e-04 0.0000e+00 0.0000e+00 9.9123e-02\n",
            "90  9.0000e-01 9.9200e-04       (2,) 1.4764e-05 1.4764e-05 9.9997e-01 7.9553e-02 1.0179e-01 -4.9224e-02 1.0008e-01 1.2297e-01 5.5450e-04 5.6230e-04 5.9200e-04 0.0000e+00 0.0000e+00 9.9200e-02\n",
            "91  9.1000e-01 9.9276e-04       (2,) 1.4626e-05 1.4626e-05 9.9997e-01 7.9577e-02 1.0182e-01 -4.9235e-02 1.0010e-01 1.2299e-01 5.5523e-04 5.6233e-04 5.9276e-04 0.0000e+00 0.0000e+00 9.9276e-02\n",
            "92  9.2000e-01 9.9353e-04       (2,) 1.4490e-05 1.4490e-05 9.9997e-01 7.9600e-02 1.0185e-01 -4.9246e-02 1.0012e-01 1.2301e-01 5.5596e-04 5.6236e-04 5.9353e-04 0.0000e+00 0.0000e+00 9.9353e-02\n",
            "93  9.3000e-01 9.9430e-04       (2,) 1.4355e-05 1.4355e-05 9.9997e-01 7.9624e-02 1.0188e-01 -4.9257e-02 1.0015e-01 1.2303e-01 5.5669e-04 5.6239e-04 5.9430e-04 0.0000e+00 0.0000e+00 9.9430e-02\n",
            "94  9.4000e-01 9.9507e-04       (2,) 1.4223e-05 1.4223e-05 9.9997e-01 7.9648e-02 1.0190e-01 -4.9269e-02 1.0017e-01 1.2305e-01 5.5742e-04 5.6242e-04 5.9507e-04 0.0000e+00 0.0000e+00 9.9507e-02\n",
            "95  9.5000e-01 9.9584e-04       (2,) 1.3964e-05 1.3964e-05 9.9997e-01 7.9672e-02 1.0193e-01 -4.9280e-02 1.0020e-01 1.2307e-01 5.5815e-04 5.6245e-04 5.9584e-04 0.0000e+00 0.0000e+00 9.9584e-02\n",
            "96  9.6000e-01 9.9661e-04       (2,) 1.3837e-05 1.3837e-05 9.9997e-01 7.9696e-02 1.0196e-01 -4.9291e-02 1.0022e-01 1.2309e-01 5.5888e-04 5.6248e-04 5.9661e-04 0.0000e+00 0.0000e+00 9.9661e-02\n",
            "97  9.7000e-01 9.9738e-04       (2,) 1.3712e-05 1.3712e-05 9.9997e-01 7.9720e-02 1.0199e-01 -4.9302e-02 1.0024e-01 1.2311e-01 5.5961e-04 5.6252e-04 5.9738e-04 0.0000e+00 0.0000e+00 9.9738e-02\n",
            "98  9.8000e-01 9.9815e-04       (2,) 1.3588e-05 1.3588e-05 9.9997e-01 7.9744e-02 1.0202e-01 -4.9314e-02 1.0027e-01 1.2313e-01 5.6035e-04 5.6255e-04 5.9815e-04 0.0000e+00 0.0000e+00 9.9815e-02\n",
            "99  9.9000e-01 9.9893e-04       (2,) 1.3466e-05 1.3466e-05 9.9997e-01 7.9768e-02 1.0205e-01 -4.9325e-02 1.0029e-01 1.2315e-01 5.6108e-04 5.6259e-04 5.9892e-04 0.0000e+00 0.0000e+00 9.9893e-02\n",
            "100 1.0000e+00 9.9970e-04       (2,) 1.3228e-05 1.3228e-05 9.9997e-01 7.9791e-02 1.0207e-01 -4.9336e-02 1.0032e-01 1.2317e-01 5.6181e-04 5.6262e-04 5.9970e-04 0.0000e+00 0.0000e+00 9.9970e-02\n",
            "101 1.0100e+00 1.0005e-03       (2,) 1.3110e-05 1.3110e-05 9.9997e-01 7.9815e-02 1.0210e-01 -4.9347e-02 1.0034e-01 1.2319e-01 5.6255e-04 5.6266e-04 6.0047e-04 0.0000e+00 0.0000e+00 1.0005e-01\n",
            "102 1.0200e+00 1.0012e-03       (2,) 1.2995e-05 1.2995e-05 9.9997e-01 7.9839e-02 1.0213e-01 -4.9359e-02 1.0036e-01 1.2321e-01 5.6329e-04 5.6270e-04 6.0125e-04 0.0000e+00 0.0000e+00 1.0012e-01\n",
            "103 1.0300e+00 1.0020e-03       (2,) 1.2881e-05 1.2881e-05 9.9997e-01 7.9863e-02 1.0216e-01 -4.9370e-02 1.0039e-01 1.2323e-01 5.6402e-04 5.6274e-04 6.0202e-04 0.0000e+00 0.0000e+00 1.0020e-01\n",
            "104 1.0400e+00 1.0028e-03       (2,) 1.2768e-05 1.2768e-05 9.9997e-01 7.9887e-02 1.0219e-01 -4.9381e-02 1.0041e-01 1.2325e-01 5.6476e-04 5.6278e-04 6.0280e-04 0.0000e+00 0.0000e+00 1.0028e-01\n",
            "105 1.0500e+00 1.0036e-03       (2,) 1.2657e-05 1.2657e-05 9.9997e-01 7.9911e-02 1.0222e-01 -4.9393e-02 1.0044e-01 1.2326e-01 5.6550e-04 5.6282e-04 6.0358e-04 0.0000e+00 0.0000e+00 1.0036e-01\n",
            "106 1.0600e+00 1.0044e-03       (2,) 1.2548e-05 1.2548e-05 9.9997e-01 7.9935e-02 1.0225e-01 -4.9404e-02 1.0046e-01 1.2328e-01 5.6624e-04 5.6287e-04 6.0435e-04 0.0000e+00 0.0000e+00 1.0044e-01\n",
            "107 1.0700e+00 1.0051e-03       (2,) 1.2440e-05 1.2440e-05 9.9998e-01 7.9959e-02 1.0228e-01 -4.9415e-02 1.0049e-01 1.2330e-01 5.6698e-04 5.6291e-04 6.0513e-04 0.0000e+00 0.0000e+00 1.0051e-01\n",
            "108 1.0800e+00 1.0059e-03       (2,) 1.2227e-05 1.2227e-05 9.9998e-01 7.9983e-02 1.0230e-01 -4.9427e-02 1.0051e-01 1.2332e-01 5.6772e-04 5.6296e-04 6.0591e-04 0.0000e+00 0.0000e+00 1.0059e-01\n",
            "109 1.0900e+00 1.0067e-03       (2,) 1.2123e-05 1.2123e-05 9.9998e-01 8.0008e-02 1.0233e-01 -4.9438e-02 1.0053e-01 1.2334e-01 5.6846e-04 5.6300e-04 6.0669e-04 0.0000e+00 0.0000e+00 1.0067e-01\n",
            "110 1.1000e+00 1.0075e-03       (2,) 1.2021e-05 1.2021e-05 9.9998e-01 8.0032e-02 1.0236e-01 -4.9449e-02 1.0056e-01 1.2336e-01 5.6920e-04 5.6305e-04 6.0747e-04 0.0000e+00 0.0000e+00 1.0075e-01\n",
            "111 1.1100e+00 1.0083e-03       (2,) 1.1919e-05 1.1919e-05 9.9998e-01 8.0056e-02 1.0239e-01 -4.9460e-02 1.0058e-01 1.2338e-01 5.6994e-04 5.6310e-04 6.0825e-04 0.0000e+00 0.0000e+00 1.0083e-01\n",
            "112 1.1200e+00 1.0090e-03       (2,) 1.1819e-05 1.1819e-05 9.9998e-01 8.0080e-02 1.0242e-01 -4.9472e-02 1.0061e-01 1.2340e-01 5.7069e-04 5.6315e-04 6.0903e-04 0.0000e+00 0.0000e+00 1.0090e-01\n",
            "113 1.1300e+00 1.0098e-03       (2,) 1.1720e-05 1.1720e-05 9.9998e-01 8.0104e-02 1.0245e-01 -4.9483e-02 1.0063e-01 1.2342e-01 5.7143e-04 5.6320e-04 6.0982e-04 0.0000e+00 0.0000e+00 1.0098e-01\n",
            "114 1.1400e+00 1.0106e-03       (2,) 1.1623e-05 1.1623e-05 9.9998e-01 8.0128e-02 1.0248e-01 -4.9494e-02 1.0065e-01 1.2344e-01 5.7218e-04 5.6325e-04 6.1060e-04 0.0000e+00 0.0000e+00 1.0106e-01\n",
            "115 1.1500e+00 1.0114e-03       (2,) 1.1526e-05 1.1526e-05 9.9998e-01 8.0152e-02 1.0251e-01 -4.9506e-02 1.0068e-01 1.2346e-01 5.7292e-04 5.6330e-04 6.1138e-04 0.0000e+00 0.0000e+00 1.0114e-01\n",
            "116 1.1600e+00 1.0122e-03       (2,) 1.1431e-05 1.1431e-05 9.9998e-01 8.0176e-02 1.0253e-01 -4.9517e-02 1.0070e-01 1.2348e-01 5.7367e-04 5.6335e-04 6.1217e-04 0.0000e+00 0.0000e+00 1.0122e-01\n",
            "117 1.1700e+00 1.0130e-03       (2,) 1.1337e-05 1.1337e-05 9.9998e-01 8.0200e-02 1.0256e-01 -4.9529e-02 1.0073e-01 1.2350e-01 5.7442e-04 5.6341e-04 6.1295e-04 0.0000e+00 0.0000e+00 1.0130e-01\n",
            "118 1.1800e+00 1.0137e-03       (2,) 1.1244e-05 1.1244e-05 9.9998e-01 8.0225e-02 1.0259e-01 -4.9540e-02 1.0075e-01 1.2352e-01 5.7517e-04 5.6346e-04 6.1374e-04 0.0000e+00 0.0000e+00 1.0137e-01\n",
            "119 1.1900e+00 1.0145e-03       (2,) 1.1152e-05 1.1152e-05 9.9998e-01 8.0249e-02 1.0262e-01 -4.9551e-02 1.0078e-01 1.2354e-01 5.7591e-04 5.6352e-04 6.1453e-04 0.0000e+00 0.0000e+00 1.0145e-01\n",
            "120 1.2000e+00 1.0153e-03       (2,) 1.1061e-05 1.1061e-05 9.9998e-01 8.0273e-02 1.0265e-01 -4.9563e-02 1.0080e-01 1.2356e-01 5.7666e-04 5.6358e-04 6.1531e-04 0.0000e+00 0.0000e+00 1.0153e-01\n",
            "121 1.2100e+00 1.0161e-03       (2,) 1.0883e-05 1.0883e-05 9.9998e-01 8.0297e-02 1.0268e-01 -4.9574e-02 1.0082e-01 1.2358e-01 5.7741e-04 5.6364e-04 6.1610e-04 0.0000e+00 0.0000e+00 1.0161e-01\n",
            "122 1.2200e+00 1.0169e-03       (2,) 1.0796e-05 1.0796e-05 9.9998e-01 8.0322e-02 1.0271e-01 -4.9585e-02 1.0085e-01 1.2360e-01 5.7817e-04 5.6369e-04 6.1689e-04 0.0000e+00 0.0000e+00 1.0169e-01\n",
            "123 1.2300e+00 1.0177e-03       (2,) 1.0710e-05 1.0710e-05 9.9998e-01 8.0346e-02 1.0274e-01 -4.9597e-02 1.0087e-01 1.2362e-01 5.7892e-04 5.6376e-04 6.1768e-04 0.0000e+00 0.0000e+00 1.0177e-01\n",
            "124 1.2400e+00 1.0185e-03       (2,) 1.0624e-05 1.0624e-05 9.9998e-01 8.0370e-02 1.0277e-01 -4.9608e-02 1.0090e-01 1.2364e-01 5.7967e-04 5.6382e-04 6.1847e-04 0.0000e+00 0.0000e+00 1.0185e-01\n",
            "125 1.2500e+00 1.0193e-03       (2,) 1.0540e-05 1.0540e-05 9.9998e-01 8.0394e-02 1.0279e-01 -4.9620e-02 1.0092e-01 1.2366e-01 5.8042e-04 5.6388e-04 6.1926e-04 0.0000e+00 0.0000e+00 1.0193e-01\n",
            "126 1.2600e+00 1.0201e-03       (2,) 1.0457e-05 1.0457e-05 9.9998e-01 8.0419e-02 1.0282e-01 -4.9631e-02 1.0095e-01 1.2368e-01 5.8118e-04 5.6394e-04 6.2005e-04 0.0000e+00 0.0000e+00 1.0201e-01\n",
            "127 1.2700e+00 1.0208e-03       (2,) 1.0374e-05 1.0374e-05 9.9998e-01 8.0443e-02 1.0285e-01 -4.9642e-02 1.0097e-01 1.2370e-01 5.8193e-04 5.6401e-04 6.2084e-04 0.0000e+00 0.0000e+00 1.0208e-01\n",
            "128 1.2800e+00 1.0216e-03       (2,) 1.0293e-05 1.0293e-05 9.9998e-01 8.0467e-02 1.0288e-01 -4.9654e-02 1.0100e-01 1.2372e-01 5.8269e-04 5.6407e-04 6.2164e-04 0.0000e+00 0.0000e+00 1.0216e-01\n",
            "129 1.2900e+00 1.0224e-03       (2,) 1.0212e-05 1.0212e-05 9.9998e-01 8.0492e-02 1.0291e-01 -4.9665e-02 1.0102e-01 1.2374e-01 5.8345e-04 5.6414e-04 6.2243e-04 0.0000e+00 0.0000e+00 1.0224e-01\n",
            "130 1.3000e+00 1.0232e-03       (2,) 1.0133e-05 1.0133e-05 9.9998e-01 8.0516e-02 1.0294e-01 -4.9677e-02 1.0104e-01 1.2376e-01 5.8420e-04 5.6421e-04 6.2322e-04 0.0000e+00 0.0000e+00 1.0232e-01\n",
            "131 1.3100e+00 1.0240e-03       (2,) 1.0054e-05 1.0054e-05 9.9998e-01 8.0540e-02 1.0297e-01 -4.9688e-02 1.0107e-01 1.2378e-01 5.8496e-04 5.6428e-04 6.2402e-04 0.0000e+00 0.0000e+00 1.0240e-01\n",
            "132 1.3200e+00 1.0248e-03       (2,) 9.9768e-06 9.9768e-06 9.9998e-01 8.0565e-02 1.0300e-01 -4.9699e-02 1.0109e-01 1.2380e-01 5.8572e-04 5.6434e-04 6.2482e-04 0.0000e+00 0.0000e+00 1.0248e-01\n",
            "133 1.3300e+00 1.0256e-03       (2,) 9.9000e-06 9.9000e-06 9.9998e-01 8.0589e-02 1.0303e-01 -4.9711e-02 1.0112e-01 1.2382e-01 5.8648e-04 5.6442e-04 6.2561e-04 0.0000e+00 0.0000e+00 1.0256e-01\n",
            "134 1.3400e+00 1.0264e-03       (2,) 9.8241e-06 9.8241e-06 9.9998e-01 8.0614e-02 1.0306e-01 -4.9722e-02 1.0114e-01 1.2384e-01 5.8724e-04 5.6449e-04 6.2641e-04 0.0000e+00 0.0000e+00 1.0264e-01\n",
            "135 1.3500e+00 1.0272e-03       (2,) 9.7492e-06 9.7492e-06 9.9998e-01 8.0638e-02 1.0308e-01 -4.9734e-02 1.0117e-01 1.2386e-01 5.8800e-04 5.6456e-04 6.2721e-04 0.0000e+00 0.0000e+00 1.0272e-01\n",
            "136 1.3600e+00 1.0280e-03       (2,) 9.6750e-06 9.6750e-06 9.9998e-01 8.0662e-02 1.0311e-01 -4.9745e-02 1.0119e-01 1.2388e-01 5.8876e-04 5.6463e-04 6.2801e-04 0.0000e+00 0.0000e+00 1.0280e-01\n",
            "137 1.3700e+00 1.0288e-03       (2,) 9.6017e-06 9.6017e-06 9.9998e-01 8.0687e-02 1.0314e-01 -4.9757e-02 1.0122e-01 1.2390e-01 5.8953e-04 5.6471e-04 6.2881e-04 0.0000e+00 0.0000e+00 1.0288e-01\n",
            "138 1.3800e+00 1.0296e-03       (2,) 9.6017e-06 9.6017e-06 9.9998e-01 8.0711e-02 1.0317e-01 -4.9768e-02 1.0124e-01 1.2392e-01 5.9029e-04 5.6479e-04 6.2961e-04 0.0000e+00 0.0000e+00 1.0296e-01\n",
            "139 1.3900e+00 1.0304e-03       (2,) 9.5293e-06 9.5293e-06 9.9998e-01 8.0736e-02 1.0320e-01 -4.9780e-02 1.0126e-01 1.2394e-01 5.9105e-04 5.6486e-04 6.3041e-04 0.0000e+00 0.0000e+00 1.0304e-01\n",
            "140 1.4000e+00 1.0312e-03       (2,) 9.4576e-06 9.4576e-06 9.9998e-01 8.0760e-02 1.0323e-01 -4.9791e-02 1.0129e-01 1.2396e-01 5.9182e-04 5.6494e-04 6.3121e-04 0.0000e+00 0.0000e+00 1.0312e-01\n",
            "141 1.4100e+00 1.0320e-03       (2,) 9.3868e-06 9.3868e-06 9.9998e-01 8.0785e-02 1.0326e-01 -4.9802e-02 1.0131e-01 1.2398e-01 5.9258e-04 5.6502e-04 6.3201e-04 0.0000e+00 0.0000e+00 1.0320e-01\n",
            "142 1.4200e+00 1.0328e-03       (2,) 9.3167e-06 9.3167e-06 9.9998e-01 8.0809e-02 1.0329e-01 -4.9814e-02 1.0134e-01 1.2400e-01 5.9335e-04 5.6510e-04 6.3281e-04 0.0000e+00 0.0000e+00 1.0328e-01\n",
            "143 1.4300e+00 1.0336e-03       (2,) 9.2474e-06 9.2474e-06 9.9998e-01 8.0834e-02 1.0332e-01 -4.9825e-02 1.0136e-01 1.2402e-01 5.9412e-04 5.6518e-04 6.3362e-04 0.0000e+00 0.0000e+00 1.0336e-01\n",
            "144 1.4400e+00 1.0344e-03       (2,) 9.1789e-06 9.1789e-06 9.9998e-01 8.0858e-02 1.0335e-01 -4.9837e-02 1.0139e-01 1.2404e-01 5.9489e-04 5.6526e-04 6.3442e-04 0.0000e+00 0.0000e+00 1.0344e-01\n",
            "145 1.4500e+00 1.0352e-03       (2,) 9.1112e-06 9.1112e-06 9.9998e-01 8.0883e-02 1.0338e-01 -4.9848e-02 1.0141e-01 1.2406e-01 5.9565e-04 5.6535e-04 6.3523e-04 0.0000e+00 0.0000e+00 1.0352e-01\n",
            "146 1.4600e+00 1.0360e-03       (2,) 9.0442e-06 9.0442e-06 9.9998e-01 8.0908e-02 1.0341e-01 -4.9860e-02 1.0144e-01 1.2408e-01 5.9642e-04 5.6543e-04 6.3603e-04 0.0000e+00 0.0000e+00 1.0360e-01\n",
            "147 1.4700e+00 1.0368e-03       (2,) 8.9780e-06 8.9780e-06 9.9998e-01 8.0932e-02 1.0344e-01 -4.9871e-02 1.0146e-01 1.2410e-01 5.9719e-04 5.6552e-04 6.3684e-04 0.0000e+00 0.0000e+00 1.0368e-01\n",
            "148 1.4800e+00 1.0376e-03       (2,) 8.9124e-06 8.9124e-06 9.9998e-01 8.0957e-02 1.0346e-01 -4.9883e-02 1.0149e-01 1.2412e-01 5.9797e-04 5.6560e-04 6.3765e-04 0.0000e+00 0.0000e+00 1.0376e-01\n",
            "149 1.4900e+00 1.0385e-03       (2,) 8.8476e-06 8.8476e-06 9.9998e-01 8.0981e-02 1.0349e-01 -4.9894e-02 1.0151e-01 1.2414e-01 5.9874e-04 5.6569e-04 6.3846e-04 0.0000e+00 0.0000e+00 1.0385e-01\n",
            "150 1.5000e+00 1.0393e-03       (2,) 8.7835e-06 8.7835e-06 9.9998e-01 8.1006e-02 1.0352e-01 -4.9906e-02 1.0154e-01 1.2416e-01 5.9951e-04 5.6578e-04 6.3926e-04 0.0000e+00 0.0000e+00 1.0393e-01\n",
            "\n",
            "Filtered results (16 points) saved to: /content/alpha_full_results_0.0_1.5_activeset_mu002_sparse.csv\n",
            "Saved alpha values: [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.  1.1 1.2 1.3 1.4 1.5]\n",
            "\n",
            "--- Quick KKT Check (Stationarity w.r.t. w) ---\n",
            "     alpha active_set_report active_set_kkt  max_violation  grad_consistency_violation check_result\n",
            "0.0000e+00              (1,)           (1,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.0000e-02              (1,)           (1,)     0.0000e+00                  0.0000e+00           OK\n",
            "2.0000e-02              (1,)           (1,)     0.0000e+00                  0.0000e+00           OK\n",
            "3.0000e-02              (1,)           (1,)     0.0000e+00                  0.0000e+00           OK\n",
            "4.0000e-02              (1,)           (1,)     0.0000e+00                  0.0000e+00           OK\n",
            "5.0000e-02              (1,)           (1,)     0.0000e+00                  0.0000e+00           OK\n",
            "6.0000e-02              (1,)           (1,)     0.0000e+00                  0.0000e+00           OK\n",
            "7.0000e-02              (1,)           (1,)     0.0000e+00                  0.0000e+00           OK\n",
            "8.0000e-02              (1,)           (1,)     0.0000e+00                  0.0000e+00           OK\n",
            "9.0000e-02              (1,)      (0, 1, 2)     6.6473e-05                  6.6473e-05         WARN\n",
            "1.0000e-01              (1,)      (0, 1, 2)     6.5698e-05                  6.5698e-05         WARN\n",
            "1.1000e-01              (1,)      (0, 1, 2)     6.4925e-05                  6.4925e-05         WARN\n",
            "1.2000e-01              (1,)      (0, 1, 2)     6.4153e-05                  6.4153e-05         WARN\n",
            "1.3000e-01              (1,)      (0, 1, 2)     6.3382e-05                  6.3382e-05         WARN\n",
            "1.4000e-01              (1,)      (0, 1, 2)     6.2612e-05                  6.2612e-05         WARN\n",
            "1.5000e-01              (1,)      (0, 1, 2)     6.1843e-05                  6.1843e-05         WARN\n",
            "1.6000e-01              (1,)      (0, 1, 2)     6.1075e-05                  6.1075e-05         WARN\n",
            "1.7000e-01              (1,)      (0, 1, 2)     6.0307e-05                  6.0307e-05         WARN\n",
            "1.8000e-01              (1,)      (0, 1, 2)     5.9541e-05                  5.9541e-05         WARN\n",
            "1.9000e-01              (1,)      (0, 1, 2)     5.8776e-05                  5.8776e-05         WARN\n",
            "2.0000e-01              (1,)      (0, 1, 2)     5.8012e-05                  5.8012e-05         WARN\n",
            "2.1000e-01              (1,)      (0, 1, 2)     5.7249e-05                  5.7249e-05         WARN\n",
            "2.2000e-01              (1,)      (0, 1, 2)     5.6486e-05                  5.6486e-05         WARN\n",
            "2.3000e-01              (1,)      (0, 1, 2)     5.5725e-05                  5.5725e-05         WARN\n",
            "2.4000e-01              (1,)      (0, 1, 2)     5.4965e-05                  5.4965e-05         WARN\n",
            "2.5000e-01              (1,)      (0, 1, 2)     5.4205e-05                  5.4205e-05         WARN\n",
            "2.6000e-01              (1,)      (0, 1, 2)     5.3447e-05                  5.3447e-05         WARN\n",
            "2.7000e-01              (1,)      (0, 1, 2)     5.2690e-05                  5.2690e-05         WARN\n",
            "2.8000e-01              (1,)      (0, 1, 2)     5.1933e-05                  5.1933e-05         WARN\n",
            "2.9000e-01              (1,)      (0, 1, 2)     5.1178e-05                  5.1178e-05         WARN\n",
            "3.0000e-01              (1,)      (0, 1, 2)     5.0423e-05                  5.0423e-05         WARN\n",
            "3.1000e-01              (1,)      (0, 1, 2)     4.9670e-05                  4.9670e-05         WARN\n",
            "3.2000e-01              (1,)      (0, 1, 2)     4.8917e-05                  4.8917e-05         WARN\n",
            "3.3000e-01              (1,)      (0, 1, 2)     4.8166e-05                  4.8166e-05         WARN\n",
            "3.4000e-01              (1,)      (0, 1, 2)     4.7415e-05                  4.7415e-05         WARN\n",
            "3.5000e-01              (1,)      (0, 1, 2)     4.6666e-05                  4.6666e-05         WARN\n",
            "3.6000e-01              (2,)      (0, 1, 2)     4.6001e-05                  4.6001e-05         WARN\n",
            "3.7000e-01              (2,)      (0, 1, 2)     4.5349e-05                  4.5349e-05         WARN\n",
            "3.8000e-01              (2,)      (0, 1, 2)     4.4695e-05                  4.4695e-05         WARN\n",
            "3.9000e-01              (2,)      (0, 1, 2)     4.4039e-05                  4.4039e-05         WARN\n",
            "4.0000e-01              (2,)      (0, 1, 2)     4.3380e-05                  4.3380e-05         WARN\n",
            "4.1000e-01              (2,)      (0, 1, 2)     4.2720e-05                  4.2720e-05         WARN\n",
            "4.2000e-01              (2,)      (0, 1, 2)     4.2058e-05                  4.2058e-05         WARN\n",
            "4.3000e-01              (2,)      (0, 1, 2)     4.1393e-05                  4.1393e-05         WARN\n",
            "4.4000e-01              (2,)      (0, 1, 2)     4.0726e-05                  4.0726e-05         WARN\n",
            "4.5000e-01              (2,)      (0, 1, 2)     4.0057e-05                  4.0057e-05         WARN\n",
            "4.6000e-01              (2,)      (0, 1, 2)     3.9386e-05                  3.9386e-05         WARN\n",
            "4.7000e-01              (2,)      (0, 1, 2)     3.8713e-05                  3.8713e-05         WARN\n",
            "4.8000e-01              (2,)      (0, 1, 2)     3.8038e-05                  3.8038e-05         WARN\n",
            "4.9000e-01              (2,)      (0, 1, 2)     3.7360e-05                  3.7360e-05         WARN\n",
            "5.0000e-01              (2,)      (0, 1, 2)     3.6339e-05                  3.6339e-05         WARN\n",
            "5.1000e-01              (2,)      (0, 1, 2)     3.5931e-05                  3.5931e-05         WARN\n",
            "5.2000e-01              (2,)      (0, 1, 2)     3.5972e-05                  3.5972e-05         WARN\n",
            "5.3000e-01              (2,)      (0, 1, 2)     3.6013e-05                  3.6013e-05         WARN\n",
            "5.4000e-01              (2,)      (0, 1, 2)     3.6054e-05                  3.6054e-05         WARN\n",
            "5.5000e-01              (2,)      (0, 1, 2)     3.6095e-05                  3.6095e-05         WARN\n",
            "5.6000e-01              (2,)      (0, 1, 2)     3.6136e-05                  3.6136e-05         WARN\n",
            "5.7000e-01              (2,)      (0, 1, 2)     3.6176e-05                  3.6176e-05         WARN\n",
            "5.8000e-01              (2,)      (0, 1, 2)     3.6217e-05                  3.6217e-05         WARN\n",
            "5.9000e-01              (2,)      (0, 1, 2)     3.6258e-05                  3.6258e-05         WARN\n",
            "6.0000e-01              (2,)      (0, 1, 2)     3.6298e-05                  3.6298e-05         WARN\n",
            "6.1000e-01              (2,)      (0, 1, 2)     3.6339e-05                  3.6339e-05         WARN\n",
            "6.2000e-01              (2,)      (0, 1, 2)     3.6379e-05                  3.6379e-05         WARN\n",
            "6.3000e-01              (2,)      (0, 1, 2)     3.6420e-05                  3.6420e-05         WARN\n",
            "6.4000e-01              (2,)      (0, 1, 2)     3.6460e-05                  3.6460e-05         WARN\n",
            "6.5000e-01              (2,)      (0, 1, 2)     3.6501e-05                  3.6501e-05         WARN\n",
            "6.6000e-01              (2,)      (0, 1, 2)     3.6541e-05                  3.6541e-05         WARN\n",
            "6.7000e-01              (2,)      (0, 1, 2)     3.6581e-05                  3.6581e-05         WARN\n",
            "6.8000e-01              (2,)      (0, 1, 2)     3.6622e-05                  3.6622e-05         WARN\n",
            "6.9000e-01              (2,)      (0, 1, 2)     3.6662e-05                  3.6662e-05         WARN\n",
            "7.0000e-01              (2,)      (0, 1, 2)     3.6702e-05                  3.6702e-05         WARN\n",
            "7.1000e-01              (2,)      (0, 1, 2)     3.6742e-05                  3.6742e-05         WARN\n",
            "7.2000e-01              (2,)      (0, 1, 2)     3.6782e-05                  3.6782e-05         WARN\n",
            "7.3000e-01              (2,)      (0, 1, 2)     3.6822e-05                  3.6822e-05         WARN\n",
            "7.4000e-01              (2,)      (0, 1, 2)     3.6862e-05                  3.6862e-05         WARN\n",
            "7.5000e-01              (2,)      (0, 1, 2)     3.6902e-05                  3.6902e-05         WARN\n",
            "7.6000e-01              (2,)      (0, 1, 2)     3.6942e-05                  3.6942e-05         WARN\n",
            "7.7000e-01              (2,)      (0, 1, 2)     3.6982e-05                  3.6982e-05         WARN\n",
            "7.8000e-01              (2,)      (0, 1, 2)     3.7022e-05                  3.7022e-05         WARN\n",
            "7.9000e-01              (2,)      (0, 1, 2)     3.7061e-05                  3.7061e-05         WARN\n",
            "8.0000e-01              (2,)      (0, 1, 2)     3.7101e-05                  3.7101e-05         WARN\n",
            "8.1000e-01              (2,)      (0, 1, 2)     3.7141e-05                  3.7141e-05         WARN\n",
            "8.2000e-01              (2,)      (0, 1, 2)     3.7180e-05                  3.7180e-05         WARN\n",
            "8.3000e-01              (2,)      (0, 1, 2)     3.7220e-05                  3.7220e-05         WARN\n",
            "8.4000e-01              (2,)      (0, 1, 2)     3.7259e-05                  3.7259e-05         WARN\n",
            "8.5000e-01              (2,)      (0, 1, 2)     3.7299e-05                  3.7299e-05         WARN\n",
            "8.6000e-01              (2,)      (0, 1, 2)     3.7338e-05                  3.7338e-05         WARN\n",
            "8.7000e-01              (2,)      (0, 1, 2)     3.7377e-05                  3.7377e-05         WARN\n",
            "8.8000e-01              (2,)      (0, 1, 2)     3.7417e-05                  3.7417e-05         WARN\n",
            "8.9000e-01              (2,)      (0, 1, 2)     3.7456e-05                  3.7456e-05         WARN\n",
            "9.0000e-01              (2,)      (0, 1, 2)     3.7495e-05                  3.7495e-05         WARN\n",
            "9.1000e-01              (2,)      (0, 1, 2)     3.7534e-05                  3.7534e-05         WARN\n",
            "9.2000e-01              (2,)      (0, 1, 2)     3.7573e-05                  3.7573e-05         WARN\n",
            "9.3000e-01              (2,)      (0, 1, 2)     3.7612e-05                  3.7612e-05         WARN\n",
            "9.4000e-01              (2,)      (0, 1, 2)     3.7651e-05                  3.7651e-05         WARN\n",
            "9.5000e-01              (2,)      (0, 1, 2)     3.7690e-05                  3.7690e-05         WARN\n",
            "9.6000e-01              (2,)      (0, 1, 2)     3.7729e-05                  3.7729e-05         WARN\n",
            "9.7000e-01              (2,)      (0, 1, 2)     3.7768e-05                  3.7768e-05         WARN\n",
            "9.8000e-01              (2,)      (0, 1, 2)     3.7807e-05                  3.7807e-05         WARN\n",
            "9.9000e-01              (2,)      (0, 1, 2)     3.7846e-05                  3.7846e-05         WARN\n",
            "1.0000e+00              (2,)      (0, 1, 2)     3.7884e-05                  3.7884e-05         WARN\n",
            "1.0100e+00              (2,)      (0, 1, 2)     3.7923e-05                  3.7923e-05         WARN\n",
            "1.0200e+00              (2,)      (0, 1, 2)     3.8545e-05                  3.8545e-05         WARN\n",
            "1.0300e+00              (2,)      (0, 1, 2)     3.9281e-05                  3.9281e-05         WARN\n",
            "1.0400e+00              (2,)      (0, 1, 2)     4.0016e-05                  4.0016e-05         WARN\n",
            "1.0500e+00              (2,)      (0, 1, 2)     4.0751e-05                  4.0751e-05         WARN\n",
            "1.0600e+00              (2,)      (0, 1, 2)     4.1485e-05                  4.1485e-05         WARN\n",
            "1.0700e+00              (2,)      (0, 1, 2)     4.2219e-05                  4.2219e-05         WARN\n",
            "1.0800e+00              (2,)      (0, 1, 2)     4.2953e-05                  4.2953e-05         WARN\n",
            "1.0900e+00              (2,)      (0, 1, 2)     4.3687e-05                  4.3687e-05         WARN\n",
            "1.1000e+00              (2,)      (0, 1, 2)     4.4420e-05                  4.4420e-05         WARN\n",
            "1.1100e+00              (2,)      (0, 1, 2)     4.5153e-05                  4.5153e-05         WARN\n",
            "1.1200e+00              (2,)      (0, 1, 2)     4.5886e-05                  4.5886e-05         WARN\n",
            "1.1300e+00              (2,)      (0, 1, 2)     4.6618e-05                  4.6618e-05         WARN\n",
            "1.1400e+00              (2,)      (0, 1, 2)     4.7350e-05                  4.7350e-05         WARN\n",
            "1.1500e+00              (2,)      (0, 1, 2)     4.8082e-05                  4.8082e-05         WARN\n",
            "1.1600e+00              (2,)      (0, 1, 2)     4.8813e-05                  4.8813e-05         WARN\n",
            "1.1700e+00              (2,)      (0, 1, 2)     4.9544e-05                  4.9544e-05         WARN\n",
            "1.1800e+00              (2,)      (0, 1, 2)     5.0275e-05                  5.0275e-05         WARN\n",
            "1.1900e+00              (2,)      (0, 1, 2)     5.1005e-05                  5.1005e-05         WARN\n",
            "1.2000e+00              (2,)      (0, 1, 2)     5.1735e-05                  5.1735e-05         WARN\n",
            "1.2100e+00              (2,)      (0, 1, 2)     5.2465e-05                  5.2465e-05         WARN\n",
            "1.2200e+00              (2,)      (0, 1, 2)     5.3194e-05                  5.3194e-05         WARN\n",
            "1.2300e+00              (2,)      (0, 1, 2)     5.3924e-05                  5.3924e-05         WARN\n",
            "1.2400e+00              (2,)      (0, 1, 2)     5.4652e-05                  5.4652e-05         WARN\n",
            "1.2500e+00              (2,)      (0, 1, 2)     5.5381e-05                  5.5381e-05         WARN\n",
            "1.2600e+00              (2,)      (0, 1, 2)     5.6109e-05                  5.6109e-05         WARN\n",
            "1.2700e+00              (2,)      (0, 1, 2)     5.6837e-05                  5.6837e-05         WARN\n",
            "1.2800e+00              (2,)      (0, 1, 2)     5.7564e-05                  5.7564e-05         WARN\n",
            "1.2900e+00              (2,)      (0, 1, 2)     5.8291e-05                  5.8291e-05         WARN\n",
            "1.3000e+00              (2,)      (0, 1, 2)     5.9018e-05                  5.9018e-05         WARN\n",
            "1.3100e+00              (2,)      (0, 1, 2)     5.9745e-05                  5.9745e-05         WARN\n",
            "1.3200e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.3300e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.3400e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.3500e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.3600e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.3700e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.3800e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.3900e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.4000e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.4100e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.4200e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.4300e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.4400e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.4500e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.4600e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.4700e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.4800e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.4900e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "1.5000e+00              (2,)           (2,)     0.0000e+00                  0.0000e+00           OK\n",
            "Notes on KKT Check:\n",
            " - active_set_report: Active set reported by inner solver\n",
            " - active_set_kkt: Active set inferred from w* > tol\n",
            " - max_violation: Max KKT violation (dual infeasibility or comp. slackness based on eta estimate)\n",
            " - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\n",
            "\n",
            "Total execution time: 48.14 seconds\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "4_14_full_range_activeset_mu003.py\n",
        "\n",
        "Analyzes the change of w*(alpha), H*(alpha), pi*(alpha), grad H*(alpha),\n",
        "lambda*(alpha), and active_set*(alpha) over the full alpha range [0.0, 1.5],\n",
        "using the original active set method for the inner QP solve, with mu_tilde = 0.03.\n",
        "Includes final solve step in FW for consistency and relaxed norm check in gradient.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from scipy.linalg import solve, LinAlgError, eigh, inv, qr\n",
        "from scipy.optimize import linprog, OptimizeWarning\n",
        "import traceback\n",
        "import time\n",
        "import itertools\n",
        "import os\n",
        "import math\n",
        "from collections import Counter # \n",
        "\n",
        "# pandas \n",
        "try:\n",
        "    import pandas as pd\n",
        "    PANDAS_AVAILABLE = True\n",
        "    pd.set_option('display.width', 160)\n",
        "    pd.set_option('display.float_format', '{:.6e}'.format) # \n",
        "    pd.set_option('display.max_rows', 200) # \n",
        "except ImportError:\n",
        "    PANDAS_AVAILABLE = False\n",
        "    print(\"Warning: pandas library not found. Output formatting will be basic. CSV export disabled.\")\n",
        "\n",
        "# ===  ===\n",
        "DEFAULT_TOLERANCE = 1e-9\n",
        "\n",
        "# --- OptimizationResult  (active_set_opt ) ---\n",
        "class OptimizationResult:\n",
        "    def __init__(self, success, message, w_opt=None, pi_opt=None, lambda_opt=None,\n",
        "                 H_opt=None, grad_H_opt=None, iterations=None, fw_gap=None,\n",
        "                 active_set_opt=None): # <-- active_set_opt \n",
        "        self.success = success; self.message = message; self.w_opt = w_opt; self.pi_opt = pi_opt\n",
        "        self.lambda_opt = lambda_opt; self.H_opt = H_opt; self.grad_H_opt = grad_H_opt\n",
        "        self.iterations = iterations; self.fw_gap = fw_gap\n",
        "        self.active_set_opt = active_set_opt # <-- active_set_opt \n",
        "\n",
        "# --- find_feasible_initial_pi  ---\n",
        "def find_feasible_initial_pi(R, mu_tilde, K, tolerance=1e-8):\n",
        "    M = R.shape[1]; c = np.zeros(K + 1); c[K] = 1.0\n",
        "    A_ub = np.hstack((-R.T, -np.ones((M, 1)))); b_ub = -mu_tilde * np.ones(M)\n",
        "    bounds = [(None, None)] * K + [(0, None)]; opts = {'tol': tolerance, 'disp': False, 'presolve': True}\n",
        "    result = None; methods_to_try = ['highs', 'highs-ipm', 'highs-ds', 'simplex']\n",
        "    for method in methods_to_try:\n",
        "        try:\n",
        "            with warnings.catch_warnings(): warnings.filterwarnings(\"ignore\"); result = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=method, options=opts)\n",
        "            if result.success: break\n",
        "        except ValueError: continue\n",
        "        except Exception as e: return None, False, f\"Phase 1 LP failed: {e}\"\n",
        "    if result is None or not result.success: msg = result.message if result else \"No solver\"; status = result.status if result else -1; return None, False, f\"Phase 1 LP solver failed: {msg} (status={status})\"\n",
        "    s = result.x[K]; pi = result.x[:K]\n",
        "    if np.isnan(pi).any(): return None, False, \"Phase 1 LP resulted in NaN values for pi.\"\n",
        "    if s <= tolerance * 1000:\n",
        "        G = -R.T; h = -mu_tilde * np.ones(M); violation = np.max(G @ pi - h)\n",
        "        if violation <= tolerance * 10000: return pi, True, f\"Phase 1 OK (s*={s:.1e}, vio={violation:.1e})\"\n",
        "        else: return pi, True, f\"Phase 1 OK (s*={s:.1e}), WARN: violation {violation:.1e}\"\n",
        "    else: return None, False, f\"Phase 1 Infeasible (s* = {s:.1e})\"\n",
        "\n",
        "# --- solve_kkt_system  ---\n",
        "def solve_kkt_system(Q, G_W, g, tolerance=DEFAULT_TOLERANCE):\n",
        "    K = Q.shape[0]; n_act = G_W.shape[0] if G_W is not None and G_W.ndim == 2 and G_W.shape[0] > 0 else 0\n",
        "    if n_act == 0:\n",
        "        try: p = solve(Q, -g, assume_a='sym'); l = np.array([])\n",
        "        except LinAlgError: return None, None, False\n",
        "        res_norm = np.linalg.norm(Q @ p + g); g_norm = np.linalg.norm(g); solved_ok = res_norm <= tolerance * 1e3 * (1 + g_norm)\n",
        "        return p, l, solved_ok\n",
        "    else:\n",
        "        kkt_mat = None; rhs = None\n",
        "        try: kkt_mat = np.block([[Q, G_W.T], [G_W, np.zeros((n_act, n_act))]]) ; rhs = np.concatenate([-g, np.zeros(n_act)])\n",
        "        except ValueError as e: return None, None, False\n",
        "        try: sol = solve(kkt_mat, rhs, assume_a='sym'); p = sol[:K]; l = sol[K:]\n",
        "        except LinAlgError: return None, None, False\n",
        "        except ValueError as e: return None, None, False\n",
        "        res_norm = np.linalg.norm(kkt_mat @ sol - rhs); rhs_norm = np.linalg.norm(rhs); solved_ok = res_norm <= tolerance * 1e3 * (1 + rhs_norm)\n",
        "        return p, l, solved_ok\n",
        "\n",
        "# --- solve_inner_qp_active_set  ---\n",
        "def solve_inner_qp_active_set(Vw, R, mu_tilde, initial_pi, max_iter=350, tolerance=DEFAULT_TOLERANCE, regularization_epsilon=1e-10):\n",
        "    K = Vw.shape[0]; M = R.shape[1]; Q_reg = 2 * Vw + 2 * regularization_epsilon * np.eye(K); G = -R.T; h = -mu_tilde * np.ones(M)\n",
        "    if initial_pi is None: return None, None, None, None, None, False, \"No initial pi\"\n",
        "    pi_k = np.copy(initial_pi); lam_opt = np.zeros(M); W = set()\n",
        "    active_tol = tolerance * 10\n",
        "    initial_violations = G @ pi_k - h; W = set(j for j, viol in enumerate(initial_violations) if viol > -active_tol)\n",
        "    active_indices_opt = None; kkt_matrix_opt = None\n",
        "    if np.any(initial_violations > active_tol * 10): warnings.warn(f\"Initial pi infeasible (max viol: {np.max(initial_violations):.2e}).\")\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        g_k = Q_reg @ pi_k; act = sorted(list(W)); n_act = len(act); G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "        p_k, lam_Wk, solved = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "        if not solved or p_k is None: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: KKT solve failed. ActiveSet={act}\"\n",
        "        if np.linalg.norm(p_k) <= tolerance * 10 * (1 + np.linalg.norm(pi_k)):\n",
        "            is_optimal_point = True; blocking_constraint_idx = -1; min_negative_lambda = float('inf')\n",
        "            dual_feas_tol = -tolerance * 10\n",
        "            if W:\n",
        "                if lam_Wk is None or len(lam_Wk) != n_act: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: lam_Wk inconsistent? ActiveSet={act}\"\n",
        "                lambda_map = dict(zip(act, lam_Wk))\n",
        "                for constraint_idx, lagrange_multiplier in lambda_map.items():\n",
        "                    if lagrange_multiplier < dual_feas_tol: is_optimal_point = False;\n",
        "                    if lagrange_multiplier < min_negative_lambda: min_negative_lambda = lagrange_multiplier; blocking_constraint_idx = constraint_idx\n",
        "            if is_optimal_point:\n",
        "                lam_opt.fill(0.0);\n",
        "                if W and len(lam_Wk) == n_act: lam_opt[act] = np.maximum(lam_Wk, 0)\n",
        "                final_infeas = np.max(G @ pi_k - h); msg = f\"Optimal found at iter {i+1}.\"\n",
        "                if final_infeas > active_tol: msg += f\" (WARN: violation {final_infeas:.1e})\"\n",
        "                active_indices_opt = act\n",
        "                # Vw_reg \n",
        "                return pi_k, lam_opt, active_indices_opt, None, Q_reg / 2.0, True, msg\n",
        "            else:\n",
        "                if blocking_constraint_idx in W: W.remove(blocking_constraint_idx); continue\n",
        "                else: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Neg lambda idx {blocking_constraint_idx}, not in W={act}\"\n",
        "        else:\n",
        "            alpha_k = 1.0; blocking_constraint_idx = -1; min_step_length = float('inf')\n",
        "            step_tol = tolerance * 10\n",
        "            for j in range(M):\n",
        "                if j not in W:\n",
        "                    constraint_gradient_dot_p = G[j, :] @ p_k\n",
        "                    if constraint_gradient_dot_p > step_tol:\n",
        "                        distance_to_boundary = h[j] - (G[j, :] @ pi_k)\n",
        "                        if abs(constraint_gradient_dot_p) > 1e-15:\n",
        "                            alpha_j = distance_to_boundary / constraint_gradient_dot_p\n",
        "                            step_j = max(0.0, alpha_j)\n",
        "                            if step_j < min_step_length:\n",
        "                                min_step_length = step_j; blocking_constraint_idx = j\n",
        "            alpha_k = min(1.0, min_step_length); pi_k += alpha_k * p_k\n",
        "            if alpha_k < 1.0 - step_tol and blocking_constraint_idx != -1:\n",
        "                if blocking_constraint_idx not in W: W.add(blocking_constraint_idx)\n",
        "            continue\n",
        "    msg = f\"Max iter ({max_iter}) reached.\"; final_infeas = np.max(G @ pi_k - h)\n",
        "    if final_infeas > active_tol * 10: return None, None, None, None, None, False, f\"{msg} Final infeasible. ActiveSet={sorted(list(W))}\"\n",
        "    act = sorted(list(W)); n_act = len(act); G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "    is_likely_optimal = False; active_constraints_opt = act\n",
        "    g_k = Q_reg @ pi_k; p_f, lam_f, solved_f = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "    final_lambda_estimate = np.zeros(M)\n",
        "    if solved_f and p_f is not None and np.linalg.norm(p_f) <= tolerance * 100 * (1 + np.linalg.norm(pi_k)):\n",
        "        if n_act > 0 and lam_f is not None and len(lam_f) == n_act:\n",
        "             try: final_lambda_estimate[act] = lam_f\n",
        "             except IndexError: pass\n",
        "        active_lambdas = final_lambda_estimate[act] if n_act > 0 else np.array([])\n",
        "        if n_act == 0 or np.all(active_lambdas >= -tolerance * 100): is_likely_optimal = True; msg += \" Final KKT check approx OK.\"\n",
        "        else: msg += \" Final KKT check fails (dual infeasible).\"\n",
        "    else: msg += \" Final KKT check fails (stationarity or solve error).\"\n",
        "    lam_opt = final_lambda_estimate\n",
        "    # Vw_reg \n",
        "    return pi_k, lam_opt, active_constraints_opt, None, Q_reg / 2.0, is_likely_optimal, msg\n",
        "\n",
        "# --- make_psd  ---\n",
        "def make_psd(matrix, tolerance=1e-8):\n",
        "    sym = (matrix + matrix.T) / 2\n",
        "    try: eigenvalues, eigenvectors = np.linalg.eigh(sym); min_eigenvalue = np.min(eigenvalues)\n",
        "    except LinAlgError: warnings.warn(\"...\"); return sym\n",
        "    if min_eigenvalue < tolerance: eigenvalues[eigenvalues < tolerance] = tolerance; psd_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T; return (psd_matrix + psd_matrix.T) / 2\n",
        "    else: return sym\n",
        "\n",
        "# --- calculate_Vw  ---\n",
        "def calculate_Vw(w, R, SecondMoments_a_array, tolerance=DEFAULT_TOLERANCE, psd_tolerance=1e-8):\n",
        "    K, M = R.shape; w_sum = np.sum(w); w_norm = w\n",
        "    EwX = R @ w_norm; EwXXT = np.zeros((K, K));\n",
        "    for m in range(M): EwXXT += w_norm[m] * SecondMoments_a_array[m]\n",
        "    Vw = EwXXT - np.outer(EwX, EwX); Vw_psd = make_psd(Vw, psd_tolerance)\n",
        "    return Vw_psd, EwX, EwXXT\n",
        "\n",
        "# --- calculate_H_gradient  () ---\n",
        "def calculate_H_gradient(pi_star, w, R, SecondMoments_a_array, EwX, EwXXT, tolerance=1e-9, debug_print=False):\n",
        "    M = w.shape[0]; K = R.shape[0]; grad = np.zeros(M)\n",
        "    norm_tolerance = 1e-12 # Allow very small pi_star\n",
        "\n",
        "    if pi_star is None or EwX is None:\n",
        "        if debug_print: print(\"DEBUG grad_H: Returning NaN due to None input (pi_star or EwX).\")\n",
        "        return np.full(M, np.nan)\n",
        "    if np.isnan(pi_star).any() or np.isinf(pi_star).any():\n",
        "         if debug_print: print(\"DEBUG grad_H: Returning NaN because pi_star contains NaN/Inf.\")\n",
        "         return np.full(M, np.nan)\n",
        "    pi_norm = np.linalg.norm(pi_star)\n",
        "    if pi_norm < norm_tolerance:\n",
        "         if debug_print: print(f\"DEBUG grad_H: Returning NaN because pi_star norm {pi_norm:.2e} < {norm_tolerance:.1e}\")\n",
        "         return np.full(M, np.nan)\n",
        "    if np.isnan(EwX).any() or np.isinf(EwX).any():\n",
        "        if debug_print: print(\"DEBUG grad_H: Returning NaN because EwX contains NaN/Inf.\")\n",
        "        return np.full(M, np.nan)\n",
        "\n",
        "\n",
        "    try: # \n",
        "        pi_T_EwX = pi_star.T @ EwX\n",
        "        if np.isnan(pi_T_EwX) or np.isinf(pi_T_EwX):\n",
        "             if debug_print: print(f\"DEBUG grad_H: pi_T_EwX is NaN/Inf: {pi_T_EwX}\")\n",
        "             return np.full(M, np.nan)\n",
        "\n",
        "        for j in range(M):\n",
        "            Sigma_j = SecondMoments_a_array[j]; r_j = R[:, j]\n",
        "            pi_T_Sigma_j_pi = pi_star.T @ Sigma_j @ pi_star\n",
        "            pi_T_r_j = pi_star.T @ r_j\n",
        "\n",
        "            if np.isnan(pi_T_Sigma_j_pi) or np.isinf(pi_T_Sigma_j_pi):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_Sigma_j_pi is NaN/Inf: {pi_T_Sigma_j_pi}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "            if np.isnan(pi_T_r_j) or np.isinf(pi_T_r_j):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_r_j is NaN/Inf: {pi_T_r_j}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "\n",
        "            term2 = 2 * pi_T_r_j * pi_T_EwX\n",
        "            if np.isnan(term2) or np.isinf(term2):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): term2 (2*pi_T_r_j*pi_T_EwX) is NaN/Inf: {term2}, pi_T_r_j={pi_T_r_j}, pi_T_EwX={pi_T_EwX}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "\n",
        "            grad[j] = pi_T_Sigma_j_pi - term2\n",
        "            if np.isnan(grad[j]) or np.isinf(grad[j]):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): final grad[{j}] is NaN/Inf\")\n",
        "\n",
        "    except Exception as e_calc:\n",
        "         print(f\"ERROR in calculate_H_gradient calculation: {e_calc}\")\n",
        "         print(traceback.format_exc())\n",
        "         return np.full(M, np.nan)\n",
        "\n",
        "    if np.any(np.isnan(grad)) or np.any(np.isinf(grad)):\n",
        "        warnings.warn(f\"NaN or Inf detected in calculated gradient for w={w}\")\n",
        "        if debug_print: print(f\"DEBUG grad_H: Final check found NaN/Inf in gradient: {grad}\")\n",
        "        return np.full(M, np.nan)\n",
        "    return grad\n",
        "\n",
        "# --- project_to_simplex  ---\n",
        "def project_to_simplex(v, z=1):\n",
        "    n_features = v.shape[0];\n",
        "    if n_features == 0: return np.array([])\n",
        "    v_arr = np.asarray(v)\n",
        "    if np.all(v_arr >= -1e-9) and np.isclose(np.sum(v_arr), z): return np.maximum(v_arr, 0)\n",
        "    u = np.sort(v_arr)[::-1]; cssv = np.cumsum(u) - z; ind = np.arange(n_features) + 1; cond = u - cssv / ind > 0\n",
        "    if np.any(cond): rho = ind[cond][-1]; theta = cssv[rho - 1] / float(rho); w = np.maximum(v_arr - theta, 0)\n",
        "    else:\n",
        "         w = np.zeros(n_features)\n",
        "         if z > 0: w[np.argmax(v_arr)] = z\n",
        "    w_sum = np.sum(w)\n",
        "    if not np.isclose(w_sum, z):\n",
        "        if w_sum > 1e-9: w = w * (z / w_sum)\n",
        "        elif z > 0 :\n",
        "            w = np.zeros(n_features)\n",
        "            w[np.argmax(v_arr)] = z\n",
        "    return np.maximum(w, 0)\n",
        "\n",
        "# --- frank_wolfe_optimizer  (ActiveSet) ---\n",
        "def frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde, initial_w=None, max_outer_iter=250, fw_gap_tol=1e-7, inner_max_iter=350, tolerance=1e-9, psd_make_tolerance=1e-8, qp_regularization=1e-10, debug_print=False, force_iterations=0, return_history=False):\n",
        "    K, M = R_alpha.shape\n",
        "    if initial_w is None: w_k = np.ones(M) / M\n",
        "    else: w_k = project_to_simplex(np.copy(initial_w))\n",
        "    if w_k is None:\n",
        "        result = OptimizationResult(False, \"Initial projection failed\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "    fw_gap = float('inf'); best_w = np.copy(w_k); best_pi = None; best_lam = np.zeros(M); best_H = -float('inf'); final_gHk = np.zeros(M); best_active_set = None\n",
        "    pi0, ok, p1msg = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance)\n",
        "    if not ok:\n",
        "        result = OptimizationResult(False, f\"Phase 1 failed: {p1msg}\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "    current_w=np.copy(w_k); current_H=-float('inf'); current_pi=None; current_lam=None; current_active_set=None\n",
        "    inner_solver_args_for_loop = {'max_iter': inner_max_iter, 'tolerance': tolerance, 'regularization_epsilon': qp_regularization}\n",
        "    history = []\n",
        "\n",
        "    last_successful_pi = pi0\n",
        "    last_successful_lam = np.zeros(M)\n",
        "    last_successful_active_set = tuple()\n",
        "    last_successful_gHk = np.zeros(M)\n",
        "    last_successful_fw_gap = float('inf')\n",
        "\n",
        "    for k in range(max_outer_iter):\n",
        "        iter_data = {'k': k + 1}\n",
        "        if return_history: iter_data['w_k'] = np.copy(w_k)\n",
        "        try: Vk, Ex, ExxT = calculate_Vw(w_k, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "        except Exception as e:\n",
        "            result = OptimizationResult(False, f\"Outer iter {k+1}: Vw failed: {e}\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, active_set_opt=best_active_set)\n",
        "            if return_history: history.append(iter_data); return (result, history)\n",
        "            else: return result\n",
        "\n",
        "        # === Use Active Set for inner solve ===\n",
        "        pi_init_inner = last_successful_pi if last_successful_pi is not None else pi0\n",
        "        pk, lk, act_idx_k, _, Vw_reg_k, inner_ok, inner_msg = solve_inner_qp_active_set(\n",
        "            Vk, R_alpha, mu_tilde, pi_init_inner, **inner_solver_args_for_loop\n",
        "        )\n",
        "        # ===================================\n",
        "\n",
        "        if not inner_ok or pk is None:\n",
        "            warnings.warn(f\"Outer iter {k+1}: Inner QP failed: {inner_msg}. Using last successful state if available.\")\n",
        "            if last_successful_pi is None:\n",
        "                 result = OptimizationResult(False, f\"Outer iter {k+1}: Inner QP failed and no prior success: {inner_msg}\",\n",
        "                                             w_opt=current_w, iterations=k)\n",
        "                 if return_history: history.append(iter_data); return (result, history)\n",
        "                 else: return result\n",
        "            pk = last_successful_pi\n",
        "            lk = last_successful_lam\n",
        "            act_idx_k = list(last_successful_active_set)\n",
        "            final_gHk = last_successful_gHk\n",
        "            fw_gap = last_successful_fw_gap\n",
        "            try: Hk = pk.T @ Vk @ pk\n",
        "            except: Hk = current_H\n",
        "        else:\n",
        "            last_successful_pi = pk\n",
        "            last_successful_lam = lk if lk is not None else np.zeros(M)\n",
        "            last_successful_active_set = tuple(sorted(act_idx_k)) if act_idx_k is not None else tuple()\n",
        "            Hk = pk.T @ Vk @ pk\n",
        "            try:\n",
        "                gHk = calculate_H_gradient(pk, w_k, R_alpha, SecondMoments_alpha_array, Ex, ExxT, tolerance, debug_print=debug_print)\n",
        "            except Exception as e:\n",
        "                result = OptimizationResult(False, f\"Outer iter {k+1}: Grad failed: {e}\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "                if return_history: history.append(iter_data); return (result, history)\n",
        "                else: return result\n",
        "\n",
        "            final_gHk = gHk if gHk is not None else np.full(M, np.nan) # Return NaN if calculation fails\n",
        "            last_successful_gHk = final_gHk if not np.isnan(final_gHk).any() else last_successful_gHk # Only update if valid\n",
        "\n",
        "        if return_history: iter_data['H_k'] = Hk\n",
        "        if return_history: iter_data['grad_H_k_norm'] = np.linalg.norm(final_gHk) if not np.isnan(final_gHk).any() else np.nan\n",
        "\n",
        "        current_w = np.copy(w_k); current_H = Hk; current_pi = pk; current_lam = last_successful_lam; current_active_set = last_successful_active_set\n",
        "\n",
        "        if Hk >= best_H - tolerance*1000:\n",
        "             best_H = Hk; best_w = np.copy(w_k); best_pi = np.copy(pk); best_lam = np.copy(current_lam); best_active_set = current_active_set\n",
        "\n",
        "        # --- Check for NaN gradient ---\n",
        "        if final_gHk is None or np.isnan(final_gHk).any():\n",
        "            result = OptimizationResult(False, f\"Outer iter {k+1}: Grad NaN.\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "            if return_history: history.append(iter_data); return (result, history)\n",
        "            else: return result\n",
        "        # --- End Check ---\n",
        "\n",
        "        grad_norm = np.linalg.norm(final_gHk)\n",
        "        if grad_norm < tolerance * 10: sk = w_k; sk_idx = -1\n",
        "        else: sk_idx = np.argmax(final_gHk); sk = np.zeros(M); sk[sk_idx] = 1.0\n",
        "        if return_history: iter_data['s_k_index'] = sk_idx\n",
        "\n",
        "        if w_k is None:\n",
        "             result = OptimizationResult(False, f\"k={k} w_k None\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             if return_history: history.append(iter_data); return (result, history)\n",
        "             else: return result\n",
        "\n",
        "        fw_gap = final_gHk.T @ (w_k - sk)\n",
        "        last_successful_fw_gap = fw_gap\n",
        "        if return_history: iter_data['fw_gap'] = fw_gap\n",
        "        gamma = 2.0 / (k + 3.0)\n",
        "        if return_history: iter_data['gamma_k'] = gamma\n",
        "        if return_history: history.append(iter_data)\n",
        "\n",
        "        converged = False\n",
        "        if k >= force_iterations and not np.isnan(fw_gap):\n",
        "            if abs(fw_gap) <= fw_gap_tol: converged = True; conv_msg = f\"Converged (Gap {abs(fw_gap):.2e})\"\n",
        "        if converged:\n",
        "            # --- Final Inner Solve for Consistency ---\n",
        "            final_w = current_w\n",
        "            final_pi_result = current_pi\n",
        "            final_lam_result = current_lam\n",
        "            final_active_set_result = current_active_set\n",
        "            final_H_result = current_H\n",
        "            final_grad_result = final_gHk\n",
        "            final_fw_gap_result = fw_gap\n",
        "\n",
        "            try:\n",
        "                final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "                pi0_final, ok_final, _ = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance)\n",
        "                if ok_final:\n",
        "                    pi_s_final, lam_s_final, act_idx_final, _, _, inner_ok_final, msg_final = solve_inner_qp_active_set(\n",
        "                        final_Vk, R_alpha, mu_tilde, pi0_final, # Use stable pi0 for final check\n",
        "                        **inner_solver_args_for_loop\n",
        "                    )\n",
        "                    if inner_ok_final and pi_s_final is not None and lam_s_final is not None:\n",
        "                        final_pi_result = pi_s_final\n",
        "                        final_lam_result = lam_s_final\n",
        "                        final_active_set_result = tuple(sorted(act_idx_final)) if act_idx_final is not None else tuple()\n",
        "                        final_grad_result = calculate_H_gradient(final_pi_result, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance, debug_print=False)\n",
        "                        if final_grad_result is None or np.isnan(final_grad_result).any(): final_grad_result = np.zeros(M)\n",
        "                        grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                        if grad_norm_final < tolerance * 10: sk_final = final_w\n",
        "                        else: sk_idx_final = np.argmax(final_grad_result); sk_final = np.zeros(M); sk_final[sk_idx_final] = 1.0\n",
        "                        final_fw_gap_result = final_grad_result.T @ (final_w - sk_final) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                    else:\n",
        "                        warnings.warn(f\"Warning: Final inner QP solve failed after convergence: {msg_final}. Using last iteration's values.\")\n",
        "                else:\n",
        "                    warnings.warn(f\"Warning: Could not find feasible pi0 for final solve after convergence. Using last iteration's values.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Error during final inner solve after convergence: {e}. Using last iteration's values.\")\n",
        "            # --- End of Final Inner Solve ---\n",
        "\n",
        "            result = OptimizationResult(True, conv_msg, w_opt=final_w, pi_opt=final_pi_result, lambda_opt=final_lam_result,\n",
        "                                        H_opt=final_H_result, grad_H_opt=final_grad_result, iterations=k + 1,\n",
        "                                        fw_gap=final_fw_gap_result, active_set_opt=final_active_set_result)\n",
        "            return (result, history) if return_history else result\n",
        "\n",
        "        # --- Prepare for next iteration ---\n",
        "        if w_k is None or sk is None:\n",
        "             result = OptimizationResult(False, f\"k={k} w_k/sk None before update\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             return (result, history) if return_history else result\n",
        "        w_k_next = (1.0 - gamma) * w_k + gamma * sk; w_k = project_to_simplex(w_k_next)\n",
        "        if w_k is None:\n",
        "             result = OptimizationResult(False, f\"k={k} proj None\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             return (result, history) if return_history else result\n",
        "        pi0 = pk # Use last inner solution as initial guess\n",
        "\n",
        "    # Max iter reached, perform final solve for consistency\n",
        "    final_w = current_w\n",
        "    final_pi_result = current_pi\n",
        "    final_lam_result = current_lam\n",
        "    final_active_set_result = current_active_set\n",
        "    final_H_result = current_H\n",
        "    final_grad_result = last_successful_gHk\n",
        "    final_fw_gap_result = last_successful_fw_gap\n",
        "\n",
        "    try:\n",
        "        final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "        pi0_final, ok_final, _ = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance) # Use stable pi0\n",
        "        if ok_final:\n",
        "            pi_s_final, lam_s_final, act_idx_final, _, _, inner_ok_final, msg_final = solve_inner_qp_active_set(\n",
        "                final_Vk, R_alpha, mu_tilde, pi0_final, # <-- Use stable pi0_final\n",
        "                **inner_solver_args_for_loop\n",
        "            )\n",
        "            if inner_ok_final and pi_s_final is not None and lam_s_final is not None:\n",
        "                final_pi_result = pi_s_final\n",
        "                final_lam_result = lam_s_final\n",
        "                final_active_set_result = tuple(sorted(act_idx_final)) if act_idx_final is not None else tuple()\n",
        "                # Recalculate gradient and gap\n",
        "                final_grad_result = calculate_H_gradient(final_pi_result, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance, debug_print=False)\n",
        "                if final_grad_result is None or np.isnan(final_grad_result).any(): final_grad_result = np.zeros(M)\n",
        "                grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                if grad_norm_final < tolerance * 10: sk_final = final_w\n",
        "                else: sk_idx_final = np.argmax(final_grad_result); sk_final = np.zeros(M); sk_final[sk_idx_final] = 1.0\n",
        "                final_fw_gap_result = final_grad_result.T @ (final_w - sk_final) if not np.isnan(final_grad_result).any() else np.nan\n",
        "            else:\n",
        "                warnings.warn(f\"Warning: Final inner QP solve failed after max iter: {msg_final}. Using last iteration's values.\")\n",
        "        else:\n",
        "             warnings.warn(f\"Warning: Could not find feasible pi0 for final solve after max iter. Using last iteration's values.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error during final inner solve after max iter: {e}. Using last iteration's values.\")\n",
        "\n",
        "    if return_history:\n",
        "        grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "        if grad_norm_final < tolerance * 10: sk_idx_final = -1\n",
        "        else: sk_idx_final = np.argmax(final_grad_result) if not np.isnan(final_grad_result).any() else -1\n",
        "        history.append({\n",
        "            'k': max_outer_iter + 1, 'w_k': np.copy(final_w), 'H_k': final_H_result,\n",
        "            'grad_H_k_norm': grad_norm_final, 's_k_index': sk_idx_final, 'fw_gap': final_fw_gap_result,\n",
        "            'gamma_k': np.nan\n",
        "        })\n",
        "\n",
        "    result = OptimizationResult(True, f\"Max Iter ({max_outer_iter})\", w_opt=final_w, pi_opt=final_pi_result, lambda_opt=final_lam_result,\n",
        "                                H_opt=final_H_result, grad_H_opt=final_grad_result, iterations=max_outer_iter,\n",
        "                                fw_gap=final_fw_gap_result, active_set_opt=final_active_set_result)\n",
        "    return (result, history) if return_history else result\n",
        "\n",
        "\n",
        "# --- generate_params_profile_switching  ---\n",
        "def generate_params_profile_switching(alpha, alpha_max, K=5, M=3, R_base=np.array([0.08, 0.07, 0.06, 0.05, 0.04]), sigma_base=np.array([0.18, 0.15, 0.20, 0.12, 0.10]), Corr_base=np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]), good_prof_r_factor=0.05, good_prof_r_offset=0.002, good_prof_s_factor=-0.02, good_prof_corr_factor=1.0, bad_prof_r_factor=-0.02, bad_prof_r_offset=-0.001, bad_prof_s_factor=0.02, bad_prof_corr_offset=0.0, sigma_min_epsilon=1e-4, psd_tolerance=1e-9):\n",
        "    assert alpha >= 0 and K == len(R_base) and K == len(sigma_base) and K == Corr_base.shape[0] and M == 3, \"Dimension mismatch\"\n",
        "    non_diag_indices = np.where(~np.eye(K, dtype=bool)); R_neutral = R_base; sigma_neutral = np.maximum(sigma_min_epsilon, sigma_base); Corr_neutral = make_psd(Corr_base, psd_tolerance)\n",
        "    R_good = R_base + (R_base * good_prof_r_factor + good_prof_r_offset); sigma_good = np.maximum(sigma_min_epsilon, sigma_base + (sigma_base * good_prof_s_factor)); Corr_good_target = np.copy(Corr_base); Corr_good_target[non_diag_indices] *= good_prof_corr_factor; Corr_good = make_psd(Corr_good_target, psd_tolerance)\n",
        "    R_bad = R_base + (R_base * bad_prof_r_factor + bad_prof_r_offset); sigma_bad = np.maximum(sigma_min_epsilon, sigma_base + (sigma_base * bad_prof_s_factor)); Corr_bad_target = np.copy(Corr_base); Corr_bad_target[non_diag_indices] = np.clip(Corr_bad_target[non_diag_indices] + bad_prof_corr_offset, -1.0+psd_tolerance, 1.0-psd_tolerance); Corr_bad = make_psd(Corr_bad_target, psd_tolerance)\n",
        "    beta = np.clip(alpha / alpha_max if alpha_max > 0 else (1.0 if alpha > 0 else 0.0), 0.0, 1.0)\n",
        "    R_alpha = np.zeros((K, M)); sigma_alpha = np.zeros((K, M)); Corr_alpha = [np.eye(K) for _ in range(M)]; Cov_alpha = [np.eye(K) for _ in range(M)]; SecondMoments_a_array = np.zeros((M, K, K))\n",
        "    R_alpha[:, 0] = (1 - beta) * R_good + beta * R_neutral; sigma_alpha[:, 0] = np.maximum(sigma_min_epsilon, (1 - beta) * sigma_good + beta * sigma_neutral); Corr_alpha[0] = make_psd((1 - beta) * Corr_good + beta * Corr_neutral, psd_tolerance)\n",
        "    R_alpha[:, 1] = (1 - beta) * R_bad + beta * R_good; sigma_alpha[:, 1] = np.maximum(sigma_min_epsilon, (1 - beta) * sigma_bad + beta * sigma_good); Corr_alpha[1] = make_psd((1 - beta) * Corr_bad + beta * Corr_good, psd_tolerance)\n",
        "    R_alpha[:, 2] = (1 - beta) * R_neutral + beta * R_bad; sigma_alpha[:, 2] = np.maximum(sigma_min_epsilon, (1 - beta) * sigma_neutral + beta * sigma_bad); Corr_alpha[2] = make_psd((1 - beta) * Corr_neutral + beta * Corr_bad, psd_tolerance)\n",
        "    for m in range(M): sigma_diag_m = np.diag(sigma_alpha[:, m]); Cov_alpha[m] = sigma_diag_m @ Corr_alpha[m] @ sigma_diag_m; SecondMoments_a_array[m, :, :] = Cov_alpha[m] + np.outer(R_alpha[:, m], R_alpha[:, m])\n",
        "    return R_alpha, SecondMoments_a_array\n",
        "\n",
        "# ===  (ActiveSet) ===\n",
        "def analyze_alpha_full_results(alpha_range, param_gen_kwargs, optimizer_kwargs, mu_tilde):\n",
        "    \"\"\"  alpha  FWH*pi*grad H*lambda*active_set*  \"\"\"\n",
        "    results_over_alpha = []\n",
        "    K = param_gen_kwargs.get('K', 5); M = param_gen_kwargs.get('M', 3)\n",
        "\n",
        "    print(\"\\n--- Starting Full Results Analysis over Alpha Range (using Active Set Method) ---\")\n",
        "    total_alphas = len(alpha_range)\n",
        "    start_loop_time = time.time()\n",
        "\n",
        "    for idx, alpha in enumerate(alpha_range):\n",
        "        loop_start_time = time.time()\n",
        "        print(f\"\\rAnalyzing alpha = {alpha:.6f} ({idx+1}/{total_alphas}) ... \", end=\"\")\n",
        "\n",
        "        R_alpha, SecondMoments_alpha_array = generate_params_profile_switching(alpha, **param_gen_kwargs)\n",
        "        alpha_result = {'alpha': alpha}\n",
        "\n",
        "        w_fw_default = np.full(M, np.nan)\n",
        "        H_star_fw = np.nan\n",
        "        pi_opt = np.full(K, np.nan)\n",
        "        grad_H_opt = np.full(M, np.nan)\n",
        "        lambda_opt = np.full(M, np.nan)\n",
        "        active_set_opt = tuple()\n",
        "\n",
        "        try:\n",
        "            # --- FW Optimizer (Active Set) ---\n",
        "            fw_result = frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde,\n",
        "                                              initial_w=None, return_history=False,\n",
        "                                              debug_print=False, # OFF\n",
        "                                              **optimizer_kwargs)\n",
        "            if fw_result.success:\n",
        "                 w_fw_default = fw_result.w_opt if fw_result.w_opt is not None else np.full(M, np.nan)\n",
        "                 H_star_fw = fw_result.H_opt if fw_result.H_opt is not None else np.nan\n",
        "                 pi_opt = fw_result.pi_opt if fw_result.pi_opt is not None else np.full(K, np.nan)\n",
        "                 grad_H_opt = fw_result.grad_H_opt if fw_result.grad_H_opt is not None else np.full(M, np.nan)\n",
        "                 lambda_opt = fw_result.lambda_opt if fw_result.lambda_opt is not None else np.full(M, np.nan)\n",
        "                 active_set_opt = fw_result.active_set_opt if fw_result.active_set_opt is not None else tuple()\n",
        "            else:\n",
        "                 w_fw_default = np.full(M, np.nan)\n",
        "                 H_star_fw = np.nan\n",
        "                 pi_opt = np.full(K, np.nan)\n",
        "                 grad_H_opt = np.full(M, np.nan)\n",
        "                 lambda_opt = np.full(M, np.nan)\n",
        "                 active_set_opt = tuple()\n",
        "                 print(f\"\\n  Warn: FW failed for alpha={alpha:.6f}: {fw_result.message}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n  Error during FW run for alpha={alpha:.6f}: {e}\")\n",
        "            w_fw_default = np.full(M, np.nan)\n",
        "            H_star_fw = np.nan\n",
        "            pi_opt = np.full(K, np.nan)\n",
        "            grad_H_opt = np.full(M, np.nan)\n",
        "            lambda_opt = np.full(M, np.nan)\n",
        "            active_set_opt = tuple()\n",
        "\n",
        "        alpha_result['w_fw_default'] = w_fw_default\n",
        "        alpha_result['H_star'] = H_star_fw\n",
        "        alpha_result['pi_opt'] = pi_opt\n",
        "        alpha_result['grad_H_opt'] = grad_H_opt\n",
        "        alpha_result['lambda_opt'] = lambda_opt\n",
        "        alpha_result['active_set_opt'] = active_set_opt\n",
        "\n",
        "        results_over_alpha.append(alpha_result)\n",
        "\n",
        "    print(\"\\n--- Finished Full Results Analysis ---\")\n",
        "    df_results = pd.DataFrame(results_over_alpha)\n",
        "    if PANDAS_AVAILABLE:\n",
        "        fw_vecs = np.stack([res.get('w_fw_default', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        pi_vecs = np.stack([res.get('pi_opt', np.full(K, np.nan)) for res in results_over_alpha])\n",
        "        grad_vecs = np.stack([res.get('grad_H_opt', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        lambda_vecs = np.stack([res.get('lambda_opt', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        active_sets = [str(res.get('active_set_opt', tuple())) for res in results_over_alpha]\n",
        "\n",
        "        for m in range(M):\n",
        "            df_results[f'w_fw_{m}'] = fw_vecs[:, m]\n",
        "            df_results[f'grad_H_{m}'] = grad_vecs[:, m]\n",
        "            df_results[f'lambda_{m}'] = lambda_vecs[:, m]\n",
        "        for k in range(K):\n",
        "            df_results[f'pi_{k}'] = pi_vecs[:, k]\n",
        "\n",
        "        df_results['active_set'] = active_sets\n",
        "\n",
        "        df_results = df_results.drop(columns=['w_fw_default', 'pi_opt', 'grad_H_opt', 'lambda_opt', 'active_set_opt'], errors='ignore')\n",
        "\n",
        "    return df_results if PANDAS_AVAILABLE else results_over_alpha\n",
        "\n",
        "\n",
        "# ===  ===\n",
        "if __name__ == '__main__':\n",
        "    start_time_main = time.time()\n",
        "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "    warnings.filterwarnings('ignore', category=UserWarning)\n",
        "    warnings.filterwarnings('ignore', category=OptimizeWarning)\n",
        "\n",
        "    # ---  ---\n",
        "    K = 5; M = 3;\n",
        "    #  mu_tilde  0.02  \n",
        "    mu_tilde = 0.02\n",
        "    alpha_max = 1.5\n",
        "    profile_adjustments = { 'good_prof_r_factor': 0.05, 'good_prof_r_offset': 0.002,'good_prof_s_factor': -0.02, 'good_prof_corr_factor': 1.0,'bad_prof_r_factor': -0.02, 'bad_prof_r_offset': -0.001,'bad_prof_s_factor': 0.02, 'bad_prof_corr_offset': 0.0, }\n",
        "    param_gen_kwargs_main = {'K': K, 'M': M, 'alpha_max': alpha_max, **profile_adjustments}\n",
        "    solver_settings = { # \n",
        "        'max_outer_iter': 500, 'fw_gap_tol': 1e-9,\n",
        "        'inner_max_iter': 600, # ActiveSet\n",
        "        'tolerance': 1e-11, # FW\n",
        "        'psd_make_tolerance': 1e-9,\n",
        "        'qp_regularization': 1e-10, # ActiveSet\n",
        "        'force_iterations': 20\n",
        "    }\n",
        "    #   alpha  ( [0.0, 1.5]) \n",
        "    alpha_start = 0.0\n",
        "    alpha_end = 1.5\n",
        "    alpha_step = 0.01 # 0.01\n",
        "    num_alpha_steps = int(round((alpha_end - alpha_start) / alpha_step)) + 1\n",
        "    alpha_range_analyze = np.linspace(alpha_start, alpha_end, num_alpha_steps)\n",
        "    unique_pt_tolerance = 1e-5 # KKT (main )\n",
        "\n",
        "    #  CSV alpha  (0.1) \n",
        "    alpha_step_for_csv = 0.1\n",
        "    alphas_for_csv = np.arange(alpha_start, alpha_end + alpha_step_for_csv / 2, alpha_step_for_csv)\n",
        "\n",
        "    output_csv_filename = f\"alpha_full_results_{alpha_start:.1f}_{alpha_end:.1f}_activeset_mu002_sparse.csv\" # \n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"--- Starting Full Range Analysis [{alpha_start:.1f}, {alpha_end:.1f}] (using ActiveSet, mu_tilde={mu_tilde}) ---\")\n",
        "    print(f\"--- (CSV will be saved for alpha approx every {alpha_step_for_csv}) ---\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # ---  ---\n",
        "    results_df = analyze_alpha_full_results( # \n",
        "        alpha_range=alpha_range_analyze,\n",
        "        param_gen_kwargs=param_gen_kwargs_main,\n",
        "        optimizer_kwargs=solver_settings,\n",
        "        mu_tilde=mu_tilde #   mu_tilde  \n",
        "    )\n",
        "\n",
        "    # ---  & CSV & KKT ---\n",
        "    if PANDAS_AVAILABLE and isinstance(results_df, pd.DataFrame):\n",
        "        print(\"\\n--- Analysis Results Summary (DataFrame - First 5 & Last 5 rows) ---\")\n",
        "        float_format_func = lambda x: f\"{x:.4e}\" if pd.notna(x) and isinstance(x, (float, np.number)) else 'NaN'\n",
        "        pd.options.display.float_format = float_format_func\n",
        "        cols_to_show = ['alpha', 'H_star', 'active_set'] + \\\n",
        "                       [f'w_fw_{m}' for m in range(M) if f'w_fw_{m}' in results_df.columns] + \\\n",
        "                       [f'pi_{k}' for k in range(K) if f'pi_{k}' in results_df.columns] + \\\n",
        "                       [f'grad_H_{m}' for m in range(M) if f'grad_H_{m}' in results_df.columns] + \\\n",
        "                       [f'lambda_{m}' for m in range(M) if f'lambda_{m}' in results_df.columns]\n",
        "        cols_to_show = [col for col in cols_to_show if col in results_df.columns]\n",
        "        with pd.option_context('display.max_rows', 10): # \n",
        "            print(results_df[cols_to_show].to_string(na_rep='NaN'))\n",
        "        pd.reset_option('display.float_format')\n",
        "\n",
        "        # CSV ()\n",
        "        try:\n",
        "            csv_save_indices = []\n",
        "            alpha_csv_tol = alpha_step / 2.1\n",
        "            processed_targets = set()\n",
        "            for alpha_target_raw in alphas_for_csv:\n",
        "                 alpha_target = round(alpha_target_raw, 8)\n",
        "                 if alpha_target in processed_targets: continue\n",
        "                 diffs = (results_df['alpha'] - alpha_target).abs()\n",
        "                 valid_diffs = diffs.dropna()\n",
        "                 if not valid_diffs.empty and valid_diffs.min() <= alpha_csv_tol:\n",
        "                      closest_idx = valid_diffs.idxmin()\n",
        "                      if np.isclose(results_df.loc[closest_idx, 'alpha'], alpha_target, atol=alpha_csv_tol):\n",
        "                           csv_save_indices.append(closest_idx)\n",
        "                           processed_targets.add(alpha_target)\n",
        "\n",
        "            if csv_save_indices:\n",
        "                 unique_indices = sorted(list(set(csv_save_indices)))\n",
        "                 df_to_save = results_df.loc[unique_indices]\n",
        "\n",
        "                 cols_order = ['alpha', 'H_star', 'active_set'] + \\\n",
        "                              [f'w_fw_{m}' for m in range(M) if f'w_fw_{m}' in df_to_save.columns] + \\\n",
        "                              [f'pi_{k}' for k in range(K) if f'pi_{k}' in df_to_save.columns] + \\\n",
        "                              [f'grad_H_{m}' for m in range(M) if f'grad_H_{m}' in df_to_save.columns] + \\\n",
        "                              [f'lambda_{m}' for m in range(M) if f'lambda_{m}' in df_to_save.columns]\n",
        "                 cols_order = [col for col in cols_order if col in df_to_save.columns]\n",
        "                 df_to_save = df_to_save[cols_order]\n",
        "\n",
        "                 df_to_save.to_csv(output_csv_filename, index=False, float_format='%.8e')\n",
        "                 print(f\"\\nFiltered results ({len(df_to_save)} points) saved to: {os.path.abspath(output_csv_filename)}\")\n",
        "                 print(\"Saved alpha values:\", np.round(df_to_save['alpha'].values, 4))\n",
        "            else:\n",
        "                 print(\"\\nWarning: No data points found matching the sparse alpha values for CSV saving.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError saving filtered results to CSV: {e}\")\n",
        "\n",
        "        # KKT ()\n",
        "        print(\"\\n--- Quick KKT Check (Stationarity w.r.t. w) ---\")\n",
        "        kkt_violations = []\n",
        "        # unique_pt_tolerance  main \n",
        "        for index, row in results_df.iterrows():\n",
        "            alpha = row['alpha']\n",
        "            w_star = np.array([row.get(f'w_fw_{m}', np.nan) for m in range(M)]) # Use NaN if not found\n",
        "            grad_h = np.array([row.get(f'grad_H_{m}', np.nan) for m in range(M)])\n",
        "            active_set_str = row.get('active_set', '()') # Active set from inner solver\n",
        "\n",
        "            # Check if necessary data is available\n",
        "            if np.isnan(w_star).any() or np.isnan(grad_h).any() or np.isclose(np.sum(w_star), 0):\n",
        "                kkt_violations.append({\n",
        "                    'alpha': alpha,\n",
        "                    'active_set_report': active_set_str,\n",
        "                    'active_set_kkt': 'NaN',\n",
        "                    'max_violation': np.nan,\n",
        "                    'grad_consistency_violation': np.nan,\n",
        "                    'check_result': 'Skipped (NaN or zero w)'\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            check_tol = solver_settings['tolerance'] * 1e3\n",
        "\n",
        "            max_grad_h = np.max(grad_h)\n",
        "            nu_estimate = max_grad_h # Estimate nu*\n",
        "\n",
        "            stationarity_violation = 0.0\n",
        "            active_indices_kkt = set() # Indices where w_m > tol\n",
        "            for m in range(M):\n",
        "                if w_star[m] > unique_pt_tolerance:\n",
        "                    active_indices_kkt.add(m)\n",
        "\n",
        "            # Check gradient consistency for active indices based on w_star\n",
        "            active_grads = grad_h[list(active_indices_kkt)] if active_indices_kkt else []\n",
        "            grad_consistency_violation = 0.0\n",
        "            if len(active_grads) > 1:\n",
        "                grad_consistency_violation = np.max(active_grads) - np.min(active_grads)\n",
        "                if not np.isclose(grad_consistency_violation, 0.0, atol=check_tol):\n",
        "                     stationarity_violation = max(stationarity_violation, grad_consistency_violation)\n",
        "\n",
        "            # Check if inactive components have smaller gradients (dual feasibility check)\n",
        "            for m in range(M):\n",
        "                if m not in active_indices_kkt: # If w_m is near zero\n",
        "                    nu_for_check = np.min(active_grads) if active_grads else max_grad_h\n",
        "                    eta_m_estimate = nu_for_check - grad_h[m]\n",
        "                    if eta_m_estimate < -check_tol: # Check dual feasibility (eta_m >= 0)\n",
        "                        stationarity_violation = max(stationarity_violation, abs(eta_m_estimate))\n",
        "\n",
        "            kkt_violations.append({\n",
        "                'alpha': alpha,\n",
        "                'active_set_report': active_set_str, # From inner solver\n",
        "                'active_set_kkt': str(tuple(sorted(active_indices_kkt))), # From w*\n",
        "                'max_violation': stationarity_violation,\n",
        "                'grad_consistency_violation': grad_consistency_violation,\n",
        "                'check_result': 'OK' if stationarity_violation < check_tol * 10 else 'WARN' # Simple check result\n",
        "            })\n",
        "\n",
        "        if PANDAS_AVAILABLE and kkt_violations:\n",
        "            df_kkt = pd.DataFrame(kkt_violations)\n",
        "            kkt_cols_order = ['alpha', 'active_set_report','active_set_kkt', 'max_violation', 'grad_consistency_violation', 'check_result']\n",
        "            kkt_cols_order = [col for col in kkt_cols_order if col in df_kkt.columns]\n",
        "            with pd.option_context('display.float_format', '{:.4e}'.format):\n",
        "                print(df_kkt[kkt_cols_order].to_string(index=False, na_rep='NaN'))\n",
        "            print(\"Notes on KKT Check:\")\n",
        "            print(\" - active_set_report: Active set reported by inner solver\")\n",
        "            print(\" - active_set_kkt: Active set inferred from w* > tol\")\n",
        "            print(\" - max_violation: Max KKT violation (dual infeasibility or comp. slackness based on eta estimate)\")\n",
        "            print(\" - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\")\n",
        "        else:\n",
        "            print(\"Could not perform KKT check or pandas not available.\")\n",
        "\n",
        "\n",
        "        # \n",
        "\n",
        "    elif isinstance(results_df, list):\n",
        "        print(\"\\n--- Analysis Results Summary (List - First 5 & Last 5) ---\")\n",
        "        # ()\n",
        "        print(\"\\n(Pandas not available, skipping CSV export)\")\n",
        "\n",
        "    end_time_main = time.time()\n",
        "    print(f\"\\nTotal execution time: {end_time_main - start_time_main:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "4_14_find_internal_solution.py\n",
        "\n",
        "Attempts to find an alpha and parameter setting where the optimal w*\n",
        "has full support (w*_m > 0 for all m), focusing on alpha around 0.5\n",
        "with milder profile differences and mu_tilde = 0.02.\n",
        "Uses the Active Set method for the inner QP solve.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from scipy.linalg import solve, LinAlgError, eigh, inv, qr\n",
        "from scipy.optimize import linprog, OptimizeWarning\n",
        "import traceback\n",
        "import time\n",
        "import itertools\n",
        "import os\n",
        "import math\n",
        "from collections import Counter # \n",
        "\n",
        "# pandas \n",
        "try:\n",
        "    import pandas as pd\n",
        "    PANDAS_AVAILABLE = True\n",
        "    pd.set_option('display.width', 160)\n",
        "    pd.set_option('display.float_format', '{:.6e}'.format) # \n",
        "    pd.set_option('display.max_rows', 200) # \n",
        "except ImportError:\n",
        "    PANDAS_AVAILABLE = False\n",
        "    print(\"Warning: pandas library not found. Output formatting will be basic. CSV export disabled.\")\n",
        "\n",
        "# ===  ===\n",
        "DEFAULT_TOLERANCE = 1e-9\n",
        "\n",
        "# --- OptimizationResult  (active_set_opt ) ---\n",
        "class OptimizationResult:\n",
        "    def __init__(self, success, message, w_opt=None, pi_opt=None, lambda_opt=None,\n",
        "                 H_opt=None, grad_H_opt=None, iterations=None, fw_gap=None,\n",
        "                 active_set_opt=None):\n",
        "        self.success = success; self.message = message; self.w_opt = w_opt; self.pi_opt = pi_opt\n",
        "        self.lambda_opt = lambda_opt; self.H_opt = H_opt; self.grad_H_opt = grad_H_opt\n",
        "        self.iterations = iterations; self.fw_gap = fw_gap\n",
        "        self.active_set_opt = active_set_opt\n",
        "\n",
        "# --- find_feasible_initial_pi  ---\n",
        "def find_feasible_initial_pi(R, mu_tilde, K, tolerance=1e-8):\n",
        "    M = R.shape[1]; c = np.zeros(K + 1); c[K] = 1.0\n",
        "    A_ub = np.hstack((-R.T, -np.ones((M, 1)))); b_ub = -mu_tilde * np.ones(M)\n",
        "    bounds = [(None, None)] * K + [(0, None)]; opts = {'tol': tolerance, 'disp': False, 'presolve': True}\n",
        "    result = None; methods_to_try = ['highs', 'highs-ipm', 'highs-ds', 'simplex']\n",
        "    for method in methods_to_try:\n",
        "        try:\n",
        "            with warnings.catch_warnings(): warnings.filterwarnings(\"ignore\"); result = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=method, options=opts)\n",
        "            if result.success: break\n",
        "        except ValueError: continue\n",
        "        except Exception as e: return None, False, f\"Phase 1 LP failed: {e}\"\n",
        "    if result is None or not result.success: msg = result.message if result else \"No solver\"; status = result.status if result else -1; return None, False, f\"Phase 1 LP solver failed: {msg} (status={status})\"\n",
        "    s = result.x[K]; pi = result.x[:K]\n",
        "    if np.isnan(pi).any(): return None, False, \"Phase 1 LP resulted in NaN values for pi.\"\n",
        "    if s <= tolerance * 1000:\n",
        "        G = -R.T; h = -mu_tilde * np.ones(M); violation = np.max(G @ pi - h)\n",
        "        if violation <= tolerance * 10000: return pi, True, f\"Phase 1 OK (s*={s:.1e}, vio={violation:.1e})\"\n",
        "        else: return pi, True, f\"Phase 1 OK (s*={s:.1e}), WARN: violation {violation:.1e}\"\n",
        "    else: return None, False, f\"Phase 1 Infeasible (s* = {s:.1e})\"\n",
        "\n",
        "# --- solve_kkt_system  ---\n",
        "def solve_kkt_system(Q, G_W, g, tolerance=DEFAULT_TOLERANCE):\n",
        "    K = Q.shape[0]; n_act = G_W.shape[0] if G_W is not None and G_W.ndim == 2 and G_W.shape[0] > 0 else 0\n",
        "    if n_act == 0:\n",
        "        try: p = solve(Q, -g, assume_a='sym'); l = np.array([])\n",
        "        except LinAlgError: return None, None, False\n",
        "        res_norm = np.linalg.norm(Q @ p + g); g_norm = np.linalg.norm(g); solved_ok = res_norm <= tolerance * 1e3 * (1 + g_norm)\n",
        "        return p, l, solved_ok\n",
        "    else:\n",
        "        kkt_mat = None; rhs = None\n",
        "        try: kkt_mat = np.block([[Q, G_W.T], [G_W, np.zeros((n_act, n_act))]]) ; rhs = np.concatenate([-g, np.zeros(n_act)])\n",
        "        except ValueError as e: return None, None, False\n",
        "        try: sol = solve(kkt_mat, rhs, assume_a='sym'); p = sol[:K]; l = sol[K:]\n",
        "        except LinAlgError: return None, None, False\n",
        "        except ValueError as e: return None, None, False\n",
        "        res_norm = np.linalg.norm(kkt_mat @ sol - rhs); rhs_norm = np.linalg.norm(rhs); solved_ok = res_norm <= tolerance * 1e3 * (1 + rhs_norm)\n",
        "        return p, l, solved_ok\n",
        "\n",
        "# --- solve_inner_qp_active_set  ---\n",
        "def solve_inner_qp_active_set(Vw, R, mu_tilde, initial_pi, max_iter=350, tolerance=DEFAULT_TOLERANCE, regularization_epsilon=1e-10):\n",
        "    K = Vw.shape[0]; M = R.shape[1]; Q_reg = 2 * Vw + 2 * regularization_epsilon * np.eye(K); G = -R.T; h = -mu_tilde * np.ones(M)\n",
        "    if initial_pi is None: return None, None, None, None, None, False, \"No initial pi\"\n",
        "    pi_k = np.copy(initial_pi); lam_opt = np.zeros(M); W = set()\n",
        "    active_tol = tolerance * 10\n",
        "    initial_violations = G @ pi_k - h; W = set(j for j, viol in enumerate(initial_violations) if viol > -active_tol)\n",
        "    active_indices_opt = None; kkt_matrix_opt = None\n",
        "    if np.any(initial_violations > active_tol * 10): warnings.warn(f\"Initial pi infeasible (max viol: {np.max(initial_violations):.2e}).\")\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        g_k = Q_reg @ pi_k; act = sorted(list(W)); n_act = len(act); G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "        p_k, lam_Wk, solved = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "        if not solved or p_k is None: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: KKT solve failed. ActiveSet={act}\"\n",
        "        if np.linalg.norm(p_k) <= tolerance * 10 * (1 + np.linalg.norm(pi_k)):\n",
        "            is_optimal_point = True; blocking_constraint_idx = -1; min_negative_lambda = float('inf')\n",
        "            dual_feas_tol = -tolerance * 10\n",
        "            if W:\n",
        "                if lam_Wk is None or len(lam_Wk) != n_act: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: lam_Wk inconsistent? ActiveSet={act}\"\n",
        "                lambda_map = dict(zip(act, lam_Wk))\n",
        "                for constraint_idx, lagrange_multiplier in lambda_map.items():\n",
        "                    if lagrange_multiplier < dual_feas_tol: is_optimal_point = False;\n",
        "                    if lagrange_multiplier < min_negative_lambda: min_negative_lambda = lagrange_multiplier; blocking_constraint_idx = constraint_idx\n",
        "            if is_optimal_point:\n",
        "                lam_opt.fill(0.0);\n",
        "                if W and len(lam_Wk) == n_act: lam_opt[act] = np.maximum(lam_Wk, 0)\n",
        "                final_infeas = np.max(G @ pi_k - h); msg = f\"Optimal found at iter {i+1}.\"\n",
        "                if final_infeas > active_tol: msg += f\" (WARN: violation {final_infeas:.1e})\"\n",
        "                active_indices_opt = act\n",
        "                return pi_k, lam_opt, active_indices_opt, None, Q_reg / 2.0, True, msg\n",
        "            else:\n",
        "                if blocking_constraint_idx in W: W.remove(blocking_constraint_idx); continue\n",
        "                else: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Neg lambda idx {blocking_constraint_idx}, not in W={act}\"\n",
        "        else:\n",
        "            alpha_k = 1.0; blocking_constraint_idx = -1; min_step_length = float('inf')\n",
        "            step_tol = tolerance * 10\n",
        "            for j in range(M):\n",
        "                if j not in W:\n",
        "                    constraint_gradient_dot_p = G[j, :] @ p_k\n",
        "                    if constraint_gradient_dot_p > step_tol:\n",
        "                        distance_to_boundary = h[j] - (G[j, :] @ pi_k)\n",
        "                        if abs(constraint_gradient_dot_p) > 1e-15:\n",
        "                            alpha_j = distance_to_boundary / constraint_gradient_dot_p\n",
        "                            step_j = max(0.0, alpha_j)\n",
        "                            if step_j < min_step_length:\n",
        "                                min_step_length = step_j; blocking_constraint_idx = j\n",
        "            alpha_k = min(1.0, min_step_length); pi_k += alpha_k * p_k\n",
        "            if alpha_k < 1.0 - step_tol and blocking_constraint_idx != -1:\n",
        "                if blocking_constraint_idx not in W: W.add(blocking_constraint_idx)\n",
        "            continue\n",
        "    msg = f\"Max iter ({max_iter}) reached.\"; final_infeas = np.max(G @ pi_k - h)\n",
        "    if final_infeas > active_tol * 10: return None, None, None, None, None, False, f\"{msg} Final infeasible. ActiveSet={sorted(list(W))}\"\n",
        "    act = sorted(list(W)); n_act = len(act); G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "    is_likely_optimal = False; active_constraints_opt = act\n",
        "    g_k = Q_reg @ pi_k; p_f, lam_f, solved_f = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "    final_lambda_estimate = np.zeros(M)\n",
        "    if solved_f and p_f is not None and np.linalg.norm(p_f) <= tolerance * 100 * (1 + np.linalg.norm(pi_k)):\n",
        "        if n_act > 0 and lam_f is not None and len(lam_f) == n_act:\n",
        "             try: final_lambda_estimate[act] = lam_f\n",
        "             except IndexError: pass\n",
        "        active_lambdas = final_lambda_estimate[act] if n_act > 0 else np.array([])\n",
        "        if n_act == 0 or np.all(active_lambdas >= -tolerance * 100): is_likely_optimal = True; msg += \" Final KKT check approx OK.\"\n",
        "        else: msg += \" Final KKT check fails (dual infeasible).\"\n",
        "    else: msg += \" Final KKT check fails (stationarity or solve error).\"\n",
        "    lam_opt = final_lambda_estimate\n",
        "    return pi_k, lam_opt, active_constraints_opt, None, Q_reg / 2.0, is_likely_optimal, msg\n",
        "\n",
        "# --- make_psd  ---\n",
        "def make_psd(matrix, tolerance=1e-8):\n",
        "    sym = (matrix + matrix.T) / 2\n",
        "    try: eigenvalues, eigenvectors = np.linalg.eigh(sym); min_eigenvalue = np.min(eigenvalues)\n",
        "    except LinAlgError: warnings.warn(\"...\"); return sym\n",
        "    if min_eigenvalue < tolerance: eigenvalues[eigenvalues < tolerance] = tolerance; psd_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T; return (psd_matrix + psd_matrix.T) / 2\n",
        "    else: return sym\n",
        "\n",
        "# --- calculate_Vw  ---\n",
        "def calculate_Vw(w, R, SecondMoments_a_array, tolerance=DEFAULT_TOLERANCE, psd_tolerance=1e-8):\n",
        "    K, M = R.shape; w_sum = np.sum(w); w_norm = w\n",
        "    EwX = R @ w_norm; EwXXT = np.zeros((K, K));\n",
        "    for m in range(M): EwXXT += w_norm[m] * SecondMoments_a_array[m]\n",
        "    Vw = EwXXT - np.outer(EwX, EwX); Vw_psd = make_psd(Vw, psd_tolerance)\n",
        "    return Vw_psd, EwX, EwXXT\n",
        "\n",
        "# --- calculate_H_gradient  () ---\n",
        "def calculate_H_gradient(pi_star, w, R, SecondMoments_a_array, EwX, EwXXT, tolerance=1e-9, debug_print=False):\n",
        "    M = w.shape[0]; K = R.shape[0]; grad = np.zeros(M)\n",
        "    norm_tolerance = 1e-12 # Allow very small pi_star\n",
        "\n",
        "    if pi_star is None or EwX is None:\n",
        "        if debug_print: print(\"DEBUG grad_H: Returning NaN due to None input (pi_star or EwX).\")\n",
        "        return np.full(M, np.nan)\n",
        "    if np.isnan(pi_star).any() or np.isinf(pi_star).any():\n",
        "         if debug_print: print(\"DEBUG grad_H: Returning NaN because pi_star contains NaN/Inf.\")\n",
        "         return np.full(M, np.nan)\n",
        "    pi_norm = np.linalg.norm(pi_star)\n",
        "    if pi_norm < norm_tolerance:\n",
        "         if debug_print: print(f\"DEBUG grad_H: Returning NaN because pi_star norm {pi_norm:.2e} < {norm_tolerance:.1e}\")\n",
        "         return np.full(M, np.nan)\n",
        "    if np.isnan(EwX).any() or np.isinf(EwX).any():\n",
        "        if debug_print: print(\"DEBUG grad_H: Returning NaN because EwX contains NaN/Inf.\")\n",
        "        return np.full(M, np.nan)\n",
        "\n",
        "    try:\n",
        "        pi_T_EwX = pi_star.T @ EwX\n",
        "        if np.isnan(pi_T_EwX) or np.isinf(pi_T_EwX):\n",
        "             if debug_print: print(f\"DEBUG grad_H: pi_T_EwX is NaN/Inf: {pi_T_EwX}\")\n",
        "             return np.full(M, np.nan)\n",
        "\n",
        "        for j in range(M):\n",
        "            Sigma_j = SecondMoments_a_array[j]; r_j = R[:, j]\n",
        "            pi_T_Sigma_j_pi = pi_star.T @ Sigma_j @ pi_star\n",
        "            pi_T_r_j = pi_star.T @ r_j\n",
        "\n",
        "            if np.isnan(pi_T_Sigma_j_pi) or np.isinf(pi_T_Sigma_j_pi):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_Sigma_j_pi is NaN/Inf: {pi_T_Sigma_j_pi}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "            if np.isnan(pi_T_r_j) or np.isinf(pi_T_r_j):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_r_j is NaN/Inf: {pi_T_r_j}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "\n",
        "            term2 = 2 * pi_T_r_j * pi_T_EwX\n",
        "            if np.isnan(term2) or np.isinf(term2):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): term2 (2*pi_T_r_j*pi_T_EwX) is NaN/Inf: {term2}, pi_T_r_j={pi_T_r_j}, pi_T_EwX={pi_T_EwX}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "\n",
        "            grad[j] = pi_T_Sigma_j_pi - term2\n",
        "            if np.isnan(grad[j]) or np.isinf(grad[j]):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): final grad[{j}] is NaN/Inf\")\n",
        "\n",
        "    except Exception as e_calc:\n",
        "         print(f\"ERROR in calculate_H_gradient calculation: {e_calc}\")\n",
        "         print(traceback.format_exc())\n",
        "         return np.full(M, np.nan)\n",
        "\n",
        "    if np.any(np.isnan(grad)) or np.any(np.isinf(grad)):\n",
        "        warnings.warn(f\"NaN or Inf detected in calculated gradient for w={w}\")\n",
        "        if debug_print: print(f\"DEBUG grad_H: Final check found NaN/Inf in gradient: {grad}\")\n",
        "        return np.full(M, np.nan)\n",
        "    return grad\n",
        "\n",
        "# --- project_to_simplex  ---\n",
        "def project_to_simplex(v, z=1):\n",
        "    n_features = v.shape[0];\n",
        "    if n_features == 0: return np.array([])\n",
        "    v_arr = np.asarray(v)\n",
        "    if np.all(v_arr >= -1e-9) and np.isclose(np.sum(v_arr), z): return np.maximum(v_arr, 0)\n",
        "    u = np.sort(v_arr)[::-1]; cssv = np.cumsum(u) - z; ind = np.arange(n_features) + 1; cond = u - cssv / ind > 0\n",
        "    if np.any(cond): rho = ind[cond][-1]; theta = cssv[rho - 1] / float(rho); w = np.maximum(v_arr - theta, 0)\n",
        "    else:\n",
        "         w = np.zeros(n_features)\n",
        "         if z > 0: w[np.argmax(v_arr)] = z\n",
        "    w_sum = np.sum(w)\n",
        "    if not np.isclose(w_sum, z):\n",
        "        if w_sum > 1e-9: w = w * (z / w_sum)\n",
        "        elif z > 0 :\n",
        "            w = np.zeros(n_features)\n",
        "            w[np.argmax(v_arr)] = z\n",
        "    return np.maximum(w, 0)\n",
        "\n",
        "# --- frank_wolfe_optimizer  (ActiveSet) ---\n",
        "def frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde, initial_w=None, max_outer_iter=250, fw_gap_tol=1e-7, inner_max_iter=350, tolerance=1e-9, psd_make_tolerance=1e-8, qp_regularization=1e-10, debug_print=False, force_iterations=0, return_history=False):\n",
        "    K, M = R_alpha.shape\n",
        "    if initial_w is None: w_k = np.ones(M) / M\n",
        "    else: w_k = project_to_simplex(np.copy(initial_w))\n",
        "    if w_k is None:\n",
        "        result = OptimizationResult(False, \"Initial projection failed\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "    fw_gap = float('inf'); best_w = np.copy(w_k); best_pi = None; best_lam = np.zeros(M); best_H = -float('inf'); final_gHk = np.zeros(M); best_active_set = None\n",
        "    pi0, ok, p1msg = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance)\n",
        "    if not ok:\n",
        "        result = OptimizationResult(False, f\"Phase 1 failed: {p1msg}\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "    current_w=np.copy(w_k); current_H=-float('inf'); current_pi=None; current_lam=None; current_active_set=None\n",
        "    inner_solver_args_for_loop = {'max_iter': inner_max_iter, 'tolerance': tolerance, 'regularization_epsilon': qp_regularization}\n",
        "    history = []\n",
        "\n",
        "    last_successful_pi = pi0\n",
        "    last_successful_lam = np.zeros(M)\n",
        "    last_successful_active_set = tuple()\n",
        "    last_successful_gHk = np.zeros(M)\n",
        "    last_successful_fw_gap = float('inf')\n",
        "\n",
        "    for k in range(max_outer_iter):\n",
        "        iter_data = {'k': k + 1}\n",
        "        if return_history: iter_data['w_k'] = np.copy(w_k)\n",
        "        try: Vk, Ex, ExxT = calculate_Vw(w_k, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "        except Exception as e:\n",
        "            result = OptimizationResult(False, f\"Outer iter {k+1}: Vw failed: {e}\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, active_set_opt=best_active_set)\n",
        "            if return_history: history.append(iter_data); return (result, history)\n",
        "            else: return result\n",
        "\n",
        "        # === Use Active Set for inner solve ===\n",
        "        pi_init_inner = last_successful_pi if last_successful_pi is not None else pi0\n",
        "        pk, lk, act_idx_k, _, Vw_reg_k, inner_ok, inner_msg = solve_inner_qp_active_set(\n",
        "            Vk, R_alpha, mu_tilde, pi_init_inner, **inner_solver_args_for_loop\n",
        "        )\n",
        "        # ===================================\n",
        "\n",
        "        if not inner_ok or pk is None:\n",
        "            warnings.warn(f\"Outer iter {k+1}: Inner QP failed: {inner_msg}. Using last successful state if available.\")\n",
        "            if last_successful_pi is None:\n",
        "                 result = OptimizationResult(False, f\"Outer iter {k+1}: Inner QP failed and no prior success: {inner_msg}\",\n",
        "                                             w_opt=current_w, iterations=k)\n",
        "                 if return_history: history.append(iter_data); return (result, history)\n",
        "                 else: return result\n",
        "            pk = last_successful_pi\n",
        "            lk = last_successful_lam\n",
        "            act_idx_k = list(last_successful_active_set)\n",
        "            final_gHk = last_successful_gHk\n",
        "            fw_gap = last_successful_fw_gap\n",
        "            try: Hk = pk.T @ Vk @ pk\n",
        "            except: Hk = current_H\n",
        "        else:\n",
        "            last_successful_pi = pk\n",
        "            last_successful_lam = lk if lk is not None else np.zeros(M)\n",
        "            last_successful_active_set = tuple(sorted(act_idx_k)) if act_idx_k is not None else tuple()\n",
        "            Hk = pk.T @ Vk @ pk\n",
        "            try:\n",
        "                gHk = calculate_H_gradient(pk, w_k, R_alpha, SecondMoments_alpha_array, Ex, ExxT, tolerance, debug_print=debug_print)\n",
        "            except Exception as e:\n",
        "                result = OptimizationResult(False, f\"Outer iter {k+1}: Grad failed: {e}\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "                if return_history: history.append(iter_data); return (result, history)\n",
        "                else: return result\n",
        "\n",
        "            final_gHk = gHk if gHk is not None else np.full(M, np.nan)\n",
        "            last_successful_gHk = final_gHk if not np.isnan(final_gHk).any() else last_successful_gHk\n",
        "\n",
        "        if return_history: iter_data['H_k'] = Hk\n",
        "        if return_history: iter_data['grad_H_k_norm'] = np.linalg.norm(final_gHk) if not np.isnan(final_gHk).any() else np.nan\n",
        "\n",
        "        current_w = np.copy(w_k); current_H = Hk; current_pi = pk; current_lam = last_successful_lam; current_active_set = last_successful_active_set\n",
        "\n",
        "        if Hk >= best_H - tolerance*1000:\n",
        "             best_H = Hk; best_w = np.copy(w_k); best_pi = np.copy(pk); best_lam = np.copy(current_lam); best_active_set = current_active_set\n",
        "\n",
        "        if final_gHk is None or np.isnan(final_gHk).any():\n",
        "            result = OptimizationResult(False, f\"Outer iter {k+1}: Grad NaN.\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "            if return_history: history.append(iter_data); return (result, history)\n",
        "            else: return result\n",
        "\n",
        "        grad_norm = np.linalg.norm(final_gHk)\n",
        "        if grad_norm < tolerance * 10: sk = w_k; sk_idx = -1\n",
        "        else: sk_idx = np.argmax(final_gHk); sk = np.zeros(M); sk[sk_idx] = 1.0\n",
        "        if return_history: iter_data['s_k_index'] = sk_idx\n",
        "\n",
        "        if w_k is None:\n",
        "             result = OptimizationResult(False, f\"k={k} w_k None\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             if return_history: history.append(iter_data); return (result, history)\n",
        "             else: return result\n",
        "\n",
        "        fw_gap = final_gHk.T @ (w_k - sk)\n",
        "        last_successful_fw_gap = fw_gap\n",
        "        if return_history: iter_data['fw_gap'] = fw_gap\n",
        "        gamma = 2.0 / (k + 3.0)\n",
        "        if return_history: iter_data['gamma_k'] = gamma\n",
        "        if return_history: history.append(iter_data)\n",
        "\n",
        "        converged = False\n",
        "        if k >= force_iterations and not np.isnan(fw_gap):\n",
        "            if abs(fw_gap) <= fw_gap_tol: converged = True; conv_msg = f\"Converged (Gap {abs(fw_gap):.2e})\"\n",
        "        if converged:\n",
        "            # --- Final Inner Solve for Consistency ---\n",
        "            final_w = current_w\n",
        "            final_pi_result = current_pi\n",
        "            final_lam_result = current_lam\n",
        "            final_active_set_result = current_active_set\n",
        "            final_H_result = current_H\n",
        "            final_grad_result = final_gHk\n",
        "            final_fw_gap_result = fw_gap\n",
        "\n",
        "            try:\n",
        "                final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "                pi0_final, ok_final, _ = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance)\n",
        "                if ok_final:\n",
        "                    pi_s_final, lam_s_final, act_idx_final, _, _, inner_ok_final, msg_final = solve_inner_qp_active_set(\n",
        "                        final_Vk, R_alpha, mu_tilde, pi0_final, # Use stable pi0 for final check\n",
        "                        **inner_solver_args_for_loop\n",
        "                    )\n",
        "                    if inner_ok_final and pi_s_final is not None and lam_s_final is not None:\n",
        "                        final_pi_result = pi_s_final\n",
        "                        final_lam_result = lam_s_final\n",
        "                        final_active_set_result = tuple(sorted(act_idx_final)) if act_idx_final is not None else tuple()\n",
        "                        final_grad_result = calculate_H_gradient(final_pi_result, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance, debug_print=False)\n",
        "                        if final_grad_result is None or np.isnan(final_grad_result).any(): final_grad_result = np.zeros(M)\n",
        "                        grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                        if grad_norm_final < tolerance * 10: sk_final = final_w\n",
        "                        else: sk_idx_final = np.argmax(final_grad_result); sk_final = np.zeros(M); sk_final[sk_idx_final] = 1.0\n",
        "                        final_fw_gap_result = final_grad_result.T @ (final_w - sk_final) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                    else:\n",
        "                        warnings.warn(f\"Warning: Final inner QP solve failed after convergence: {msg_final}. Using last iteration's values.\")\n",
        "                else:\n",
        "                     warnings.warn(f\"Warning: Could not find feasible pi0 for final solve after convergence. Using last iteration's values.\")\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Error during final inner solve after convergence: {e}. Using last iteration's values.\")\n",
        "            # --- End of Final Inner Solve ---\n",
        "\n",
        "            result = OptimizationResult(True, conv_msg, w_opt=final_w, pi_opt=final_pi_result, lambda_opt=final_lam_result,\n",
        "                                        H_opt=final_H_result, grad_H_opt=final_grad_result, iterations=k + 1,\n",
        "                                        fw_gap=final_fw_gap_result, active_set_opt=final_active_set_result)\n",
        "            return (result, history) if return_history else result\n",
        "\n",
        "        # --- Prepare for next iteration ---\n",
        "        if w_k is None or sk is None:\n",
        "             result = OptimizationResult(False, f\"k={k} w_k/sk None before update\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             return (result, history) if return_history else result\n",
        "        w_k_next = (1.0 - gamma) * w_k + gamma * sk; w_k = project_to_simplex(w_k_next)\n",
        "        if w_k is None:\n",
        "             result = OptimizationResult(False, f\"k={k} proj None\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             return (result, history) if return_history else result\n",
        "        pi0 = pk # Use last inner solution as initial guess\n",
        "\n",
        "    # Max iter reached, perform final solve for consistency\n",
        "    final_w = current_w\n",
        "    final_pi_result = current_pi\n",
        "    final_lam_result = current_lam\n",
        "    final_active_set_result = current_active_set\n",
        "    final_H_result = current_H\n",
        "    final_grad_result = last_successful_gHk\n",
        "    final_fw_gap_result = last_successful_fw_gap\n",
        "\n",
        "    try:\n",
        "        final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "        pi0_final, ok_final, _ = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance) # Use stable pi0\n",
        "        if ok_final:\n",
        "            pi_s_final, lam_s_final, act_idx_final, _, _, inner_ok_final, msg_final = solve_inner_qp_active_set(\n",
        "                final_Vk, R_alpha, mu_tilde, pi0_final, # <-- Use stable pi0_final\n",
        "                **inner_solver_args_for_loop\n",
        "            )\n",
        "            if inner_ok_final and pi_s_final is not None and lam_s_final is not None:\n",
        "                final_pi_result = pi_s_final\n",
        "                final_lam_result = lam_s_final\n",
        "                final_active_set_result = tuple(sorted(act_idx_final)) if act_idx_final is not None else tuple()\n",
        "                # Recalculate gradient and gap\n",
        "                final_grad_result = calculate_H_gradient(final_pi_result, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance, debug_print=False)\n",
        "                if final_grad_result is None or np.isnan(final_grad_result).any(): final_grad_result = np.zeros(M)\n",
        "                grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                if grad_norm_final < tolerance * 10: sk_final = final_w\n",
        "                else: sk_idx_final = np.argmax(final_grad_result); sk_final = np.zeros(M); sk_final[sk_idx_final] = 1.0\n",
        "                final_fw_gap_result = final_grad_result.T @ (final_w - sk_final) if not np.isnan(final_grad_result).any() else np.nan\n",
        "            else:\n",
        "                warnings.warn(f\"Warning: Final inner QP solve failed after max iter: {msg_final}. Using last iteration's values.\")\n",
        "        else:\n",
        "             warnings.warn(f\"Warning: Could not find feasible pi0 for final solve after max iter. Using last iteration's values.\")\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error during final inner solve after max iter: {e}. Using last iteration's values.\")\n",
        "\n",
        "    if return_history:\n",
        "        grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "        if grad_norm_final < tolerance * 10: sk_idx_final = -1\n",
        "        else: sk_idx_final = np.argmax(final_grad_result) if not np.isnan(final_grad_result).any() else -1\n",
        "        history.append({\n",
        "            'k': max_outer_iter + 1, 'w_k': np.copy(final_w), 'H_k': final_H_result,\n",
        "            'grad_H_k_norm': grad_norm_final, 's_k_index': sk_idx_final, 'fw_gap': final_fw_gap_result,\n",
        "            'gamma_k': np.nan\n",
        "        })\n",
        "\n",
        "    result = OptimizationResult(True, f\"Max Iter ({max_outer_iter})\", w_opt=final_w, pi_opt=final_pi_result, lambda_opt=final_lam_result,\n",
        "                                H_opt=final_H_result, grad_H_opt=final_grad_result, iterations=max_outer_iter,\n",
        "                                fw_gap=final_fw_gap_result, active_set_opt=final_active_set_result)\n",
        "    return (result, history) if return_history else result\n",
        "\n",
        "\n",
        "# --- generate_params_profile_switching  () ---\n",
        "def generate_params_profile_switching(alpha, alpha_max, K=5, M=3, R_base=np.array([0.08, 0.07, 0.06, 0.05, 0.04]), sigma_base=np.array([0.18, 0.15, 0.20, 0.12, 0.10]), Corr_base=np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]),\n",
        "                                      good_prof_r_factor=0.01, good_prof_r_offset=0.0005, # <-- \n",
        "                                      good_prof_s_factor=-0.005, good_prof_corr_factor=1.0, # <-- \n",
        "                                      bad_prof_r_factor=-0.005, bad_prof_r_offset=-0.0002, # <-- \n",
        "                                      bad_prof_s_factor=0.005, bad_prof_corr_offset=0.0, # <-- \n",
        "                                      sigma_min_epsilon=1e-4, psd_tolerance=1e-9):\n",
        "    assert alpha >= 0 and K == len(R_base) and K == len(sigma_base) and K == Corr_base.shape[0] and M == 3, \"Dimension mismatch\"\n",
        "    non_diag_indices = np.where(~np.eye(K, dtype=bool)); R_neutral = R_base; sigma_neutral = np.maximum(sigma_min_epsilon, sigma_base); Corr_neutral = make_psd(Corr_base, psd_tolerance)\n",
        "    R_good = R_base + (R_base * good_prof_r_factor + good_prof_r_offset); sigma_good = np.maximum(sigma_min_epsilon, sigma_base + (sigma_base * good_prof_s_factor)); Corr_good_target = np.copy(Corr_base); Corr_good_target[non_diag_indices] *= good_prof_corr_factor; Corr_good = make_psd(Corr_good_target, psd_tolerance)\n",
        "    R_bad = R_base + (R_base * bad_prof_r_factor + bad_prof_r_offset); sigma_bad = np.maximum(sigma_min_epsilon, sigma_base + (sigma_base * bad_prof_s_factor)); Corr_bad_target = np.copy(Corr_base); Corr_bad_target[non_diag_indices] = np.clip(Corr_bad_target[non_diag_indices] + bad_prof_corr_offset, -1.0+psd_tolerance, 1.0-psd_tolerance); Corr_bad = make_psd(Corr_bad_target, psd_tolerance)\n",
        "    beta = np.clip(alpha / alpha_max if alpha_max > 0 else (1.0 if alpha > 0 else 0.0), 0.0, 1.0)\n",
        "    R_alpha = np.zeros((K, M)); sigma_alpha = np.zeros((K, M)); Corr_alpha = [np.eye(K) for _ in range(M)]; Cov_alpha = [np.eye(K) for _ in range(M)]; SecondMoments_a_array = np.zeros((M, K, K))\n",
        "    R_alpha[:, 0] = (1 - beta) * R_good + beta * R_neutral; sigma_alpha[:, 0] = np.maximum(sigma_min_epsilon, (1 - beta) * sigma_good + beta * sigma_neutral); Corr_alpha[0] = make_psd((1 - beta) * Corr_good + beta * Corr_neutral, psd_tolerance)\n",
        "    R_alpha[:, 1] = (1 - beta) * R_bad + beta * R_good; sigma_alpha[:, 1] = np.maximum(sigma_min_epsilon, (1 - beta) * sigma_bad + beta * sigma_good); Corr_alpha[1] = make_psd((1 - beta) * Corr_bad + beta * Corr_good, psd_tolerance)\n",
        "    R_alpha[:, 2] = (1 - beta) * R_neutral + beta * R_bad; sigma_alpha[:, 2] = np.maximum(sigma_min_epsilon, (1 - beta) * sigma_neutral + beta * sigma_bad); Corr_alpha[2] = make_psd((1 - beta) * Corr_neutral + beta * Corr_bad, psd_tolerance)\n",
        "    for m in range(M): sigma_diag_m = np.diag(sigma_alpha[:, m]); Cov_alpha[m] = sigma_diag_m @ Corr_alpha[m] @ sigma_diag_m; SecondMoments_a_array[m, :, :] = Cov_alpha[m] + np.outer(R_alpha[:, m], R_alpha[:, m])\n",
        "    return R_alpha, SecondMoments_a_array\n",
        "\n",
        "# ===  (ActiveSet) ===\n",
        "def analyze_alpha_full_results(alpha_range, param_gen_kwargs, optimizer_kwargs, mu_tilde):\n",
        "    \"\"\"  alpha  FWH*pi*grad H*lambda*active_set*  \"\"\"\n",
        "    results_over_alpha = []\n",
        "    K = param_gen_kwargs.get('K', 5); M = param_gen_kwargs.get('M', 3)\n",
        "\n",
        "    print(\"\\n--- Starting Full Results Analysis over Alpha Range (using Active Set Method) ---\")\n",
        "    total_alphas = len(alpha_range)\n",
        "    start_loop_time = time.time()\n",
        "\n",
        "    for idx, alpha in enumerate(alpha_range):\n",
        "        loop_start_time = time.time()\n",
        "        print(f\"\\rAnalyzing alpha = {alpha:.6f} ({idx+1}/{total_alphas}) ... \", end=\"\")\n",
        "\n",
        "        #   kwargs  \n",
        "        R_alpha, SecondMoments_alpha_array = generate_params_profile_switching(alpha, **param_gen_kwargs)\n",
        "        alpha_result = {'alpha': alpha}\n",
        "\n",
        "        w_fw_default = np.full(M, np.nan)\n",
        "        H_star_fw = np.nan\n",
        "        pi_opt = np.full(K, np.nan)\n",
        "        grad_H_opt = np.full(M, np.nan)\n",
        "        lambda_opt = np.full(M, np.nan)\n",
        "        active_set_opt = tuple()\n",
        "\n",
        "        try:\n",
        "            # --- FW Optimizer (Active Set) ---\n",
        "            fw_result = frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde,\n",
        "                                              initial_w=None, return_history=False,\n",
        "                                              debug_print=False, # OFF\n",
        "                                              **optimizer_kwargs)\n",
        "            if fw_result.success:\n",
        "                 w_fw_default = fw_result.w_opt if fw_result.w_opt is not None else np.full(M, np.nan)\n",
        "                 H_star_fw = fw_result.H_opt if fw_result.H_opt is not None else np.nan\n",
        "                 pi_opt = fw_result.pi_opt if fw_result.pi_opt is not None else np.full(K, np.nan)\n",
        "                 grad_H_opt = fw_result.grad_H_opt if fw_result.grad_H_opt is not None else np.full(M, np.nan)\n",
        "                 lambda_opt = fw_result.lambda_opt if fw_result.lambda_opt is not None else np.full(M, np.nan)\n",
        "                 active_set_opt = fw_result.active_set_opt if fw_result.active_set_opt is not None else tuple()\n",
        "            else:\n",
        "                 w_fw_default = np.full(M, np.nan)\n",
        "                 H_star_fw = np.nan\n",
        "                 pi_opt = np.full(K, np.nan)\n",
        "                 grad_H_opt = np.full(M, np.nan)\n",
        "                 lambda_opt = np.full(M, np.nan)\n",
        "                 active_set_opt = tuple()\n",
        "                 print(f\"\\n  Warn: FW failed for alpha={alpha:.6f}: {fw_result.message}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n  Error during FW run for alpha={alpha:.6f}: {e}\")\n",
        "            w_fw_default = np.full(M, np.nan)\n",
        "            H_star_fw = np.nan\n",
        "            pi_opt = np.full(K, np.nan)\n",
        "            grad_H_opt = np.full(M, np.nan)\n",
        "            lambda_opt = np.full(M, np.nan)\n",
        "            active_set_opt = tuple()\n",
        "\n",
        "        alpha_result['w_fw_default'] = w_fw_default\n",
        "        alpha_result['H_star'] = H_star_fw\n",
        "        alpha_result['pi_opt'] = pi_opt\n",
        "        alpha_result['grad_H_opt'] = grad_H_opt\n",
        "        alpha_result['lambda_opt'] = lambda_opt\n",
        "        alpha_result['active_set_opt'] = active_set_opt\n",
        "\n",
        "        results_over_alpha.append(alpha_result)\n",
        "\n",
        "    print(\"\\n--- Finished Full Results Analysis ---\")\n",
        "    df_results = pd.DataFrame(results_over_alpha)\n",
        "    if PANDAS_AVAILABLE:\n",
        "        fw_vecs = np.stack([res.get('w_fw_default', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        pi_vecs = np.stack([res.get('pi_opt', np.full(K, np.nan)) for res in results_over_alpha])\n",
        "        grad_vecs = np.stack([res.get('grad_H_opt', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        lambda_vecs = np.stack([res.get('lambda_opt', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        active_sets = [str(res.get('active_set_opt', tuple())) for res in results_over_alpha]\n",
        "\n",
        "        for m in range(M):\n",
        "            df_results[f'w_fw_{m}'] = fw_vecs[:, m]\n",
        "            df_results[f'grad_H_{m}'] = grad_vecs[:, m]\n",
        "            df_results[f'lambda_{m}'] = lambda_vecs[:, m]\n",
        "        for k in range(K):\n",
        "            df_results[f'pi_{k}'] = pi_vecs[:, k]\n",
        "\n",
        "        df_results['active_set'] = active_sets\n",
        "\n",
        "        df_results = df_results.drop(columns=['w_fw_default', 'pi_opt', 'grad_H_opt', 'lambda_opt', 'active_set_opt'], errors='ignore')\n",
        "\n",
        "    return df_results if PANDAS_AVAILABLE else results_over_alpha\n",
        "\n",
        "\n",
        "# ===  ===\n",
        "if __name__ == '__main__':\n",
        "    start_time_main = time.time()\n",
        "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "    warnings.filterwarnings('ignore', category=UserWarning)\n",
        "    warnings.filterwarnings('ignore', category=OptimizeWarning)\n",
        "\n",
        "    # ---  ---\n",
        "    K = 5; M = 3;\n",
        "    #  mu_tilde  0.02  \n",
        "    mu_tilde = 0.02\n",
        "    alpha_max = 1.5\n",
        "\n",
        "    #   \n",
        "    profile_adjustments_mild = {\n",
        "        'good_prof_r_factor': 0.01, 'good_prof_r_offset': 0.0005,\n",
        "        'good_prof_s_factor': -0.005, 'good_prof_corr_factor': 1.0,\n",
        "        'bad_prof_r_factor': -0.005, 'bad_prof_r_offset': -0.0002,\n",
        "        'bad_prof_s_factor': 0.005, 'bad_prof_corr_offset': 0.0,\n",
        "    }\n",
        "    param_gen_kwargs_main = {'K': K, 'M': M, 'alpha_max': alpha_max, **profile_adjustments_mild} # <-- \n",
        "\n",
        "    solver_settings = { # \n",
        "        'max_outer_iter': 500, 'fw_gap_tol': 1e-9,\n",
        "        'inner_max_iter': 600,\n",
        "        'tolerance': 1e-11,\n",
        "        'psd_make_tolerance': 1e-9,\n",
        "        'qp_regularization': 1e-10, # \n",
        "        'force_iterations': 20\n",
        "    }\n",
        "    #   alpha  (0.4  0.6  0.001 ) \n",
        "    alpha_start = 0.400\n",
        "    alpha_end = 0.600\n",
        "    alpha_step = 0.001\n",
        "    num_alpha_steps = int(round((alpha_end - alpha_start) / alpha_step)) + 1\n",
        "    alpha_range_analyze = np.linspace(alpha_start, alpha_end, num_alpha_steps)\n",
        "    unique_pt_tolerance = 1e-5 # KKT\n",
        "\n",
        "    output_csv_filename = f\"alpha_internal_sol_search_{alpha_start:.3f}_{alpha_end:.3f}_activeset_mu002_mild.csv\" # \n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"--- Searching for Internal Solution around alpha=0.5 [{alpha_start:.3f}, {alpha_end:.3f}] ---\")\n",
        "    print(f\"--- (Using ActiveSet, mu_tilde={mu_tilde}, Milder Profiles) ---\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # ---  ---\n",
        "    results_df = analyze_alpha_full_results(\n",
        "        alpha_range=alpha_range_analyze,\n",
        "        param_gen_kwargs=param_gen_kwargs_main, # <-- \n",
        "        optimizer_kwargs=solver_settings,\n",
        "        mu_tilde=mu_tilde\n",
        "    )\n",
        "\n",
        "    # ---  & CSV & KKT ---\n",
        "    if PANDAS_AVAILABLE and isinstance(results_df, pd.DataFrame):\n",
        "        print(\"\\n--- Analysis Results Summary (DataFrame) ---\")\n",
        "        float_format_func = lambda x: f\"{x:.4e}\" if pd.notna(x) and isinstance(x, (float, np.number)) else x\n",
        "        pd.options.display.float_format = float_format_func\n",
        "        cols_to_show = ['alpha', 'H_star', 'active_set'] + \\\n",
        "                       [f'w_fw_{m}' for m in range(M) if f'w_fw_{m}' in results_df.columns] + \\\n",
        "                       [f'pi_{k}' for k in range(K) if f'pi_{k}' in results_df.columns] + \\\n",
        "                       [f'grad_H_{m}' for m in range(M) if f'grad_H_{m}' in results_df.columns] + \\\n",
        "                       [f'lambda_{m}' for m in range(M) if f'lambda_{m}' in results_df.columns]\n",
        "        cols_to_show = [col for col in cols_to_show if col in results_df.columns]\n",
        "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 200): # \n",
        "             print(results_df[cols_to_show].to_string(index=False, na_rep='NaN'))\n",
        "        pd.reset_option('display.float_format')\n",
        "\n",
        "        # CSV ()\n",
        "        try:\n",
        "            df_to_save = results_df.copy()\n",
        "            cols_order = ['alpha', 'H_star', 'active_set'] + \\\n",
        "                         [f'w_fw_{m}' for m in range(M) if f'w_fw_{m}' in df_to_save.columns] + \\\n",
        "                         [f'pi_{k}' for k in range(K) if f'pi_{k}' in df_to_save.columns] + \\\n",
        "                         [f'grad_H_{m}' for m in range(M) if f'grad_H_{m}' in df_to_save.columns] + \\\n",
        "                         [f'lambda_{m}' for m in range(M) if f'lambda_{m}' in df_to_save.columns]\n",
        "            cols_order = [col for col in cols_order if col in df_to_save.columns]\n",
        "            df_to_save = df_to_save[cols_order]\n",
        "\n",
        "            df_to_save.to_csv(output_csv_filename, index=False, float_format='%.8e')\n",
        "            print(f\"\\nFull detailed results ({len(df_to_save)} points) saved to: {os.path.abspath(output_csv_filename)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError saving results to CSV: {e}\")\n",
        "\n",
        "        # KKT ()\n",
        "        print(\"\\n--- Quick KKT Check (Stationarity w.r.t. w) ---\")\n",
        "        kkt_violations = []\n",
        "        # unique_pt_tolerance  main \n",
        "        for index, row in results_df.iterrows():\n",
        "            alpha = row['alpha']\n",
        "            w_star = np.array([row.get(f'w_fw_{m}', np.nan) for m in range(M)])\n",
        "            grad_h = np.array([row.get(f'grad_H_{m}', np.nan) for m in range(M)])\n",
        "            active_set_str = row.get('active_set', '()')\n",
        "\n",
        "            if np.isnan(w_star).any() or np.isnan(grad_h).any() or np.isclose(np.sum(w_star), 0):\n",
        "                kkt_violations.append({\n",
        "                    'alpha': alpha,\n",
        "                    'active_set_report': active_set_str,\n",
        "                    'active_set_kkt': 'NaN',\n",
        "                    'max_violation': np.nan,\n",
        "                    'grad_consistency_violation': np.nan,\n",
        "                    'check_result': 'Skipped (NaN or zero w)'\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            check_tol = solver_settings['tolerance'] * 1e3\n",
        "\n",
        "            max_grad_h = np.max(grad_h)\n",
        "            nu_estimate = max_grad_h\n",
        "\n",
        "            stationarity_violation = 0.0\n",
        "            active_indices_kkt = set()\n",
        "            for m in range(M):\n",
        "                if w_star[m] > unique_pt_tolerance:\n",
        "                    active_indices_kkt.add(m)\n",
        "\n",
        "            active_grads = grad_h[list(active_indices_kkt)] if active_indices_kkt else []\n",
        "            grad_consistency_violation = 0.0\n",
        "            if len(active_grads) > 1:\n",
        "                grad_consistency_violation = np.max(active_grads) - np.min(active_grads)\n",
        "                if not np.isclose(grad_consistency_violation, 0.0, atol=check_tol):\n",
        "                     stationarity_violation = max(stationarity_violation, grad_consistency_violation)\n",
        "\n",
        "            for m in range(M):\n",
        "                if m not in active_indices_kkt:\n",
        "                    nu_for_check = np.min(active_grads) if active_grads else max_grad_h\n",
        "                    eta_m_estimate = nu_for_check - grad_h[m]\n",
        "                    if eta_m_estimate < -check_tol:\n",
        "                        stationarity_violation = max(stationarity_violation, abs(eta_m_estimate))\n",
        "\n",
        "            kkt_violations.append({\n",
        "                'alpha': alpha,\n",
        "                'active_set_report': active_set_str,\n",
        "                'active_set_kkt': str(tuple(sorted(active_indices_kkt))),\n",
        "                'max_violation': stationarity_violation,\n",
        "                'grad_consistency_violation': grad_consistency_violation,\n",
        "                'check_result': 'OK' if stationarity_violation < check_tol * 10 else 'WARN'\n",
        "            })\n",
        "\n",
        "        if PANDAS_AVAILABLE and kkt_violations:\n",
        "            df_kkt = pd.DataFrame(kkt_violations)\n",
        "            kkt_cols_order = ['alpha', 'active_set_report','active_set_kkt', 'max_violation', 'grad_consistency_violation', 'check_result']\n",
        "            kkt_cols_order = [col for col in kkt_cols_order if col in df_kkt.columns]\n",
        "            with pd.option_context('display.float_format', '{:.4e}'.format):\n",
        "                 print(df_kkt[kkt_cols_order].to_string(index=False, na_rep='NaN'))\n",
        "            print(\"Notes on KKT Check:\")\n",
        "            print(\" - active_set_report: Active set reported by inner solver\")\n",
        "            print(\" - active_set_kkt: Active set inferred from w* > tol\")\n",
        "            print(\" - max_violation: Max KKT violation (dual infeasibility or comp. slackness based on eta estimate)\")\n",
        "            print(\" - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\")\n",
        "        else:\n",
        "            print(\"Could not perform KKT check or pandas not available.\")\n",
        "\n",
        "    elif isinstance(results_df, list):\n",
        "        print(\"\\n--- Analysis Results Summary (List - First 5 & Last 5) ---\")\n",
        "        # ()\n",
        "        print(\"\\n(Pandas not available, skipping CSV export)\")\n",
        "\n",
        "    end_time_main = time.time()\n",
        "    print(f\"\\nTotal execution time: {end_time_main - start_time_main:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYejHYkhyVcx",
        "outputId": "440d66fc-f630-4c34-a8eb-5a3fa20eb1f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "--- Searching for Internal Solution around alpha=0.5 [0.400, 0.600] ---\n",
            "--- (Using ActiveSet, mu_tilde=0.02, Milder Profiles) ---\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Starting Full Results Analysis over Alpha Range (using Active Set Method) ---\n",
            "Analyzing alpha = 0.600000 (201/201) ... \n",
            "--- Finished Full Results Analysis ---\n",
            "\n",
            "--- Analysis Results Summary (DataFrame) ---\n",
            "     alpha     H_star active_set     w_fw_0     w_fw_1     w_fw_2       pi_0       pi_1        pi_2       pi_3       pi_4   grad_H_0   grad_H_1   grad_H_2   lambda_0   lambda_1   lambda_2\n",
            "4.0000e-01 9.3420e-04       (2,) 7.6260e-05 9.9985e-01 7.6260e-05 7.7666e-02 9.9536e-02 -4.8332e-02 9.8170e-02 1.2140e-01 5.2231e-04 5.3338e-04 5.3151e-04 0.0000e+00 0.0000e+00 9.3420e-02\n",
            "4.0100e-01 9.3420e-04       (2,) 7.6260e-05 9.9985e-01 7.6260e-05 7.7667e-02 9.9537e-02 -4.8333e-02 9.8170e-02 1.2140e-01 5.2231e-04 5.3335e-04 5.3151e-04 0.0000e+00 0.0000e+00 9.3420e-02\n",
            "4.0200e-01 9.3420e-04       (2,) 7.6260e-05 9.9985e-01 7.6260e-05 7.7667e-02 9.9537e-02 -4.8333e-02 9.8171e-02 1.2140e-01 5.2231e-04 5.3333e-04 5.3151e-04 0.0000e+00 0.0000e+00 9.3420e-02\n",
            "4.0300e-01 9.3420e-04       (2,) 7.7918e-05 9.9984e-01 7.7918e-05 7.7668e-02 9.9538e-02 -4.8333e-02 9.8171e-02 1.2140e-01 5.2231e-04 5.3331e-04 5.3151e-04 0.0000e+00 0.0000e+00 9.3420e-02\n",
            "4.0400e-01 9.3419e-04       (2,) 7.7918e-05 9.9984e-01 7.7918e-05 7.7668e-02 9.9539e-02 -4.8333e-02 9.8172e-02 1.2140e-01 5.2230e-04 5.3329e-04 5.3150e-04 0.0000e+00 0.0000e+00 9.3419e-02\n",
            "4.0500e-01 9.3419e-04       (2,) 7.7918e-05 9.9984e-01 7.7918e-05 7.7669e-02 9.9539e-02 -4.8334e-02 9.8173e-02 1.2140e-01 5.2230e-04 5.3327e-04 5.3150e-04 0.0000e+00 0.0000e+00 9.3419e-02\n",
            "4.0600e-01 9.3419e-04       (2,) 7.7918e-05 9.9984e-01 7.7918e-05 7.7669e-02 9.9540e-02 -4.8334e-02 9.8173e-02 1.2140e-01 5.2230e-04 5.3325e-04 5.3150e-04 0.0000e+00 0.0000e+00 9.3419e-02\n",
            "4.0700e-01 9.3419e-04       (2,) 7.7918e-05 9.9984e-01 7.7918e-05 7.7670e-02 9.9540e-02 -4.8334e-02 9.8174e-02 1.2140e-01 5.2229e-04 5.3323e-04 5.3150e-04 0.0000e+00 0.0000e+00 9.3419e-02\n",
            "4.0800e-01 9.3419e-04       (2,) 7.7918e-05 9.9984e-01 7.7918e-05 7.7670e-02 9.9541e-02 -4.8334e-02 9.8174e-02 1.2140e-01 5.2229e-04 5.3321e-04 5.3149e-04 0.0000e+00 0.0000e+00 9.3419e-02\n",
            "4.0900e-01 9.3418e-04       (2,) 7.7918e-05 9.9984e-01 7.7918e-05 7.7671e-02 9.9542e-02 -4.8335e-02 9.8175e-02 1.2140e-01 5.2229e-04 5.3319e-04 5.3149e-04 0.0000e+00 0.0000e+00 9.3418e-02\n",
            "4.1000e-01 9.3418e-04       (2,) 7.9631e-05 9.9984e-01 7.9631e-05 7.7671e-02 9.9542e-02 -4.8335e-02 9.8175e-02 1.2140e-01 5.2229e-04 5.3317e-04 5.3149e-04 0.0000e+00 0.0000e+00 9.3418e-02\n",
            "4.1100e-01 9.3418e-04       (2,) 7.9631e-05 9.9984e-01 7.9631e-05 7.7672e-02 9.9543e-02 -4.8335e-02 9.8176e-02 1.2140e-01 5.2228e-04 5.3315e-04 5.3149e-04 0.0000e+00 0.0000e+00 9.3418e-02\n",
            "4.1200e-01 9.3418e-04       (2,) 7.9631e-05 9.9984e-01 7.9631e-05 7.7672e-02 9.9543e-02 -4.8335e-02 9.8176e-02 1.2140e-01 5.2228e-04 5.3312e-04 5.3149e-04 0.0000e+00 0.0000e+00 9.3418e-02\n",
            "4.1300e-01 9.3418e-04       (2,) 7.9631e-05 9.9984e-01 7.9631e-05 7.7673e-02 9.9544e-02 -4.8336e-02 9.8177e-02 1.2140e-01 5.2228e-04 5.3310e-04 5.3148e-04 0.0000e+00 0.0000e+00 9.3418e-02\n",
            "4.1400e-01 9.3417e-04       (2,) 7.9631e-05 9.9984e-01 7.9631e-05 7.7673e-02 9.9545e-02 -4.8336e-02 9.8177e-02 1.2141e-01 5.2228e-04 5.3308e-04 5.3148e-04 0.0000e+00 0.0000e+00 9.3417e-02\n",
            "4.1500e-01 9.3417e-04       (2,) 7.9631e-05 9.9984e-01 7.9631e-05 7.7674e-02 9.9545e-02 -4.8336e-02 9.8178e-02 1.2141e-01 5.2227e-04 5.3306e-04 5.3148e-04 0.0000e+00 0.0000e+00 9.3417e-02\n",
            "4.1600e-01 9.3417e-04       (2,) 7.9631e-05 9.9984e-01 7.9631e-05 7.7674e-02 9.9546e-02 -4.8336e-02 9.8178e-02 1.2141e-01 5.2227e-04 5.3304e-04 5.3148e-04 0.0000e+00 0.0000e+00 9.3417e-02\n",
            "4.1700e-01 9.3417e-04       (2,) 7.9631e-05 9.9984e-01 7.9631e-05 7.7675e-02 9.9546e-02 -4.8337e-02 9.8179e-02 1.2141e-01 5.2227e-04 5.3302e-04 5.3147e-04 0.0000e+00 0.0000e+00 9.3417e-02\n",
            "4.1800e-01 9.3417e-04       (2,) 8.1400e-05 9.9984e-01 8.1400e-05 7.7675e-02 9.9547e-02 -4.8337e-02 9.8179e-02 1.2141e-01 5.2226e-04 5.3300e-04 5.3147e-04 0.0000e+00 0.0000e+00 9.3417e-02\n",
            "4.1900e-01 9.3417e-04       (2,) 8.1400e-05 9.9984e-01 8.1400e-05 7.7676e-02 9.9548e-02 -4.8337e-02 9.8180e-02 1.2141e-01 5.2226e-04 5.3298e-04 5.3147e-04 0.0000e+00 0.0000e+00 9.3417e-02\n",
            "4.2000e-01 9.3416e-04       (2,) 8.1400e-05 9.9984e-01 8.1400e-05 7.7676e-02 9.9548e-02 -4.8337e-02 9.8180e-02 1.2141e-01 5.2226e-04 5.3296e-04 5.3147e-04 0.0000e+00 0.0000e+00 9.3416e-02\n",
            "4.2100e-01 9.3416e-04       (2,) 8.1400e-05 9.9984e-01 8.1400e-05 7.7677e-02 9.9549e-02 -4.8338e-02 9.8181e-02 1.2141e-01 5.2226e-04 5.3294e-04 5.3147e-04 0.0000e+00 0.0000e+00 9.3416e-02\n",
            "4.2200e-01 9.3416e-04       (2,) 8.1400e-05 9.9984e-01 8.1400e-05 7.7677e-02 9.9550e-02 -4.8338e-02 9.8182e-02 1.2141e-01 5.2225e-04 5.3292e-04 5.3146e-04 0.0000e+00 0.0000e+00 9.3416e-02\n",
            "4.2300e-01 9.3416e-04       (2,) 8.1400e-05 9.9984e-01 8.1400e-05 7.7678e-02 9.9550e-02 -4.8338e-02 9.8182e-02 1.2141e-01 5.2225e-04 5.3289e-04 5.3146e-04 0.0000e+00 0.0000e+00 9.3416e-02\n",
            "4.2400e-01 9.3416e-04       (2,) 8.1400e-05 9.9984e-01 8.1400e-05 7.7678e-02 9.9551e-02 -4.8338e-02 9.8183e-02 1.2141e-01 5.2225e-04 5.3287e-04 5.3146e-04 0.0000e+00 0.0000e+00 9.3416e-02\n",
            "4.2500e-01 9.3415e-04       (2,) 8.3229e-05 9.9983e-01 8.3229e-05 7.7679e-02 9.9551e-02 -4.8339e-02 9.8183e-02 1.2141e-01 5.2224e-04 5.3285e-04 5.3146e-04 0.0000e+00 0.0000e+00 9.3415e-02\n",
            "4.2600e-01 9.3415e-04       (2,) 8.3229e-05 9.9983e-01 8.3229e-05 7.7679e-02 9.9552e-02 -4.8339e-02 9.8184e-02 1.2141e-01 5.2224e-04 5.3283e-04 5.3145e-04 0.0000e+00 0.0000e+00 9.3415e-02\n",
            "4.2700e-01 9.3415e-04       (2,) 8.3229e-05 9.9983e-01 8.3229e-05 7.7680e-02 9.9553e-02 -4.8339e-02 9.8184e-02 1.2141e-01 5.2224e-04 5.3281e-04 5.3145e-04 0.0000e+00 0.0000e+00 9.3415e-02\n",
            "4.2800e-01 9.3415e-04       (2,) 8.3229e-05 9.9983e-01 8.3229e-05 7.7680e-02 9.9553e-02 -4.8339e-02 9.8185e-02 1.2141e-01 5.2224e-04 5.3279e-04 5.3145e-04 0.0000e+00 0.0000e+00 9.3415e-02\n",
            "4.2900e-01 9.3415e-04       (2,) 8.3229e-05 9.9983e-01 8.3229e-05 7.7681e-02 9.9554e-02 -4.8340e-02 9.8185e-02 1.2141e-01 5.2223e-04 5.3277e-04 5.3145e-04 0.0000e+00 0.0000e+00 9.3415e-02\n",
            "4.3000e-01 9.3415e-04       (2,) 8.3229e-05 9.9983e-01 8.3229e-05 7.7681e-02 9.9554e-02 -4.8340e-02 9.8186e-02 1.2141e-01 5.2223e-04 5.3275e-04 5.3144e-04 0.0000e+00 0.0000e+00 9.3415e-02\n",
            "4.3100e-01 9.3414e-04       (2,) 8.3229e-05 9.9983e-01 8.3229e-05 7.7682e-02 9.9555e-02 -4.8340e-02 9.8186e-02 1.2141e-01 5.2223e-04 5.3273e-04 5.3144e-04 0.0000e+00 0.0000e+00 9.3414e-02\n",
            "4.3200e-01 9.3414e-04       (2,) 8.5121e-05 9.9983e-01 8.5121e-05 7.7682e-02 9.9556e-02 -4.8340e-02 9.8187e-02 1.2141e-01 5.2222e-04 5.3271e-04 5.3144e-04 0.0000e+00 0.0000e+00 9.3414e-02\n",
            "4.3300e-01 9.3414e-04       (2,) 8.5121e-05 9.9983e-01 8.5121e-05 7.7683e-02 9.9556e-02 -4.8341e-02 9.8187e-02 1.2141e-01 5.2222e-04 5.3268e-04 5.3144e-04 0.0000e+00 0.0000e+00 9.3414e-02\n",
            "4.3400e-01 9.3414e-04       (2,) 8.5121e-05 9.9983e-01 8.5121e-05 7.7683e-02 9.9557e-02 -4.8341e-02 9.8188e-02 1.2141e-01 5.2222e-04 5.3266e-04 5.3144e-04 0.0000e+00 0.0000e+00 9.3414e-02\n",
            "4.3500e-01 9.3414e-04       (2,) 8.5121e-05 9.9983e-01 8.5121e-05 7.7684e-02 9.9557e-02 -4.8341e-02 9.8188e-02 1.2142e-01 5.2222e-04 5.3264e-04 5.3143e-04 0.0000e+00 0.0000e+00 9.3414e-02\n",
            "4.3600e-01 9.3413e-04       (2,) 8.5121e-05 9.9983e-01 8.5121e-05 7.7684e-02 9.9558e-02 -4.8341e-02 9.8189e-02 1.2142e-01 5.2221e-04 5.3262e-04 5.3143e-04 0.0000e+00 0.0000e+00 9.3413e-02\n",
            "4.3700e-01 9.3413e-04       (2,) 8.5121e-05 9.9983e-01 8.5121e-05 7.7685e-02 9.9559e-02 -4.8342e-02 9.8189e-02 1.2142e-01 5.2221e-04 5.3260e-04 5.3143e-04 0.0000e+00 0.0000e+00 9.3413e-02\n",
            "4.3800e-01 9.3413e-04       (2,) 8.5121e-05 9.9983e-01 8.5121e-05 7.7685e-02 9.9559e-02 -4.8342e-02 9.8190e-02 1.2142e-01 5.2221e-04 5.3258e-04 5.3143e-04 0.0000e+00 0.0000e+00 9.3413e-02\n",
            "4.3900e-01 9.3413e-04       (2,) 8.5121e-05 9.9983e-01 8.5121e-05 7.7686e-02 9.9560e-02 -4.8342e-02 9.8190e-02 1.2142e-01 5.2221e-04 5.3256e-04 5.3142e-04 0.0000e+00 0.0000e+00 9.3413e-02\n",
            "4.4000e-01 9.3413e-04       (2,) 8.7078e-05 9.9983e-01 8.7078e-05 7.7686e-02 9.9561e-02 -4.8342e-02 9.8191e-02 1.2142e-01 5.2220e-04 5.3254e-04 5.3142e-04 0.0000e+00 0.0000e+00 9.3413e-02\n",
            "4.4100e-01 9.3413e-04       (2,) 8.7078e-05 9.9983e-01 8.7078e-05 7.7687e-02 9.9561e-02 -4.8343e-02 9.8192e-02 1.2142e-01 5.2220e-04 5.3252e-04 5.3142e-04 0.0000e+00 0.0000e+00 9.3413e-02\n",
            "4.4200e-01 9.3412e-04       (2,) 8.7078e-05 9.9983e-01 8.7078e-05 7.7687e-02 9.9562e-02 -4.8343e-02 9.8192e-02 1.2142e-01 5.2220e-04 5.3250e-04 5.3142e-04 0.0000e+00 0.0000e+00 9.3412e-02\n",
            "4.4300e-01 9.3412e-04       (2,) 8.7078e-05 9.9983e-01 8.7078e-05 7.7688e-02 9.9562e-02 -4.8343e-02 9.8193e-02 1.2142e-01 5.2219e-04 5.3248e-04 5.3141e-04 0.0000e+00 0.0000e+00 9.3412e-02\n",
            "4.4400e-01 9.3412e-04       (2,) 8.7078e-05 9.9983e-01 8.7078e-05 7.7688e-02 9.9563e-02 -4.8343e-02 9.8193e-02 1.2142e-01 5.2219e-04 5.3245e-04 5.3141e-04 0.0000e+00 0.0000e+00 9.3412e-02\n",
            "4.4500e-01 9.3412e-04       (2,) 8.7078e-05 9.9983e-01 8.7078e-05 7.7689e-02 9.9564e-02 -4.8344e-02 9.8194e-02 1.2142e-01 5.2219e-04 5.3243e-04 5.3141e-04 0.0000e+00 0.0000e+00 9.3412e-02\n",
            "4.4600e-01 9.3412e-04       (2,) 8.7078e-05 9.9983e-01 8.7078e-05 7.7689e-02 9.9564e-02 -4.8344e-02 9.8194e-02 1.2142e-01 5.2219e-04 5.3241e-04 5.3141e-04 0.0000e+00 0.0000e+00 9.3412e-02\n",
            "4.4700e-01 9.3411e-04       (2,) 8.9103e-05 9.9982e-01 8.9103e-05 7.7690e-02 9.9565e-02 -4.8344e-02 9.8195e-02 1.2142e-01 5.2218e-04 5.3239e-04 5.3141e-04 0.0000e+00 0.0000e+00 9.3411e-02\n",
            "4.4800e-01 9.3411e-04       (2,) 8.9103e-05 9.9982e-01 8.9103e-05 7.7690e-02 9.9565e-02 -4.8344e-02 9.8195e-02 1.2142e-01 5.2218e-04 5.3237e-04 5.3140e-04 0.0000e+00 0.0000e+00 9.3411e-02\n",
            "4.4900e-01 9.3411e-04       (2,) 8.9103e-05 9.9982e-01 8.9103e-05 7.7691e-02 9.9566e-02 -4.8345e-02 9.8196e-02 1.2142e-01 5.2218e-04 5.3235e-04 5.3140e-04 0.0000e+00 0.0000e+00 9.3411e-02\n",
            "4.5000e-01 9.3411e-04       (2,) 8.9103e-05 9.9982e-01 8.9103e-05 7.7691e-02 9.9567e-02 -4.8345e-02 9.8196e-02 1.2142e-01 5.2217e-04 5.3233e-04 5.3140e-04 0.0000e+00 0.0000e+00 9.3411e-02\n",
            "4.5100e-01 9.3411e-04       (2,) 8.9103e-05 9.9982e-01 8.9103e-05 7.7692e-02 9.9567e-02 -4.8345e-02 9.8197e-02 1.2142e-01 5.2217e-04 5.3231e-04 5.3140e-04 0.0000e+00 0.0000e+00 9.3411e-02\n",
            "4.5200e-01 9.3410e-04       (2,) 8.9103e-05 9.9982e-01 8.9103e-05 7.7692e-02 9.9568e-02 -4.8345e-02 9.8197e-02 1.2142e-01 5.2217e-04 5.3229e-04 5.3139e-04 0.0000e+00 0.0000e+00 9.3410e-02\n",
            "4.5300e-01 9.3410e-04       (2,) 8.9103e-05 9.9982e-01 8.9103e-05 7.7693e-02 9.9568e-02 -4.8346e-02 9.8198e-02 1.2142e-01 5.2217e-04 5.3227e-04 5.3139e-04 0.0000e+00 0.0000e+00 9.3410e-02\n",
            "4.5400e-01 9.3410e-04       (2,) 9.1199e-05 9.9982e-01 9.1199e-05 7.7693e-02 9.9569e-02 -4.8346e-02 9.8198e-02 1.2142e-01 5.2216e-04 5.3224e-04 5.3139e-04 0.0000e+00 0.0000e+00 9.3410e-02\n",
            "4.5500e-01 9.3410e-04       (2,) 9.1199e-05 9.9982e-01 9.1199e-05 7.7694e-02 9.9570e-02 -4.8346e-02 9.8199e-02 1.2142e-01 5.2216e-04 5.3222e-04 5.3139e-04 0.0000e+00 0.0000e+00 9.3410e-02\n",
            "4.5600e-01 9.3410e-04       (2,) 9.1199e-05 9.9982e-01 9.1199e-05 7.7694e-02 9.9570e-02 -4.8346e-02 9.8199e-02 1.2143e-01 5.2216e-04 5.3220e-04 5.3139e-04 0.0000e+00 0.0000e+00 9.3410e-02\n",
            "4.5700e-01 9.3410e-04       (2,) 9.1199e-05 9.9982e-01 9.1199e-05 7.7695e-02 9.9571e-02 -4.8347e-02 9.8200e-02 1.2143e-01 5.2216e-04 5.3218e-04 5.3138e-04 0.0000e+00 0.0000e+00 9.3410e-02\n",
            "4.5800e-01 9.3409e-04       (2,) 9.1199e-05 9.9982e-01 9.1199e-05 7.7695e-02 9.9572e-02 -4.8347e-02 9.8201e-02 1.2143e-01 5.2215e-04 5.3216e-04 5.3138e-04 0.0000e+00 0.0000e+00 9.3409e-02\n",
            "4.5900e-01 9.3409e-04       (2,) 9.1199e-05 9.9982e-01 9.1199e-05 7.7696e-02 9.9572e-02 -4.8347e-02 9.8201e-02 1.2143e-01 5.2215e-04 5.3214e-04 5.3138e-04 0.0000e+00 0.0000e+00 9.3409e-02\n",
            "4.6000e-01 9.3409e-04       (2,) 9.1199e-05 9.9982e-01 9.1199e-05 7.7696e-02 9.9573e-02 -4.8347e-02 9.8202e-02 1.2143e-01 5.2215e-04 5.3212e-04 5.3138e-04 0.0000e+00 0.0000e+00 9.3409e-02\n",
            "4.6100e-01 9.3409e-04       (2,) 9.3371e-05 9.9981e-01 9.3371e-05 7.7697e-02 9.9573e-02 -4.8348e-02 9.8202e-02 1.2143e-01 5.2214e-04 5.3210e-04 5.3137e-04 0.0000e+00 0.0000e+00 9.3409e-02\n",
            "4.6200e-01 9.3409e-04       (2,) 9.3371e-05 9.9981e-01 9.3371e-05 7.7697e-02 9.9574e-02 -4.8348e-02 9.8203e-02 1.2143e-01 5.2214e-04 5.3208e-04 5.3137e-04 0.0000e+00 0.0000e+00 9.3409e-02\n",
            "4.6300e-01 9.3408e-04       (2,) 9.3371e-05 9.9981e-01 9.3371e-05 7.7698e-02 9.9575e-02 -4.8348e-02 9.8203e-02 1.2143e-01 5.2214e-04 5.3206e-04 5.3137e-04 0.0000e+00 0.0000e+00 9.3408e-02\n",
            "4.6400e-01 9.3408e-04       (2,) 9.3371e-05 9.9981e-01 9.3371e-05 7.7698e-02 9.9575e-02 -4.8348e-02 9.8204e-02 1.2143e-01 5.2214e-04 5.3204e-04 5.3137e-04 0.0000e+00 0.0000e+00 9.3408e-02\n",
            "4.6500e-01 9.3408e-04       (2,) 9.3371e-05 9.9981e-01 9.3371e-05 7.7699e-02 9.9576e-02 -4.8349e-02 9.8204e-02 1.2143e-01 5.2213e-04 5.3201e-04 5.3136e-04 0.0000e+00 0.0000e+00 9.3408e-02\n",
            "4.6600e-01 9.3408e-04       (2,) 9.3371e-05 9.9981e-01 9.3371e-05 7.7700e-02 9.9576e-02 -4.8349e-02 9.8205e-02 1.2143e-01 5.2213e-04 5.3199e-04 5.3136e-04 0.0000e+00 0.0000e+00 9.3408e-02\n",
            "4.6700e-01 9.3408e-04       (2,) 9.5621e-05 9.9981e-01 9.5621e-05 7.7700e-02 9.9577e-02 -4.8349e-02 9.8205e-02 1.2143e-01 5.2213e-04 5.3197e-04 5.3136e-04 0.0000e+00 0.0000e+00 9.3408e-02\n",
            "4.6800e-01 9.3408e-04       (2,) 9.5621e-05 9.9981e-01 9.5621e-05 7.7701e-02 9.9578e-02 -4.8349e-02 9.8206e-02 1.2143e-01 5.2212e-04 5.3195e-04 5.3136e-04 0.0000e+00 0.0000e+00 9.3408e-02\n",
            "4.6900e-01 9.3407e-04       (2,) 9.5621e-05 9.9981e-01 9.5621e-05 7.7701e-02 9.9578e-02 -4.8350e-02 9.8206e-02 1.2143e-01 5.2212e-04 5.3193e-04 5.3136e-04 0.0000e+00 0.0000e+00 9.3407e-02\n",
            "4.7000e-01 9.3407e-04       (2,) 9.5621e-05 9.9981e-01 9.5621e-05 7.7702e-02 9.9579e-02 -4.8350e-02 9.8207e-02 1.2143e-01 5.2212e-04 5.3191e-04 5.3135e-04 0.0000e+00 0.0000e+00 9.3407e-02\n",
            "4.7100e-01 9.3407e-04       (2,) 9.5621e-05 9.9981e-01 9.5621e-05 7.7702e-02 9.9579e-02 -4.8350e-02 9.8207e-02 1.2143e-01 5.2212e-04 5.3189e-04 5.3135e-04 0.0000e+00 0.0000e+00 9.3407e-02\n",
            "4.7200e-01 9.3407e-04       (2,) 9.5621e-05 9.9981e-01 9.5621e-05 7.7703e-02 9.9580e-02 -4.8350e-02 9.8208e-02 1.2143e-01 5.2211e-04 5.3187e-04 5.3135e-04 0.0000e+00 0.0000e+00 9.3407e-02\n",
            "4.7300e-01 9.3407e-04       (2,) 9.5621e-05 9.9981e-01 9.5621e-05 7.7703e-02 9.9581e-02 -4.8351e-02 9.8208e-02 1.2143e-01 5.2211e-04 5.3185e-04 5.3135e-04 0.0000e+00 0.0000e+00 9.3407e-02\n",
            "4.7400e-01 9.3406e-04       (2,) 9.7953e-05 9.9980e-01 9.7953e-05 7.7704e-02 9.9581e-02 -4.8351e-02 9.8209e-02 1.2143e-01 5.2211e-04 5.3183e-04 5.3134e-04 0.0000e+00 0.0000e+00 9.3406e-02\n",
            "4.7500e-01 9.3406e-04       (2,) 9.7953e-05 9.9980e-01 9.7953e-05 7.7704e-02 9.9582e-02 -4.8351e-02 9.8210e-02 1.2143e-01 5.2210e-04 5.3180e-04 5.3134e-04 0.0000e+00 0.0000e+00 9.3406e-02\n",
            "4.7600e-01 9.3406e-04       (2,) 9.7953e-05 9.9980e-01 9.7953e-05 7.7705e-02 9.9583e-02 -4.8351e-02 9.8210e-02 1.2143e-01 5.2210e-04 5.3178e-04 5.3134e-04 0.0000e+00 0.0000e+00 9.3406e-02\n",
            "4.7700e-01 9.3406e-04       (2,) 9.7953e-05 9.9980e-01 9.7953e-05 7.7705e-02 9.9583e-02 -4.8352e-02 9.8211e-02 1.2144e-01 5.2210e-04 5.3176e-04 5.3134e-04 0.0000e+00 0.0000e+00 9.3406e-02\n",
            "4.7800e-01 9.3406e-04       (2,) 9.7953e-05 9.9980e-01 9.7953e-05 7.7706e-02 9.9584e-02 -4.8352e-02 9.8211e-02 1.2144e-01 5.2210e-04 5.3174e-04 5.3133e-04 0.0000e+00 0.0000e+00 9.3406e-02\n",
            "4.7900e-01 9.3406e-04       (2,) 9.7953e-05 9.9980e-01 9.7953e-05 7.7706e-02 9.9584e-02 -4.8352e-02 9.8212e-02 1.2144e-01 5.2209e-04 5.3172e-04 5.3133e-04 0.0000e+00 0.0000e+00 9.3406e-02\n",
            "4.8000e-01 9.3405e-04       (2,) 9.7953e-05 9.9980e-01 9.7953e-05 7.7707e-02 9.9585e-02 -4.8352e-02 9.8212e-02 1.2144e-01 5.2209e-04 5.3170e-04 5.3133e-04 0.0000e+00 0.0000e+00 9.3405e-02\n",
            "4.8100e-01 9.3405e-04       (2,) 1.0037e-04 9.9980e-01 1.0037e-04 7.7707e-02 9.9586e-02 -4.8353e-02 9.8213e-02 1.2144e-01 5.2209e-04 5.3168e-04 5.3133e-04 0.0000e+00 0.0000e+00 9.3405e-02\n",
            "4.8200e-01 9.3405e-04       (2,) 1.0037e-04 9.9980e-01 1.0037e-04 7.7708e-02 9.9586e-02 -4.8353e-02 9.8213e-02 1.2144e-01 5.2209e-04 5.3166e-04 5.3133e-04 0.0000e+00 0.0000e+00 9.3405e-02\n",
            "4.8300e-01 9.3405e-04       (2,) 1.0037e-04 9.9980e-01 1.0037e-04 7.7708e-02 9.9587e-02 -4.8353e-02 9.8214e-02 1.2144e-01 5.2208e-04 5.3164e-04 5.3132e-04 0.0000e+00 0.0000e+00 9.3405e-02\n",
            "4.8400e-01 9.3405e-04       (2,) 1.0037e-04 9.9980e-01 1.0037e-04 7.7709e-02 9.9587e-02 -4.8353e-02 9.8214e-02 1.2144e-01 5.2208e-04 5.3162e-04 5.3132e-04 0.0000e+00 0.0000e+00 9.3405e-02\n",
            "4.8500e-01 9.3404e-04       (2,) 1.0037e-04 9.9980e-01 1.0037e-04 7.7709e-02 9.9588e-02 -4.8354e-02 9.8215e-02 1.2144e-01 5.2208e-04 5.3159e-04 5.3132e-04 0.0000e+00 0.0000e+00 9.3404e-02\n",
            "4.8600e-01 9.3404e-04       (2,) 1.0037e-04 9.9980e-01 1.0037e-04 7.7710e-02 9.9589e-02 -4.8354e-02 9.8215e-02 1.2144e-01 5.2207e-04 5.3157e-04 5.3132e-04 0.0000e+00 0.0000e+00 9.3404e-02\n",
            "4.8700e-01 9.3404e-04       (2,) 1.0288e-04 9.9979e-01 1.0288e-04 7.7710e-02 9.9589e-02 -4.8354e-02 9.8216e-02 1.2144e-01 5.2207e-04 5.3155e-04 5.3131e-04 0.0000e+00 0.0000e+00 9.3404e-02\n",
            "4.8800e-01 9.3404e-04       (2,) 1.0288e-04 9.9979e-01 1.0288e-04 7.7711e-02 9.9590e-02 -4.8354e-02 9.8216e-02 1.2144e-01 5.2207e-04 5.3153e-04 5.3131e-04 0.0000e+00 0.0000e+00 9.3404e-02\n",
            "4.8900e-01 9.3404e-04       (2,) 1.0288e-04 9.9979e-01 1.0288e-04 7.7711e-02 9.9590e-02 -4.8355e-02 9.8217e-02 1.2144e-01 5.2207e-04 5.3151e-04 5.3131e-04 0.0000e+00 0.0000e+00 9.3404e-02\n",
            "4.9000e-01 9.3403e-04       (2,) 1.0288e-04 9.9979e-01 1.0288e-04 7.7712e-02 9.9591e-02 -4.8355e-02 9.8217e-02 1.2144e-01 5.2206e-04 5.3149e-04 5.3131e-04 0.0000e+00 0.0000e+00 9.3403e-02\n",
            "4.9100e-01 9.3403e-04       (2,) 1.0288e-04 9.9979e-01 1.0288e-04 7.7712e-02 9.9592e-02 -4.8355e-02 9.8218e-02 1.2144e-01 5.2206e-04 5.3147e-04 5.3131e-04 0.0000e+00 0.0000e+00 9.3403e-02\n",
            "4.9200e-01 9.3403e-04       (2,) 1.0288e-04 9.9979e-01 1.0288e-04 7.7713e-02 9.9592e-02 -4.8355e-02 9.8219e-02 1.2144e-01 5.2206e-04 5.3145e-04 5.3130e-04 0.0000e+00 0.0000e+00 9.3403e-02\n",
            "4.9300e-01 9.3403e-04       (2,) 1.0288e-04 9.9979e-01 1.0288e-04 7.7713e-02 9.9593e-02 -4.8356e-02 9.8219e-02 1.2144e-01 5.2206e-04 5.3143e-04 5.3130e-04 0.0000e+00 0.0000e+00 9.3403e-02\n",
            "4.9400e-01 9.3403e-04       (2,) 1.0549e-04 9.9979e-01 1.0549e-04 7.7714e-02 9.9594e-02 -4.8356e-02 9.8220e-02 1.2144e-01 5.2205e-04 5.3141e-04 5.3130e-04 0.0000e+00 0.0000e+00 9.3403e-02\n",
            "4.9500e-01 9.3403e-04       (2,) 1.0549e-04 9.9979e-01 1.0549e-04 7.7714e-02 9.9594e-02 -4.8356e-02 9.8220e-02 1.2144e-01 5.2205e-04 5.3139e-04 5.3130e-04 0.0000e+00 0.0000e+00 9.3403e-02\n",
            "4.9600e-01 9.3402e-04       (2,) 1.0549e-04 9.9979e-01 1.0549e-04 7.7715e-02 9.9595e-02 -4.8356e-02 9.8221e-02 1.2144e-01 5.2205e-04 5.3136e-04 5.3129e-04 0.0000e+00 0.0000e+00 9.3402e-02\n",
            "4.9700e-01 9.3402e-04       (2,) 1.0549e-04 9.9979e-01 1.0549e-04 7.7715e-02 9.9595e-02 -4.8357e-02 9.8221e-02 1.2144e-01 5.2204e-04 5.3134e-04 5.3129e-04 0.0000e+00 0.0000e+00 9.3402e-02\n",
            "4.9800e-01 9.3402e-04       (2,) 1.0549e-04 9.9979e-01 1.0549e-04 7.7716e-02 9.9596e-02 -4.8357e-02 9.8222e-02 1.2144e-01 5.2204e-04 5.3132e-04 5.3129e-04 0.0000e+00 0.0000e+00 9.3402e-02\n",
            "4.9900e-01 9.3402e-04       (2,) 1.0549e-04 9.9979e-01 1.0549e-04 7.7716e-02 9.9597e-02 -4.8357e-02 9.8222e-02 1.2145e-01 5.2204e-04 5.3130e-04 5.3129e-04 0.0000e+00 0.0000e+00 9.3402e-02\n",
            "5.0000e-01 9.3402e-04       (2,) 1.0288e-04 5.0566e-01 4.9424e-01 7.7717e-02 9.9597e-02 -4.8357e-02 9.8223e-02 1.2145e-01 5.2341e-04 5.3263e-04 5.3263e-04 0.0000e+00 0.0000e+00 9.3402e-02\n",
            "5.0100e-01 9.3403e-04       (2,) 1.0819e-04 1.0819e-04 9.9978e-01 7.7717e-02 9.9598e-02 -4.8358e-02 9.8223e-02 1.2145e-01 5.2482e-04 5.3402e-04 5.3403e-04 0.0000e+00 0.0000e+00 9.3403e-02\n",
            "5.0200e-01 9.3405e-04       (2,) 1.0819e-04 1.0819e-04 9.9978e-01 7.7718e-02 9.9598e-02 -4.8358e-02 9.8224e-02 1.2145e-01 5.2484e-04 5.3402e-04 5.3405e-04 0.0000e+00 0.0000e+00 9.3405e-02\n",
            "5.0300e-01 9.3407e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7718e-02 9.9599e-02 -4.8358e-02 9.8224e-02 1.2145e-01 5.2486e-04 5.3401e-04 5.3407e-04 0.0000e+00 0.0000e+00 9.3407e-02\n",
            "5.0400e-01 9.3408e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7719e-02 9.9600e-02 -4.8358e-02 9.8225e-02 1.2145e-01 5.2487e-04 5.3401e-04 5.3408e-04 0.0000e+00 0.0000e+00 9.3408e-02\n",
            "5.0500e-01 9.3410e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7719e-02 9.9600e-02 -4.8359e-02 9.8225e-02 1.2145e-01 5.2489e-04 5.3401e-04 5.3410e-04 0.0000e+00 0.0000e+00 9.3410e-02\n",
            "5.0600e-01 9.3412e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7720e-02 9.9601e-02 -4.8359e-02 9.8226e-02 1.2145e-01 5.2491e-04 5.3401e-04 5.3412e-04 0.0000e+00 0.0000e+00 9.3412e-02\n",
            "5.0700e-01 9.3413e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7720e-02 9.9601e-02 -4.8359e-02 9.8226e-02 1.2145e-01 5.2492e-04 5.3401e-04 5.3413e-04 0.0000e+00 0.0000e+00 9.3413e-02\n",
            "5.0800e-01 9.3415e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7721e-02 9.9602e-02 -4.8359e-02 9.8227e-02 1.2145e-01 5.2494e-04 5.3401e-04 5.3415e-04 0.0000e+00 0.0000e+00 9.3415e-02\n",
            "5.0900e-01 9.3417e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7721e-02 9.9603e-02 -4.8360e-02 9.8228e-02 1.2145e-01 5.2496e-04 5.3400e-04 5.3417e-04 0.0000e+00 0.0000e+00 9.3417e-02\n",
            "5.1000e-01 9.3418e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7722e-02 9.9603e-02 -4.8360e-02 9.8228e-02 1.2145e-01 5.2497e-04 5.3400e-04 5.3418e-04 0.0000e+00 0.0000e+00 9.3418e-02\n",
            "5.1100e-01 9.3420e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7722e-02 9.9604e-02 -4.8360e-02 9.8229e-02 1.2145e-01 5.2499e-04 5.3400e-04 5.3420e-04 0.0000e+00 0.0000e+00 9.3420e-02\n",
            "5.1200e-01 9.3422e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7723e-02 9.9605e-02 -4.8360e-02 9.8229e-02 1.2145e-01 5.2501e-04 5.3400e-04 5.3422e-04 0.0000e+00 0.0000e+00 9.3422e-02\n",
            "5.1300e-01 9.3423e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7723e-02 9.9605e-02 -4.8361e-02 9.8230e-02 1.2145e-01 5.2502e-04 5.3400e-04 5.3423e-04 0.0000e+00 0.0000e+00 9.3423e-02\n",
            "5.1400e-01 9.3425e-04       (2,) 1.0549e-04 1.0549e-04 9.9979e-01 7.7724e-02 9.9606e-02 -4.8361e-02 9.8230e-02 1.2145e-01 5.2504e-04 5.3400e-04 5.3425e-04 0.0000e+00 0.0000e+00 9.3425e-02\n",
            "5.1500e-01 9.3427e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7724e-02 9.9606e-02 -4.8361e-02 9.8231e-02 1.2145e-01 5.2506e-04 5.3399e-04 5.3427e-04 0.0000e+00 0.0000e+00 9.3427e-02\n",
            "5.1600e-01 9.3429e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7725e-02 9.9607e-02 -4.8361e-02 9.8231e-02 1.2145e-01 5.2507e-04 5.3399e-04 5.3428e-04 0.0000e+00 0.0000e+00 9.3429e-02\n",
            "5.1700e-01 9.3430e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7725e-02 9.9608e-02 -4.8362e-02 9.8232e-02 1.2145e-01 5.2509e-04 5.3399e-04 5.3430e-04 0.0000e+00 0.0000e+00 9.3430e-02\n",
            "5.1800e-01 9.3432e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7726e-02 9.9608e-02 -4.8362e-02 9.8232e-02 1.2145e-01 5.2511e-04 5.3399e-04 5.3432e-04 0.0000e+00 0.0000e+00 9.3432e-02\n",
            "5.1900e-01 9.3434e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7726e-02 9.9609e-02 -4.8362e-02 9.8233e-02 1.2145e-01 5.2512e-04 5.3399e-04 5.3433e-04 0.0000e+00 0.0000e+00 9.3434e-02\n",
            "5.2000e-01 9.3435e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7727e-02 9.9609e-02 -4.8362e-02 9.8233e-02 1.2146e-01 5.2514e-04 5.3398e-04 5.3435e-04 0.0000e+00 0.0000e+00 9.3435e-02\n",
            "5.2100e-01 9.3437e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7727e-02 9.9610e-02 -4.8363e-02 9.8234e-02 1.2146e-01 5.2516e-04 5.3398e-04 5.3437e-04 0.0000e+00 0.0000e+00 9.3437e-02\n",
            "5.2200e-01 9.3439e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7728e-02 9.9611e-02 -4.8363e-02 9.8234e-02 1.2146e-01 5.2517e-04 5.3398e-04 5.3439e-04 0.0000e+00 0.0000e+00 9.3439e-02\n",
            "5.2300e-01 9.3440e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7728e-02 9.9611e-02 -4.8363e-02 9.8235e-02 1.2146e-01 5.2519e-04 5.3398e-04 5.3440e-04 0.0000e+00 0.0000e+00 9.3440e-02\n",
            "5.2400e-01 9.3442e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7729e-02 9.9612e-02 -4.8363e-02 9.8235e-02 1.2146e-01 5.2521e-04 5.3398e-04 5.3442e-04 0.0000e+00 0.0000e+00 9.3442e-02\n",
            "5.2500e-01 9.3444e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7729e-02 9.9612e-02 -4.8364e-02 9.8236e-02 1.2146e-01 5.2522e-04 5.3398e-04 5.3444e-04 0.0000e+00 0.0000e+00 9.3444e-02\n",
            "5.2600e-01 9.3445e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7730e-02 9.9613e-02 -4.8364e-02 9.8236e-02 1.2146e-01 5.2524e-04 5.3397e-04 5.3445e-04 0.0000e+00 0.0000e+00 9.3445e-02\n",
            "5.2700e-01 9.3447e-04       (2,) 1.0288e-04 1.0288e-04 9.9979e-01 7.7730e-02 9.9614e-02 -4.8364e-02 9.8237e-02 1.2146e-01 5.2526e-04 5.3397e-04 5.3447e-04 0.0000e+00 0.0000e+00 9.3447e-02\n",
            "5.2800e-01 9.3449e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7731e-02 9.9614e-02 -4.8364e-02 9.8238e-02 1.2146e-01 5.2527e-04 5.3397e-04 5.3449e-04 0.0000e+00 0.0000e+00 9.3449e-02\n",
            "5.2900e-01 9.3450e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7731e-02 9.9615e-02 -4.8365e-02 9.8238e-02 1.2146e-01 5.2529e-04 5.3397e-04 5.3450e-04 0.0000e+00 0.0000e+00 9.3450e-02\n",
            "5.3000e-01 9.3452e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7732e-02 9.9616e-02 -4.8365e-02 9.8239e-02 1.2146e-01 5.2531e-04 5.3397e-04 5.3452e-04 0.0000e+00 0.0000e+00 9.3452e-02\n",
            "5.3100e-01 9.3454e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7732e-02 9.9616e-02 -4.8365e-02 9.8239e-02 1.2146e-01 5.2532e-04 5.3397e-04 5.3454e-04 0.0000e+00 0.0000e+00 9.3454e-02\n",
            "5.3200e-01 9.3455e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7733e-02 9.9617e-02 -4.8365e-02 9.8240e-02 1.2146e-01 5.2534e-04 5.3396e-04 5.3455e-04 0.0000e+00 0.0000e+00 9.3455e-02\n",
            "5.3300e-01 9.3457e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7733e-02 9.9617e-02 -4.8366e-02 9.8240e-02 1.2146e-01 5.2536e-04 5.3396e-04 5.3457e-04 0.0000e+00 0.0000e+00 9.3457e-02\n",
            "5.3400e-01 9.3459e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7734e-02 9.9618e-02 -4.8366e-02 9.8241e-02 1.2146e-01 5.2537e-04 5.3396e-04 5.3459e-04 0.0000e+00 0.0000e+00 9.3459e-02\n",
            "5.3500e-01 9.3460e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7735e-02 9.9619e-02 -4.8366e-02 9.8241e-02 1.2146e-01 5.2539e-04 5.3396e-04 5.3460e-04 0.0000e+00 0.0000e+00 9.3460e-02\n",
            "5.3600e-01 9.3462e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7735e-02 9.9619e-02 -4.8366e-02 9.8242e-02 1.2146e-01 5.2541e-04 5.3396e-04 5.3462e-04 0.0000e+00 0.0000e+00 9.3462e-02\n",
            "5.3700e-01 9.3464e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7736e-02 9.9620e-02 -4.8367e-02 9.8242e-02 1.2146e-01 5.2542e-04 5.3395e-04 5.3464e-04 0.0000e+00 0.0000e+00 9.3464e-02\n",
            "5.3800e-01 9.3465e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7736e-02 9.9620e-02 -4.8367e-02 9.8243e-02 1.2146e-01 5.2544e-04 5.3395e-04 5.3465e-04 0.0000e+00 0.0000e+00 9.3465e-02\n",
            "5.3900e-01 9.3467e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7737e-02 9.9621e-02 -4.8367e-02 9.8243e-02 1.2146e-01 5.2546e-04 5.3395e-04 5.3467e-04 0.0000e+00 0.0000e+00 9.3467e-02\n",
            "5.4000e-01 9.3469e-04       (2,) 1.0037e-04 1.0037e-04 9.9980e-01 7.7737e-02 9.9622e-02 -4.8367e-02 9.8244e-02 1.2146e-01 5.2547e-04 5.3395e-04 5.3469e-04 0.0000e+00 0.0000e+00 9.3469e-02\n",
            "5.4100e-01 9.3471e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7738e-02 9.9622e-02 -4.8368e-02 9.8244e-02 1.2147e-01 5.2549e-04 5.3395e-04 5.3470e-04 0.0000e+00 0.0000e+00 9.3471e-02\n",
            "5.4200e-01 9.3472e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7738e-02 9.9623e-02 -4.8368e-02 9.8245e-02 1.2147e-01 5.2551e-04 5.3395e-04 5.3472e-04 0.0000e+00 0.0000e+00 9.3472e-02\n",
            "5.4300e-01 9.3474e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7739e-02 9.9624e-02 -4.8368e-02 9.8245e-02 1.2147e-01 5.2552e-04 5.3394e-04 5.3474e-04 0.0000e+00 0.0000e+00 9.3474e-02\n",
            "5.4400e-01 9.3476e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7739e-02 9.9624e-02 -4.8368e-02 9.8246e-02 1.2147e-01 5.2554e-04 5.3394e-04 5.3476e-04 0.0000e+00 0.0000e+00 9.3476e-02\n",
            "5.4500e-01 9.3477e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7740e-02 9.9625e-02 -4.8369e-02 9.8247e-02 1.2147e-01 5.2555e-04 5.3394e-04 5.3477e-04 0.0000e+00 0.0000e+00 9.3477e-02\n",
            "5.4600e-01 9.3479e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7740e-02 9.9625e-02 -4.8369e-02 9.8247e-02 1.2147e-01 5.2557e-04 5.3394e-04 5.3479e-04 0.0000e+00 0.0000e+00 9.3479e-02\n",
            "5.4700e-01 9.3481e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7741e-02 9.9626e-02 -4.8369e-02 9.8248e-02 1.2147e-01 5.2559e-04 5.3394e-04 5.3481e-04 0.0000e+00 0.0000e+00 9.3481e-02\n",
            "5.4800e-01 9.3482e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7741e-02 9.9627e-02 -4.8369e-02 9.8248e-02 1.2147e-01 5.2560e-04 5.3394e-04 5.3482e-04 0.0000e+00 0.0000e+00 9.3482e-02\n",
            "5.4900e-01 9.3484e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7742e-02 9.9627e-02 -4.8370e-02 9.8249e-02 1.2147e-01 5.2562e-04 5.3393e-04 5.3484e-04 0.0000e+00 0.0000e+00 9.3484e-02\n",
            "5.5000e-01 9.3486e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7742e-02 9.9628e-02 -4.8370e-02 9.8249e-02 1.2147e-01 5.2564e-04 5.3393e-04 5.3486e-04 0.0000e+00 0.0000e+00 9.3486e-02\n",
            "5.5100e-01 9.3487e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7743e-02 9.9628e-02 -4.8370e-02 9.8250e-02 1.2147e-01 5.2565e-04 5.3393e-04 5.3487e-04 0.0000e+00 0.0000e+00 9.3487e-02\n",
            "5.5200e-01 9.3489e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7743e-02 9.9629e-02 -4.8370e-02 9.8250e-02 1.2147e-01 5.2567e-04 5.3393e-04 5.3489e-04 0.0000e+00 0.0000e+00 9.3489e-02\n",
            "5.5300e-01 9.3491e-04       (2,) 9.7953e-05 9.7953e-05 9.9980e-01 7.7744e-02 9.9630e-02 -4.8371e-02 9.8251e-02 1.2147e-01 5.2569e-04 5.3393e-04 5.3491e-04 0.0000e+00 0.0000e+00 9.3491e-02\n",
            "5.5400e-01 9.3492e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7744e-02 9.9630e-02 -4.8371e-02 9.8251e-02 1.2147e-01 5.2570e-04 5.3392e-04 5.3492e-04 0.0000e+00 0.0000e+00 9.3492e-02\n",
            "5.5500e-01 9.3494e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7745e-02 9.9631e-02 -4.8371e-02 9.8252e-02 1.2147e-01 5.2572e-04 5.3392e-04 5.3494e-04 0.0000e+00 0.0000e+00 9.3494e-02\n",
            "5.5600e-01 9.3496e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7745e-02 9.9631e-02 -4.8371e-02 9.8252e-02 1.2147e-01 5.2574e-04 5.3392e-04 5.3496e-04 0.0000e+00 0.0000e+00 9.3496e-02\n",
            "5.5700e-01 9.3497e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7746e-02 9.9632e-02 -4.8372e-02 9.8253e-02 1.2147e-01 5.2575e-04 5.3392e-04 5.3497e-04 0.0000e+00 0.0000e+00 9.3497e-02\n",
            "5.5800e-01 9.3499e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7746e-02 9.9633e-02 -4.8372e-02 9.8253e-02 1.2147e-01 5.2577e-04 5.3392e-04 5.3499e-04 0.0000e+00 0.0000e+00 9.3499e-02\n",
            "5.5900e-01 9.3501e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7747e-02 9.9633e-02 -4.8372e-02 9.8254e-02 1.2147e-01 5.2579e-04 5.3392e-04 5.3501e-04 0.0000e+00 0.0000e+00 9.3501e-02\n",
            "5.6000e-01 9.3502e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7747e-02 9.9634e-02 -4.8372e-02 9.8254e-02 1.2147e-01 5.2580e-04 5.3391e-04 5.3502e-04 0.0000e+00 0.0000e+00 9.3502e-02\n",
            "5.6100e-01 9.3504e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7748e-02 9.9635e-02 -4.8373e-02 9.8255e-02 1.2147e-01 5.2582e-04 5.3391e-04 5.3504e-04 0.0000e+00 0.0000e+00 9.3504e-02\n",
            "5.6200e-01 9.3506e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7748e-02 9.9635e-02 -4.8373e-02 9.8256e-02 1.2147e-01 5.2584e-04 5.3391e-04 5.3506e-04 0.0000e+00 0.0000e+00 9.3506e-02\n",
            "5.6300e-01 9.3508e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7749e-02 9.9636e-02 -4.8373e-02 9.8256e-02 1.2148e-01 5.2585e-04 5.3391e-04 5.3507e-04 0.0000e+00 0.0000e+00 9.3508e-02\n",
            "5.6400e-01 9.3509e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7749e-02 9.9636e-02 -4.8373e-02 9.8257e-02 1.2148e-01 5.2587e-04 5.3391e-04 5.3509e-04 0.0000e+00 0.0000e+00 9.3509e-02\n",
            "5.6500e-01 9.3511e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7750e-02 9.9637e-02 -4.8374e-02 9.8257e-02 1.2148e-01 5.2589e-04 5.3391e-04 5.3511e-04 0.0000e+00 0.0000e+00 9.3511e-02\n",
            "5.6600e-01 9.3513e-04       (2,) 9.5621e-05 9.5621e-05 9.9981e-01 7.7750e-02 9.9638e-02 -4.8374e-02 9.8258e-02 1.2148e-01 5.2590e-04 5.3390e-04 5.3513e-04 0.0000e+00 0.0000e+00 9.3513e-02\n",
            "5.6700e-01 9.3514e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7751e-02 9.9638e-02 -4.8374e-02 9.8258e-02 1.2148e-01 5.2592e-04 5.3390e-04 5.3514e-04 0.0000e+00 0.0000e+00 9.3514e-02\n",
            "5.6800e-01 9.3516e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7751e-02 9.9639e-02 -4.8374e-02 9.8259e-02 1.2148e-01 5.2594e-04 5.3390e-04 5.3516e-04 0.0000e+00 0.0000e+00 9.3516e-02\n",
            "5.6900e-01 9.3518e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7752e-02 9.9639e-02 -4.8375e-02 9.8259e-02 1.2148e-01 5.2595e-04 5.3390e-04 5.3518e-04 0.0000e+00 0.0000e+00 9.3518e-02\n",
            "5.7000e-01 9.3519e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7752e-02 9.9640e-02 -4.8375e-02 9.8260e-02 1.2148e-01 5.2597e-04 5.3390e-04 5.3519e-04 0.0000e+00 0.0000e+00 9.3519e-02\n",
            "5.7100e-01 9.3521e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7753e-02 9.9641e-02 -4.8375e-02 9.8260e-02 1.2148e-01 5.2599e-04 5.3389e-04 5.3521e-04 0.0000e+00 0.0000e+00 9.3521e-02\n",
            "5.7200e-01 9.3523e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7753e-02 9.9641e-02 -4.8375e-02 9.8261e-02 1.2148e-01 5.2600e-04 5.3389e-04 5.3523e-04 0.0000e+00 0.0000e+00 9.3523e-02\n",
            "5.7300e-01 9.3524e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7754e-02 9.9642e-02 -4.8376e-02 9.8261e-02 1.2148e-01 5.2602e-04 5.3389e-04 5.3524e-04 0.0000e+00 0.0000e+00 9.3524e-02\n",
            "5.7400e-01 9.3526e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7754e-02 9.9642e-02 -4.8376e-02 9.8262e-02 1.2148e-01 5.2604e-04 5.3389e-04 5.3526e-04 0.0000e+00 0.0000e+00 9.3526e-02\n",
            "5.7500e-01 9.3528e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7755e-02 9.9643e-02 -4.8376e-02 9.8262e-02 1.2148e-01 5.2605e-04 5.3389e-04 5.3528e-04 0.0000e+00 0.0000e+00 9.3528e-02\n",
            "5.7600e-01 9.3529e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7755e-02 9.9644e-02 -4.8376e-02 9.8263e-02 1.2148e-01 5.2607e-04 5.3389e-04 5.3529e-04 0.0000e+00 0.0000e+00 9.3529e-02\n",
            "5.7700e-01 9.3531e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7756e-02 9.9644e-02 -4.8377e-02 9.8263e-02 1.2148e-01 5.2609e-04 5.3388e-04 5.3531e-04 0.0000e+00 0.0000e+00 9.3531e-02\n",
            "5.7800e-01 9.3533e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7756e-02 9.9645e-02 -4.8377e-02 9.8264e-02 1.2148e-01 5.2610e-04 5.3388e-04 5.3533e-04 0.0000e+00 0.0000e+00 9.3533e-02\n",
            "5.7900e-01 9.3534e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7757e-02 9.9646e-02 -4.8377e-02 9.8265e-02 1.2148e-01 5.2612e-04 5.3388e-04 5.3534e-04 0.0000e+00 0.0000e+00 9.3534e-02\n",
            "5.8000e-01 9.3536e-04       (2,) 9.3371e-05 9.3371e-05 9.9981e-01 7.7757e-02 9.9646e-02 -4.8377e-02 9.8265e-02 1.2148e-01 5.2614e-04 5.3388e-04 5.3536e-04 0.0000e+00 0.0000e+00 9.3536e-02\n",
            "5.8100e-01 9.3538e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7758e-02 9.9647e-02 -4.8378e-02 9.8266e-02 1.2148e-01 5.2615e-04 5.3388e-04 5.3538e-04 0.0000e+00 0.0000e+00 9.3538e-02\n",
            "5.8200e-01 9.3540e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7758e-02 9.9647e-02 -4.8378e-02 9.8266e-02 1.2148e-01 5.2617e-04 5.3388e-04 5.3539e-04 0.0000e+00 0.0000e+00 9.3540e-02\n",
            "5.8300e-01 9.3541e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7759e-02 9.9648e-02 -4.8378e-02 9.8267e-02 1.2148e-01 5.2619e-04 5.3387e-04 5.3541e-04 0.0000e+00 0.0000e+00 9.3541e-02\n",
            "5.8400e-01 9.3543e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7759e-02 9.9649e-02 -4.8378e-02 9.8267e-02 1.2149e-01 5.2620e-04 5.3387e-04 5.3543e-04 0.0000e+00 0.0000e+00 9.3543e-02\n",
            "5.8500e-01 9.3545e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7760e-02 9.9649e-02 -4.8379e-02 9.8268e-02 1.2149e-01 5.2622e-04 5.3387e-04 5.3545e-04 0.0000e+00 0.0000e+00 9.3545e-02\n",
            "5.8600e-01 9.3546e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7760e-02 9.9650e-02 -4.8379e-02 9.8268e-02 1.2149e-01 5.2624e-04 5.3387e-04 5.3546e-04 0.0000e+00 0.0000e+00 9.3546e-02\n",
            "5.8700e-01 9.3548e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7761e-02 9.9650e-02 -4.8379e-02 9.8269e-02 1.2149e-01 5.2625e-04 5.3387e-04 5.3548e-04 0.0000e+00 0.0000e+00 9.3548e-02\n",
            "5.8800e-01 9.3550e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7761e-02 9.9651e-02 -4.8379e-02 9.8269e-02 1.2149e-01 5.2627e-04 5.3387e-04 5.3550e-04 0.0000e+00 0.0000e+00 9.3550e-02\n",
            "5.8900e-01 9.3551e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7762e-02 9.9652e-02 -4.8380e-02 9.8270e-02 1.2149e-01 5.2629e-04 5.3386e-04 5.3551e-04 0.0000e+00 0.0000e+00 9.3551e-02\n",
            "5.9000e-01 9.3553e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7762e-02 9.9652e-02 -4.8380e-02 9.8270e-02 1.2149e-01 5.2630e-04 5.3386e-04 5.3553e-04 0.0000e+00 0.0000e+00 9.3553e-02\n",
            "5.9100e-01 9.3555e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7763e-02 9.9653e-02 -4.8380e-02 9.8271e-02 1.2149e-01 5.2632e-04 5.3386e-04 5.3555e-04 0.0000e+00 0.0000e+00 9.3555e-02\n",
            "5.9200e-01 9.3556e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7763e-02 9.9654e-02 -4.8380e-02 9.8271e-02 1.2149e-01 5.2634e-04 5.3386e-04 5.3556e-04 0.0000e+00 0.0000e+00 9.3556e-02\n",
            "5.9300e-01 9.3558e-04       (2,) 9.1199e-05 9.1199e-05 9.9982e-01 7.7764e-02 9.9654e-02 -4.8381e-02 9.8272e-02 1.2149e-01 5.2635e-04 5.3386e-04 5.3558e-04 0.0000e+00 0.0000e+00 9.3558e-02\n",
            "5.9400e-01 9.3560e-04       (2,) 8.9103e-05 8.9103e-05 9.9982e-01 7.7764e-02 9.9655e-02 -4.8381e-02 9.8272e-02 1.2149e-01 5.2637e-04 5.3385e-04 5.3560e-04 0.0000e+00 0.0000e+00 9.3560e-02\n",
            "5.9500e-01 9.3561e-04       (2,) 8.9103e-05 8.9103e-05 9.9982e-01 7.7765e-02 9.9655e-02 -4.8381e-02 9.8273e-02 1.2149e-01 5.2639e-04 5.3385e-04 5.3561e-04 0.0000e+00 0.0000e+00 9.3561e-02\n",
            "5.9600e-01 9.3563e-04       (2,) 8.9103e-05 8.9103e-05 9.9982e-01 7.7765e-02 9.9656e-02 -4.8381e-02 9.8274e-02 1.2149e-01 5.2640e-04 5.3385e-04 5.3563e-04 0.0000e+00 0.0000e+00 9.3563e-02\n",
            "5.9700e-01 9.3565e-04       (2,) 8.9103e-05 8.9103e-05 9.9982e-01 7.7766e-02 9.9657e-02 -4.8382e-02 9.8274e-02 1.2149e-01 5.2642e-04 5.3385e-04 5.3565e-04 0.0000e+00 0.0000e+00 9.3565e-02\n",
            "5.9800e-01 9.3566e-04       (2,) 8.9103e-05 8.9103e-05 9.9982e-01 7.7766e-02 9.9657e-02 -4.8382e-02 9.8275e-02 1.2149e-01 5.2644e-04 5.3385e-04 5.3566e-04 0.0000e+00 0.0000e+00 9.3566e-02\n",
            "5.9900e-01 9.3568e-04       (2,) 8.9103e-05 8.9103e-05 9.9982e-01 7.7767e-02 9.9658e-02 -4.8382e-02 9.8275e-02 1.2149e-01 5.2645e-04 5.3385e-04 5.3568e-04 0.0000e+00 0.0000e+00 9.3568e-02\n",
            "6.0000e-01 9.3570e-04       (2,) 8.9103e-05 8.9103e-05 9.9982e-01 7.7768e-02 9.9658e-02 -4.8382e-02 9.8276e-02 1.2149e-01 5.2647e-04 5.3384e-04 5.3570e-04 0.0000e+00 0.0000e+00 9.3570e-02\n",
            "\n",
            "Full detailed results (201 points) saved to: /content/alpha_internal_sol_search_0.400_0.600_activeset_mu002_mild.csv\n",
            "\n",
            "--- Quick KKT Check (Stationarity w.r.t. w) ---\n",
            "     alpha active_set_report active_set_kkt  max_violation  grad_consistency_violation check_result\n",
            "4.0000e-01              (2,)      (0, 1, 2)     1.1061e-05                  1.1061e-05         WARN\n",
            "4.0100e-01              (2,)      (0, 1, 2)     1.1043e-05                  1.1043e-05         WARN\n",
            "4.0200e-01              (2,)      (0, 1, 2)     1.1025e-05                  1.1025e-05         WARN\n",
            "4.0300e-01              (2,)      (0, 1, 2)     1.1007e-05                  1.1007e-05         WARN\n",
            "4.0400e-01              (2,)      (0, 1, 2)     1.0989e-05                  1.0989e-05         WARN\n",
            "4.0500e-01              (2,)      (0, 1, 2)     1.0971e-05                  1.0971e-05         WARN\n",
            "4.0600e-01              (2,)      (0, 1, 2)     1.0953e-05                  1.0953e-05         WARN\n",
            "4.0700e-01              (2,)      (0, 1, 2)     1.0934e-05                  1.0934e-05         WARN\n",
            "4.0800e-01              (2,)      (0, 1, 2)     1.0916e-05                  1.0916e-05         WARN\n",
            "4.0900e-01              (2,)      (0, 1, 2)     1.0898e-05                  1.0898e-05         WARN\n",
            "4.1000e-01              (2,)      (0, 1, 2)     1.0880e-05                  1.0880e-05         WARN\n",
            "4.1100e-01              (2,)      (0, 1, 2)     1.0862e-05                  1.0862e-05         WARN\n",
            "4.1200e-01              (2,)      (0, 1, 2)     1.0844e-05                  1.0844e-05         WARN\n",
            "4.1300e-01              (2,)      (0, 1, 2)     1.0826e-05                  1.0826e-05         WARN\n",
            "4.1400e-01              (2,)      (0, 1, 2)     1.0808e-05                  1.0808e-05         WARN\n",
            "4.1500e-01              (2,)      (0, 1, 2)     1.0789e-05                  1.0789e-05         WARN\n",
            "4.1600e-01              (2,)      (0, 1, 2)     1.0771e-05                  1.0771e-05         WARN\n",
            "4.1700e-01              (2,)      (0, 1, 2)     1.0753e-05                  1.0753e-05         WARN\n",
            "4.1800e-01              (2,)      (0, 1, 2)     1.0735e-05                  1.0735e-05         WARN\n",
            "4.1900e-01              (2,)      (0, 1, 2)     1.0717e-05                  1.0717e-05         WARN\n",
            "4.2000e-01              (2,)      (0, 1, 2)     1.0699e-05                  1.0699e-05         WARN\n",
            "4.2100e-01              (2,)      (0, 1, 2)     1.0681e-05                  1.0681e-05         WARN\n",
            "4.2200e-01              (2,)      (0, 1, 2)     1.0662e-05                  1.0662e-05         WARN\n",
            "4.2300e-01              (2,)      (0, 1, 2)     1.0644e-05                  1.0644e-05         WARN\n",
            "4.2400e-01              (2,)      (0, 1, 2)     1.0626e-05                  1.0626e-05         WARN\n",
            "4.2500e-01              (2,)      (0, 1, 2)     1.0608e-05                  1.0608e-05         WARN\n",
            "4.2600e-01              (2,)      (0, 1, 2)     1.0590e-05                  1.0590e-05         WARN\n",
            "4.2700e-01              (2,)      (0, 1, 2)     1.0572e-05                  1.0572e-05         WARN\n",
            "4.2800e-01              (2,)      (0, 1, 2)     1.0554e-05                  1.0554e-05         WARN\n",
            "4.2900e-01              (2,)      (0, 1, 2)     1.0536e-05                  1.0536e-05         WARN\n",
            "4.3000e-01              (2,)      (0, 1, 2)     1.0517e-05                  1.0517e-05         WARN\n",
            "4.3100e-01              (2,)      (0, 1, 2)     1.0499e-05                  1.0499e-05         WARN\n",
            "4.3200e-01              (2,)      (0, 1, 2)     1.0481e-05                  1.0481e-05         WARN\n",
            "4.3300e-01              (2,)      (0, 1, 2)     1.0463e-05                  1.0463e-05         WARN\n",
            "4.3400e-01              (2,)      (0, 1, 2)     1.0445e-05                  1.0445e-05         WARN\n",
            "4.3500e-01              (2,)      (0, 1, 2)     1.0427e-05                  1.0427e-05         WARN\n",
            "4.3600e-01              (2,)      (0, 1, 2)     1.0408e-05                  1.0408e-05         WARN\n",
            "4.3700e-01              (2,)      (0, 1, 2)     1.0390e-05                  1.0390e-05         WARN\n",
            "4.3800e-01              (2,)      (0, 1, 2)     1.0372e-05                  1.0372e-05         WARN\n",
            "4.3900e-01              (2,)      (0, 1, 2)     1.0354e-05                  1.0354e-05         WARN\n",
            "4.4000e-01              (2,)      (0, 1, 2)     1.0336e-05                  1.0336e-05         WARN\n",
            "4.4100e-01              (2,)      (0, 1, 2)     1.0318e-05                  1.0318e-05         WARN\n",
            "4.4200e-01              (2,)      (0, 1, 2)     1.0300e-05                  1.0300e-05         WARN\n",
            "4.4300e-01              (2,)      (0, 1, 2)     1.0281e-05                  1.0281e-05         WARN\n",
            "4.4400e-01              (2,)      (0, 1, 2)     1.0263e-05                  1.0263e-05         WARN\n",
            "4.4500e-01              (2,)      (0, 1, 2)     1.0245e-05                  1.0245e-05         WARN\n",
            "4.4600e-01              (2,)      (0, 1, 2)     1.0227e-05                  1.0227e-05         WARN\n",
            "4.4700e-01              (2,)      (0, 1, 2)     1.0209e-05                  1.0209e-05         WARN\n",
            "4.4800e-01              (2,)      (0, 1, 2)     1.0191e-05                  1.0191e-05         WARN\n",
            "4.4900e-01              (2,)      (0, 1, 2)     1.0172e-05                  1.0172e-05         WARN\n",
            "4.5000e-01              (2,)      (0, 1, 2)     1.0154e-05                  1.0154e-05         WARN\n",
            "4.5100e-01              (2,)      (0, 1, 2)     1.0136e-05                  1.0136e-05         WARN\n",
            "4.5200e-01              (2,)      (0, 1, 2)     1.0118e-05                  1.0118e-05         WARN\n",
            "4.5300e-01              (2,)      (0, 1, 2)     1.0100e-05                  1.0100e-05         WARN\n",
            "4.5400e-01              (2,)      (0, 1, 2)     1.0082e-05                  1.0082e-05         WARN\n",
            "4.5500e-01              (2,)      (0, 1, 2)     1.0063e-05                  1.0063e-05         WARN\n",
            "4.5600e-01              (2,)      (0, 1, 2)     1.0045e-05                  1.0045e-05         WARN\n",
            "4.5700e-01              (2,)      (0, 1, 2)     1.0027e-05                  1.0027e-05         WARN\n",
            "4.5800e-01              (2,)      (0, 1, 2)     1.0009e-05                  1.0009e-05         WARN\n",
            "4.5900e-01              (2,)      (0, 1, 2)     9.9907e-06                  9.9907e-06         WARN\n",
            "4.6000e-01              (2,)      (0, 1, 2)     9.9725e-06                  9.9725e-06         WARN\n",
            "4.6100e-01              (2,)      (0, 1, 2)     9.9544e-06                  9.9544e-06         WARN\n",
            "4.6200e-01              (2,)      (0, 1, 2)     9.9362e-06                  9.9362e-06         WARN\n",
            "4.6300e-01              (2,)      (0, 1, 2)     9.9180e-06                  9.9180e-06         WARN\n",
            "4.6400e-01              (2,)      (0, 1, 2)     9.8998e-06                  9.8998e-06         WARN\n",
            "4.6500e-01              (2,)      (0, 1, 2)     9.8816e-06                  9.8816e-06         WARN\n",
            "4.6600e-01              (2,)      (0, 1, 2)     9.8635e-06                  9.8635e-06         WARN\n",
            "4.6700e-01              (2,)      (0, 1, 2)     9.8453e-06                  9.8453e-06         WARN\n",
            "4.6800e-01              (2,)      (0, 1, 2)     9.8271e-06                  9.8271e-06         WARN\n",
            "4.6900e-01              (2,)      (0, 1, 2)     9.8089e-06                  9.8089e-06         WARN\n",
            "4.7000e-01              (2,)      (0, 1, 2)     9.7907e-06                  9.7907e-06         WARN\n",
            "4.7100e-01              (2,)      (0, 1, 2)     9.7725e-06                  9.7725e-06         WARN\n",
            "4.7200e-01              (2,)      (0, 1, 2)     9.7543e-06                  9.7543e-06         WARN\n",
            "4.7300e-01              (2,)      (0, 1, 2)     9.7362e-06                  9.7362e-06         WARN\n",
            "4.7400e-01              (2,)      (0, 1, 2)     9.7180e-06                  9.7180e-06         WARN\n",
            "4.7500e-01              (2,)      (0, 1, 2)     9.6998e-06                  9.6998e-06         WARN\n",
            "4.7600e-01              (2,)      (0, 1, 2)     9.6816e-06                  9.6816e-06         WARN\n",
            "4.7700e-01              (2,)      (0, 1, 2)     9.6634e-06                  9.6634e-06         WARN\n",
            "4.7800e-01              (2,)      (0, 1, 2)     9.6452e-06                  9.6452e-06         WARN\n",
            "4.7900e-01              (2,)      (0, 1, 2)     9.6270e-06                  9.6270e-06         WARN\n",
            "4.8000e-01              (2,)      (0, 1, 2)     9.6088e-06                  9.6088e-06         WARN\n",
            "4.8100e-01              (2,)      (0, 1, 2)     9.5906e-06                  9.5906e-06         WARN\n",
            "4.8200e-01              (2,)      (0, 1, 2)     9.5724e-06                  9.5724e-06         WARN\n",
            "4.8300e-01              (2,)      (0, 1, 2)     9.5542e-06                  9.5542e-06         WARN\n",
            "4.8400e-01              (2,)      (0, 1, 2)     9.5360e-06                  9.5360e-06         WARN\n",
            "4.8500e-01              (2,)      (0, 1, 2)     9.5178e-06                  9.5178e-06         WARN\n",
            "4.8600e-01              (2,)      (0, 1, 2)     9.4996e-06                  9.4996e-06         WARN\n",
            "4.8700e-01              (2,)      (0, 1, 2)     9.4814e-06                  9.4814e-06         WARN\n",
            "4.8800e-01              (2,)      (0, 1, 2)     9.4632e-06                  9.4632e-06         WARN\n",
            "4.8900e-01              (2,)      (0, 1, 2)     9.4449e-06                  9.4449e-06         WARN\n",
            "4.9000e-01              (2,)      (0, 1, 2)     9.4267e-06                  9.4267e-06         WARN\n",
            "4.9100e-01              (2,)      (0, 1, 2)     9.4085e-06                  9.4085e-06         WARN\n",
            "4.9200e-01              (2,)      (0, 1, 2)     9.3903e-06                  9.3903e-06         WARN\n",
            "4.9300e-01              (2,)      (0, 1, 2)     9.3721e-06                  9.3721e-06         WARN\n",
            "4.9400e-01              (2,)      (0, 1, 2)     9.3539e-06                  9.3539e-06         WARN\n",
            "4.9500e-01              (2,)      (0, 1, 2)     9.3357e-06                  9.3357e-06         WARN\n",
            "4.9600e-01              (2,)      (0, 1, 2)     9.3175e-06                  9.3175e-06         WARN\n",
            "4.9700e-01              (2,)      (0, 1, 2)     9.2992e-06                  9.2992e-06         WARN\n",
            "4.9800e-01              (2,)      (0, 1, 2)     9.2810e-06                  9.2810e-06         WARN\n",
            "4.9900e-01              (2,)      (0, 1, 2)     9.2628e-06                  9.2628e-06         WARN\n",
            "5.0000e-01              (2,)      (0, 1, 2)     9.2286e-06                  9.2286e-06         WARN\n",
            "5.0100e-01              (2,)      (0, 1, 2)     9.2076e-06                  9.2076e-06         WARN\n",
            "5.0200e-01              (2,)      (0, 1, 2)     9.2078e-06                  9.2078e-06         WARN\n",
            "5.0300e-01              (2,)      (0, 1, 2)     9.2080e-06                  9.2080e-06         WARN\n",
            "5.0400e-01              (2,)      (0, 1, 2)     9.2083e-06                  9.2083e-06         WARN\n",
            "5.0500e-01              (2,)      (0, 1, 2)     9.2085e-06                  9.2085e-06         WARN\n",
            "5.0600e-01              (2,)      (0, 1, 2)     9.2087e-06                  9.2087e-06         WARN\n",
            "5.0700e-01              (2,)      (0, 1, 2)     9.2089e-06                  9.2089e-06         WARN\n",
            "5.0800e-01              (2,)      (0, 1, 2)     9.2091e-06                  9.2091e-06         WARN\n",
            "5.0900e-01              (2,)      (0, 1, 2)     9.2094e-06                  9.2094e-06         WARN\n",
            "5.1000e-01              (2,)      (0, 1, 2)     9.2096e-06                  9.2096e-06         WARN\n",
            "5.1100e-01              (2,)      (0, 1, 2)     9.2098e-06                  9.2098e-06         WARN\n",
            "5.1200e-01              (2,)      (0, 1, 2)     9.2100e-06                  9.2100e-06         WARN\n",
            "5.1300e-01              (2,)      (0, 1, 2)     9.2102e-06                  9.2102e-06         WARN\n",
            "5.1400e-01              (2,)      (0, 1, 2)     9.2105e-06                  9.2105e-06         WARN\n",
            "5.1500e-01              (2,)      (0, 1, 2)     9.2107e-06                  9.2107e-06         WARN\n",
            "5.1600e-01              (2,)      (0, 1, 2)     9.2109e-06                  9.2109e-06         WARN\n",
            "5.1700e-01              (2,)      (0, 1, 2)     9.2111e-06                  9.2111e-06         WARN\n",
            "5.1800e-01              (2,)      (0, 1, 2)     9.2113e-06                  9.2113e-06         WARN\n",
            "5.1900e-01              (2,)      (0, 1, 2)     9.2115e-06                  9.2115e-06         WARN\n",
            "5.2000e-01              (2,)      (0, 1, 2)     9.2118e-06                  9.2118e-06         WARN\n",
            "5.2100e-01              (2,)      (0, 1, 2)     9.2120e-06                  9.2120e-06         WARN\n",
            "5.2200e-01              (2,)      (0, 1, 2)     9.2122e-06                  9.2122e-06         WARN\n",
            "5.2300e-01              (2,)      (0, 1, 2)     9.2124e-06                  9.2124e-06         WARN\n",
            "5.2400e-01              (2,)      (0, 1, 2)     9.2126e-06                  9.2126e-06         WARN\n",
            "5.2500e-01              (2,)      (0, 1, 2)     9.2129e-06                  9.2129e-06         WARN\n",
            "5.2600e-01              (2,)      (0, 1, 2)     9.2131e-06                  9.2131e-06         WARN\n",
            "5.2700e-01              (2,)      (0, 1, 2)     9.2133e-06                  9.2133e-06         WARN\n",
            "5.2800e-01              (2,)      (0, 1, 2)     9.2135e-06                  9.2135e-06         WARN\n",
            "5.2900e-01              (2,)      (0, 1, 2)     9.2137e-06                  9.2137e-06         WARN\n",
            "5.3000e-01              (2,)      (0, 1, 2)     9.2139e-06                  9.2139e-06         WARN\n",
            "5.3100e-01              (2,)      (0, 1, 2)     9.2142e-06                  9.2142e-06         WARN\n",
            "5.3200e-01              (2,)      (0, 1, 2)     9.2144e-06                  9.2144e-06         WARN\n",
            "5.3300e-01              (2,)      (0, 1, 2)     9.2146e-06                  9.2146e-06         WARN\n",
            "5.3400e-01              (2,)      (0, 1, 2)     9.2148e-06                  9.2148e-06         WARN\n",
            "5.3500e-01              (2,)      (0, 1, 2)     9.2150e-06                  9.2150e-06         WARN\n",
            "5.3600e-01              (2,)      (0, 1, 2)     9.2153e-06                  9.2153e-06         WARN\n",
            "5.3700e-01              (2,)      (0, 1, 2)     9.2155e-06                  9.2155e-06         WARN\n",
            "5.3800e-01              (2,)      (0, 1, 2)     9.2157e-06                  9.2157e-06         WARN\n",
            "5.3900e-01              (2,)      (0, 1, 2)     9.2159e-06                  9.2159e-06         WARN\n",
            "5.4000e-01              (2,)      (0, 1, 2)     9.2161e-06                  9.2161e-06         WARN\n",
            "5.4100e-01              (2,)      (0, 1, 2)     9.2163e-06                  9.2163e-06         WARN\n",
            "5.4200e-01              (2,)      (0, 1, 2)     9.2166e-06                  9.2166e-06         WARN\n",
            "5.4300e-01              (2,)      (0, 1, 2)     9.2168e-06                  9.2168e-06         WARN\n",
            "5.4400e-01              (2,)      (0, 1, 2)     9.2170e-06                  9.2170e-06         WARN\n",
            "5.4500e-01              (2,)      (0, 1, 2)     9.2172e-06                  9.2172e-06         WARN\n",
            "5.4600e-01              (2,)      (0, 1, 2)     9.2174e-06                  9.2174e-06         WARN\n",
            "5.4700e-01              (2,)      (0, 1, 2)     9.2176e-06                  9.2176e-06         WARN\n",
            "5.4800e-01              (2,)      (0, 1, 2)     9.2179e-06                  9.2179e-06         WARN\n",
            "5.4900e-01              (2,)      (0, 1, 2)     9.2181e-06                  9.2181e-06         WARN\n",
            "5.5000e-01              (2,)      (0, 1, 2)     9.2183e-06                  9.2183e-06         WARN\n",
            "5.5100e-01              (2,)      (0, 1, 2)     9.2185e-06                  9.2185e-06         WARN\n",
            "5.5200e-01              (2,)      (0, 1, 2)     9.2187e-06                  9.2187e-06         WARN\n",
            "5.5300e-01              (2,)      (0, 1, 2)     9.2190e-06                  9.2190e-06         WARN\n",
            "5.5400e-01              (2,)      (0, 1, 2)     9.2192e-06                  9.2192e-06         WARN\n",
            "5.5500e-01              (2,)      (0, 1, 2)     9.2194e-06                  9.2194e-06         WARN\n",
            "5.5600e-01              (2,)      (0, 1, 2)     9.2196e-06                  9.2196e-06         WARN\n",
            "5.5700e-01              (2,)      (0, 1, 2)     9.2198e-06                  9.2198e-06         WARN\n",
            "5.5800e-01              (2,)      (0, 1, 2)     9.2200e-06                  9.2200e-06         WARN\n",
            "5.5900e-01              (2,)      (0, 1, 2)     9.2203e-06                  9.2203e-06         WARN\n",
            "5.6000e-01              (2,)      (0, 1, 2)     9.2205e-06                  9.2205e-06         WARN\n",
            "5.6100e-01              (2,)      (0, 1, 2)     9.2207e-06                  9.2207e-06         WARN\n",
            "5.6200e-01              (2,)      (0, 1, 2)     9.2209e-06                  9.2209e-06         WARN\n",
            "5.6300e-01              (2,)      (0, 1, 2)     9.2211e-06                  9.2211e-06         WARN\n",
            "5.6400e-01              (2,)      (0, 1, 2)     9.2213e-06                  9.2213e-06         WARN\n",
            "5.6500e-01              (2,)      (0, 1, 2)     9.2216e-06                  9.2216e-06         WARN\n",
            "5.6600e-01              (2,)      (0, 1, 2)     9.2218e-06                  9.2218e-06         WARN\n",
            "5.6700e-01              (2,)      (0, 1, 2)     9.2220e-06                  9.2220e-06         WARN\n",
            "5.6800e-01              (2,)      (0, 1, 2)     9.2222e-06                  9.2222e-06         WARN\n",
            "5.6900e-01              (2,)      (0, 1, 2)     9.2224e-06                  9.2224e-06         WARN\n",
            "5.7000e-01              (2,)      (0, 1, 2)     9.2226e-06                  9.2226e-06         WARN\n",
            "5.7100e-01              (2,)      (0, 1, 2)     9.2229e-06                  9.2229e-06         WARN\n",
            "5.7200e-01              (2,)      (0, 1, 2)     9.2231e-06                  9.2231e-06         WARN\n",
            "5.7300e-01              (2,)      (0, 1, 2)     9.2233e-06                  9.2233e-06         WARN\n",
            "5.7400e-01              (2,)      (0, 1, 2)     9.2235e-06                  9.2235e-06         WARN\n",
            "5.7500e-01              (2,)      (0, 1, 2)     9.2237e-06                  9.2237e-06         WARN\n",
            "5.7600e-01              (2,)      (0, 1, 2)     9.2239e-06                  9.2239e-06         WARN\n",
            "5.7700e-01              (2,)      (0, 1, 2)     9.2242e-06                  9.2242e-06         WARN\n",
            "5.7800e-01              (2,)      (0, 1, 2)     9.2244e-06                  9.2244e-06         WARN\n",
            "5.7900e-01              (2,)      (0, 1, 2)     9.2246e-06                  9.2246e-06         WARN\n",
            "5.8000e-01              (2,)      (0, 1, 2)     9.2248e-06                  9.2248e-06         WARN\n",
            "5.8100e-01              (2,)      (0, 1, 2)     9.2250e-06                  9.2250e-06         WARN\n",
            "5.8200e-01              (2,)      (0, 1, 2)     9.2252e-06                  9.2252e-06         WARN\n",
            "5.8300e-01              (2,)      (0, 1, 2)     9.2255e-06                  9.2255e-06         WARN\n",
            "5.8400e-01              (2,)      (0, 1, 2)     9.2257e-06                  9.2257e-06         WARN\n",
            "5.8500e-01              (2,)      (0, 1, 2)     9.2259e-06                  9.2259e-06         WARN\n",
            "5.8600e-01              (2,)      (0, 1, 2)     9.2261e-06                  9.2261e-06         WARN\n",
            "5.8700e-01              (2,)      (0, 1, 2)     9.2263e-06                  9.2263e-06         WARN\n",
            "5.8800e-01              (2,)      (0, 1, 2)     9.2265e-06                  9.2265e-06         WARN\n",
            "5.8900e-01              (2,)      (0, 1, 2)     9.2268e-06                  9.2268e-06         WARN\n",
            "5.9000e-01              (2,)      (0, 1, 2)     9.2270e-06                  9.2270e-06         WARN\n",
            "5.9100e-01              (2,)      (0, 1, 2)     9.2272e-06                  9.2272e-06         WARN\n",
            "5.9200e-01              (2,)      (0, 1, 2)     9.2274e-06                  9.2274e-06         WARN\n",
            "5.9300e-01              (2,)      (0, 1, 2)     9.2276e-06                  9.2276e-06         WARN\n",
            "5.9400e-01              (2,)      (0, 1, 2)     9.2278e-06                  9.2278e-06         WARN\n",
            "5.9500e-01              (2,)      (0, 1, 2)     9.2281e-06                  9.2281e-06         WARN\n",
            "5.9600e-01              (2,)      (0, 1, 2)     9.2283e-06                  9.2283e-06         WARN\n",
            "5.9700e-01              (2,)      (0, 1, 2)     9.2285e-06                  9.2285e-06         WARN\n",
            "5.9800e-01              (2,)      (0, 1, 2)     9.2287e-06                  9.2287e-06         WARN\n",
            "5.9900e-01              (2,)      (0, 1, 2)     9.2289e-06                  9.2289e-06         WARN\n",
            "6.0000e-01              (2,)      (0, 1, 2)     9.2291e-06                  9.2291e-06         WARN\n",
            "Notes on KKT Check:\n",
            " - active_set_report: Active set reported by inner solver\n",
            " - active_set_kkt: Active set inferred from w* > tol\n",
            " - max_violation: Max KKT violation (dual infeasibility or comp. slackness based on eta estimate)\n",
            " - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\n",
            "\n",
            "Total execution time: 19.86 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "4_14_find_internal_solution_symmetric.py\n",
        "\n",
        "Attempts to find an internal solution w* by using symmetric parameters\n",
        "around alpha=0.75 (beta=0.5) and mu_tilde = 0.01.\n",
        "Uses the Active Set method for the inner QP solve.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from scipy.linalg import solve, LinAlgError, eigh, inv, qr\n",
        "from scipy.optimize import linprog, OptimizeWarning\n",
        "import traceback\n",
        "import time\n",
        "import itertools\n",
        "import os\n",
        "import math\n",
        "from collections import Counter # \n",
        "\n",
        "# pandas \n",
        "try:\n",
        "    import pandas as pd\n",
        "    PANDAS_AVAILABLE = True\n",
        "    pd.set_option('display.width', 160)\n",
        "    pd.set_option('display.float_format', '{:.6e}'.format) # \n",
        "    pd.set_option('display.max_rows', 200) # \n",
        "except ImportError:\n",
        "    PANDAS_AVAILABLE = False\n",
        "    print(\"Warning: pandas library not found. Output formatting will be basic. CSV export disabled.\")\n",
        "\n",
        "# ===  ===\n",
        "DEFAULT_TOLERANCE = 1e-9\n",
        "\n",
        "# --- OptimizationResult  ---\n",
        "class OptimizationResult:\n",
        "    def __init__(self, success, message, w_opt=None, pi_opt=None, lambda_opt=None,\n",
        "                 H_opt=None, grad_H_opt=None, iterations=None, fw_gap=None,\n",
        "                 active_set_opt=None):\n",
        "        self.success = success; self.message = message; self.w_opt = w_opt; self.pi_opt = pi_opt\n",
        "        self.lambda_opt = lambda_opt; self.H_opt = H_opt; self.grad_H_opt = grad_H_opt\n",
        "        self.iterations = iterations; self.fw_gap = fw_gap\n",
        "        self.active_set_opt = active_set_opt\n",
        "\n",
        "# --- find_feasible_initial_pi  ---\n",
        "def find_feasible_initial_pi(R, mu_tilde, K, tolerance=1e-8):\n",
        "    M = R.shape[1]; c = np.zeros(K + 1); c[K] = 1.0\n",
        "    A_ub = np.hstack((-R.T, -np.ones((M, 1)))); b_ub = -mu_tilde * np.ones(M)\n",
        "    bounds = [(None, None)] * K + [(0, None)]; opts = {'tol': tolerance, 'disp': False, 'presolve': True}\n",
        "    result = None; methods_to_try = ['highs', 'highs-ipm', 'highs-ds', 'simplex']\n",
        "    for method in methods_to_try:\n",
        "        try:\n",
        "            with warnings.catch_warnings(): warnings.filterwarnings(\"ignore\"); result = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=method, options=opts)\n",
        "            if result.success: break\n",
        "        except ValueError: continue\n",
        "        except Exception as e: return None, False, f\"Phase 1 LP failed: {e}\"\n",
        "    if result is None or not result.success: msg = result.message if result else \"No solver\"; status = result.status if result else -1; return None, False, f\"Phase 1 LP solver failed: {msg} (status={status})\"\n",
        "    s = result.x[K]; pi = result.x[:K]\n",
        "    if np.isnan(pi).any(): return None, False, \"Phase 1 LP resulted in NaN values for pi.\"\n",
        "    if s <= tolerance * 1000:\n",
        "        G = -R.T; h = -mu_tilde * np.ones(M); violation = np.max(G @ pi - h)\n",
        "        if violation <= tolerance * 10000: return pi, True, f\"Phase 1 OK (s*={s:.1e}, vio={violation:.1e})\"\n",
        "        else: return pi, True, f\"Phase 1 OK (s*={s:.1e}), WARN: violation {violation:.1e}\"\n",
        "    else: return None, False, f\"Phase 1 Infeasible (s* = {s:.1e})\"\n",
        "\n",
        "# --- solve_kkt_system  ---\n",
        "def solve_kkt_system(Q, G_W, g, tolerance=DEFAULT_TOLERANCE):\n",
        "    K = Q.shape[0]; n_act = G_W.shape[0] if G_W is not None and G_W.ndim == 2 and G_W.shape[0] > 0 else 0\n",
        "    if n_act == 0:\n",
        "        try: p = solve(Q, -g, assume_a='sym'); l = np.array([])\n",
        "        except LinAlgError: return None, None, False\n",
        "        res_norm = np.linalg.norm(Q @ p + g); g_norm = np.linalg.norm(g); solved_ok = res_norm <= tolerance * 1e3 * (1 + g_norm)\n",
        "        return p, l, solved_ok\n",
        "    else:\n",
        "        kkt_mat = None; rhs = None\n",
        "        try: kkt_mat = np.block([[Q, G_W.T], [G_W, np.zeros((n_act, n_act))]]) ; rhs = np.concatenate([-g, np.zeros(n_act)])\n",
        "        except ValueError as e: return None, None, False\n",
        "        try: sol = solve(kkt_mat, rhs, assume_a='sym'); p = sol[:K]; l = sol[K:]\n",
        "        except LinAlgError: return None, None, False\n",
        "        except ValueError as e: return None, None, False\n",
        "        res_norm = np.linalg.norm(kkt_mat @ sol - rhs); rhs_norm = np.linalg.norm(rhs); solved_ok = res_norm <= tolerance * 1e3 * (1 + rhs_norm)\n",
        "        return p, l, solved_ok\n",
        "\n",
        "# --- solve_inner_qp_active_set  ---\n",
        "def solve_inner_qp_active_set(Vw, R, mu_tilde, initial_pi, max_iter=350, tolerance=DEFAULT_TOLERANCE, regularization_epsilon=1e-10):\n",
        "    K = Vw.shape[0]; M = R.shape[1]; Q_reg = 2 * Vw + 2 * regularization_epsilon * np.eye(K); G = -R.T; h = -mu_tilde * np.ones(M)\n",
        "    if initial_pi is None: return None, None, None, None, None, False, \"No initial pi\"\n",
        "    pi_k = np.copy(initial_pi); lam_opt = np.zeros(M); W = set()\n",
        "    active_tol = tolerance * 10\n",
        "    initial_violations = G @ pi_k - h; W = set(j for j, viol in enumerate(initial_violations) if viol > -active_tol)\n",
        "    active_indices_opt = None; kkt_matrix_opt = None\n",
        "    if np.any(initial_violations > active_tol * 10): warnings.warn(f\"Initial pi infeasible (max viol: {np.max(initial_violations):.2e}).\")\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        g_k = Q_reg @ pi_k; act = sorted(list(W)); n_act = len(act); G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "        p_k, lam_Wk, solved = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "        if not solved or p_k is None: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: KKT solve failed. ActiveSet={act}\"\n",
        "        if np.linalg.norm(p_k) <= tolerance * 10 * (1 + np.linalg.norm(pi_k)):\n",
        "            is_optimal_point = True; blocking_constraint_idx = -1; min_negative_lambda = float('inf')\n",
        "            dual_feas_tol = -tolerance * 10\n",
        "            if W:\n",
        "                if lam_Wk is None or len(lam_Wk) != n_act: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: lam_Wk inconsistent? ActiveSet={act}\"\n",
        "                lambda_map = dict(zip(act, lam_Wk))\n",
        "                for constraint_idx, lagrange_multiplier in lambda_map.items():\n",
        "                    if lagrange_multiplier < dual_feas_tol: is_optimal_point = False;\n",
        "                    if lagrange_multiplier < min_negative_lambda: min_negative_lambda = lagrange_multiplier; blocking_constraint_idx = constraint_idx\n",
        "            if is_optimal_point:\n",
        "                lam_opt.fill(0.0);\n",
        "                if W and len(lam_Wk) == n_act: lam_opt[act] = np.maximum(lam_Wk, 0)\n",
        "                final_infeas = np.max(G @ pi_k - h); msg = f\"Optimal found at iter {i+1}.\"\n",
        "                if final_infeas > active_tol: msg += f\" (WARN: violation {final_infeas:.1e})\"\n",
        "                active_indices_opt = act\n",
        "                return pi_k, lam_opt, active_indices_opt, None, Q_reg / 2.0, True, msg\n",
        "            else:\n",
        "                if blocking_constraint_idx in W: W.remove(blocking_constraint_idx); continue\n",
        "                else: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Neg lambda idx {blocking_constraint_idx}, not in W={act}\"\n",
        "        else:\n",
        "            alpha_k = 1.0; blocking_constraint_idx = -1; min_step_length = float('inf')\n",
        "            step_tol = tolerance * 10\n",
        "            for j in range(M):\n",
        "                if j not in W:\n",
        "                    constraint_gradient_dot_p = G[j, :] @ p_k\n",
        "                    if constraint_gradient_dot_p > step_tol:\n",
        "                        distance_to_boundary = h[j] - (G[j, :] @ pi_k)\n",
        "                        if abs(constraint_gradient_dot_p) > 1e-15:\n",
        "                            alpha_j = distance_to_boundary / constraint_gradient_dot_p\n",
        "                            step_j = max(0.0, alpha_j)\n",
        "                            if step_j < min_step_length:\n",
        "                                min_step_length = step_j; blocking_constraint_idx = j\n",
        "            alpha_k = min(1.0, min_step_length); pi_k += alpha_k * p_k\n",
        "            if alpha_k < 1.0 - step_tol and blocking_constraint_idx != -1:\n",
        "                if blocking_constraint_idx not in W: W.add(blocking_constraint_idx)\n",
        "            continue\n",
        "    msg = f\"Max iter ({max_iter}) reached.\"; final_infeas = np.max(G @ pi_k - h)\n",
        "    if final_infeas > active_tol * 10: return None, None, None, None, None, False, f\"{msg} Final infeasible. ActiveSet={sorted(list(W))}\"\n",
        "    act = sorted(list(W)); n_act = len(act); G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "    is_likely_optimal = False; active_constraints_opt = act\n",
        "    g_k = Q_reg @ pi_k; p_f, lam_f, solved_f = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "    final_lambda_estimate = np.zeros(M)\n",
        "    if solved_f and p_f is not None and np.linalg.norm(p_f) <= tolerance * 100 * (1 + np.linalg.norm(pi_k)):\n",
        "        if n_act > 0 and lam_f is not None and len(lam_f) == n_act:\n",
        "             try: final_lambda_estimate[act] = lam_f\n",
        "             except IndexError: pass\n",
        "        active_lambdas = final_lambda_estimate[act] if n_act > 0 else np.array([])\n",
        "        if n_act == 0 or np.all(active_lambdas >= -tolerance * 100): is_likely_optimal = True; msg += \" Final KKT check approx OK.\"\n",
        "        else: msg += \" Final KKT check fails (dual infeasible).\"\n",
        "    else: msg += \" Final KKT check fails (stationarity or solve error).\"\n",
        "    lam_opt = final_lambda_estimate\n",
        "    return pi_k, lam_opt, active_constraints_opt, None, Q_reg / 2.0, is_likely_optimal, msg\n",
        "\n",
        "# --- make_psd  ---\n",
        "def make_psd(matrix, tolerance=1e-8):\n",
        "    sym = (matrix + matrix.T) / 2\n",
        "    try: eigenvalues, eigenvectors = np.linalg.eigh(sym); min_eigenvalue = np.min(eigenvalues)\n",
        "    except LinAlgError: warnings.warn(\"...\"); return sym\n",
        "    if min_eigenvalue < tolerance: eigenvalues[eigenvalues < tolerance] = tolerance; psd_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T; return (psd_matrix + psd_matrix.T) / 2\n",
        "    else: return sym\n",
        "\n",
        "# --- calculate_Vw  ---\n",
        "def calculate_Vw(w, R, SecondMoments_a_array, tolerance=DEFAULT_TOLERANCE, psd_tolerance=1e-8):\n",
        "    K, M = R.shape; w_sum = np.sum(w); w_norm = w\n",
        "    EwX = R @ w_norm; EwXXT = np.zeros((K, K));\n",
        "    for m in range(M): EwXXT += w_norm[m] * SecondMoments_a_array[m]\n",
        "    Vw = EwXXT - np.outer(EwX, EwX); Vw_psd = make_psd(Vw, psd_tolerance)\n",
        "    return Vw_psd, EwX, EwXXT\n",
        "\n",
        "# --- calculate_H_gradient  () ---\n",
        "def calculate_H_gradient(pi_star, w, R, SecondMoments_a_array, EwX, EwXXT, tolerance=1e-9, debug_print=False):\n",
        "    M = w.shape[0]; K = R.shape[0]; grad = np.zeros(M)\n",
        "    norm_tolerance = 1e-12 # Allow very small pi_star\n",
        "\n",
        "    if pi_star is None or EwX is None:\n",
        "        if debug_print: print(\"DEBUG grad_H: Returning NaN due to None input (pi_star or EwX).\")\n",
        "        return np.full(M, np.nan)\n",
        "    if np.isnan(pi_star).any() or np.isinf(pi_star).any():\n",
        "         if debug_print: print(\"DEBUG grad_H: Returning NaN because pi_star contains NaN/Inf.\")\n",
        "         return np.full(M, np.nan)\n",
        "    pi_norm = np.linalg.norm(pi_star)\n",
        "    if pi_norm < norm_tolerance:\n",
        "         if debug_print: print(f\"DEBUG grad_H: Returning NaN because pi_star norm {pi_norm:.2e} < {norm_tolerance:.1e}\")\n",
        "         return np.full(M, np.nan)\n",
        "    if np.isnan(EwX).any() or np.isinf(EwX).any():\n",
        "        if debug_print: print(\"DEBUG grad_H: Returning NaN because EwX contains NaN/Inf.\")\n",
        "        return np.full(M, np.nan)\n",
        "\n",
        "    try:\n",
        "        pi_T_EwX = pi_star.T @ EwX\n",
        "        if np.isnan(pi_T_EwX) or np.isinf(pi_T_EwX):\n",
        "             if debug_print: print(f\"DEBUG grad_H: pi_T_EwX is NaN/Inf: {pi_T_EwX}\")\n",
        "             return np.full(M, np.nan)\n",
        "\n",
        "        for j in range(M):\n",
        "            Sigma_j = SecondMoments_a_array[j]; r_j = R[:, j]\n",
        "            pi_T_Sigma_j_pi = pi_star.T @ Sigma_j @ pi_star\n",
        "            pi_T_r_j = pi_star.T @ r_j\n",
        "\n",
        "            if np.isnan(pi_T_Sigma_j_pi) or np.isinf(pi_T_Sigma_j_pi):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_Sigma_j_pi is NaN/Inf: {pi_T_Sigma_j_pi}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "            if np.isnan(pi_T_r_j) or np.isinf(pi_T_r_j):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_r_j is NaN/Inf: {pi_T_r_j}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "\n",
        "            term2 = 2 * pi_T_r_j * pi_T_EwX\n",
        "            if np.isnan(term2) or np.isinf(term2):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): term2 (2*pi_T_r_j*pi_T_EwX) is NaN/Inf: {term2}, pi_T_r_j={pi_T_r_j}, pi_T_EwX={pi_T_EwX}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "\n",
        "            grad[j] = pi_T_Sigma_j_pi - term2\n",
        "            if np.isnan(grad[j]) or np.isinf(grad[j]):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): final grad[{j}] is NaN/Inf\")\n",
        "\n",
        "    except Exception as e_calc:\n",
        "         print(f\"ERROR in calculate_H_gradient calculation: {e_calc}\")\n",
        "         print(traceback.format_exc())\n",
        "         return np.full(M, np.nan)\n",
        "\n",
        "    if np.any(np.isnan(grad)) or np.any(np.isinf(grad)):\n",
        "        warnings.warn(f\"NaN or Inf detected in calculated gradient for w={w}\")\n",
        "        if debug_print: print(f\"DEBUG grad_H: Final check found NaN/Inf in gradient: {grad}\")\n",
        "        return np.full(M, np.nan)\n",
        "    return grad\n",
        "\n",
        "# --- project_to_simplex  ---\n",
        "def project_to_simplex(v, z=1):\n",
        "    n_features = v.shape[0];\n",
        "    if n_features == 0: return np.array([])\n",
        "    v_arr = np.asarray(v)\n",
        "    if np.all(v_arr >= -1e-9) and np.isclose(np.sum(v_arr), z): return np.maximum(v_arr, 0)\n",
        "    u = np.sort(v_arr)[::-1]; cssv = np.cumsum(u) - z; ind = np.arange(n_features) + 1; cond = u - cssv / ind > 0\n",
        "    if np.any(cond): rho = ind[cond][-1]; theta = cssv[rho - 1] / float(rho); w = np.maximum(v_arr - theta, 0)\n",
        "    else:\n",
        "         w = np.zeros(n_features)\n",
        "         if z > 0: w[np.argmax(v_arr)] = z\n",
        "    w_sum = np.sum(w)\n",
        "    if not np.isclose(w_sum, z):\n",
        "        if w_sum > 1e-9: w = w * (z / w_sum)\n",
        "        elif z > 0 :\n",
        "            w = np.zeros(n_features)\n",
        "            w[np.argmax(v_arr)] = z\n",
        "    return np.maximum(w, 0)\n",
        "\n",
        "# --- frank_wolfe_optimizer  (ActiveSet) ---\n",
        "def frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde, initial_w=None, max_outer_iter=250, fw_gap_tol=1e-7, inner_max_iter=350, tolerance=1e-9, psd_make_tolerance=1e-8, qp_regularization=1e-10, debug_print=False, force_iterations=0, return_history=False):\n",
        "    K, M = R_alpha.shape\n",
        "    if initial_w is None: w_k = np.ones(M) / M\n",
        "    else: w_k = project_to_simplex(np.copy(initial_w))\n",
        "    if w_k is None:\n",
        "        result = OptimizationResult(False, \"Initial projection failed\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "    fw_gap = float('inf'); best_w = np.copy(w_k); best_pi = None; best_lam = np.zeros(M); best_H = -float('inf'); final_gHk = np.zeros(M); best_active_set = None\n",
        "    pi0, ok, p1msg = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance)\n",
        "    if not ok:\n",
        "        result = OptimizationResult(False, f\"Phase 1 failed: {p1msg}\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "    current_w=np.copy(w_k); current_H=-float('inf'); current_pi=None; current_lam=None; current_active_set=None\n",
        "    inner_solver_args_for_loop = {'max_iter': inner_max_iter, 'tolerance': tolerance, 'regularization_epsilon': qp_regularization}\n",
        "    history = []\n",
        "\n",
        "    last_successful_pi = pi0\n",
        "    last_successful_lam = np.zeros(M)\n",
        "    last_successful_active_set = tuple()\n",
        "    last_successful_gHk = np.zeros(M)\n",
        "    last_successful_fw_gap = float('inf')\n",
        "\n",
        "    for k in range(max_outer_iter):\n",
        "        iter_data = {'k': k + 1}\n",
        "        if return_history: iter_data['w_k'] = np.copy(w_k)\n",
        "        try: Vk, Ex, ExxT = calculate_Vw(w_k, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "        except Exception as e:\n",
        "            result = OptimizationResult(False, f\"Outer iter {k+1}: Vw failed: {e}\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, active_set_opt=best_active_set)\n",
        "            if return_history: history.append(iter_data); return (result, history)\n",
        "            else: return result\n",
        "\n",
        "        # === Use Active Set for inner solve ===\n",
        "        pi_init_inner = last_successful_pi if last_successful_pi is not None else pi0\n",
        "        pk, lk, act_idx_k, _, Vw_reg_k, inner_ok, inner_msg = solve_inner_qp_active_set(\n",
        "            Vk, R_alpha, mu_tilde, pi_init_inner, **inner_solver_args_for_loop\n",
        "        )\n",
        "        # ===================================\n",
        "\n",
        "        if not inner_ok or pk is None:\n",
        "            warnings.warn(f\"Outer iter {k+1}: Inner QP failed: {inner_msg}. Using last successful state if available.\")\n",
        "            if last_successful_pi is None:\n",
        "                 result = OptimizationResult(False, f\"Outer iter {k+1}: Inner QP failed and no prior success: {inner_msg}\",\n",
        "                                             w_opt=current_w, iterations=k)\n",
        "                 if return_history: history.append(iter_data); return (result, history)\n",
        "                 else: return result\n",
        "            pk = last_successful_pi\n",
        "            lk = last_successful_lam\n",
        "            act_idx_k = list(last_successful_active_set)\n",
        "            final_gHk = last_successful_gHk\n",
        "            fw_gap = last_successful_fw_gap\n",
        "            try: Hk = pk.T @ Vk @ pk\n",
        "            except: Hk = current_H\n",
        "        else:\n",
        "            last_successful_pi = pk\n",
        "            last_successful_lam = lk if lk is not None else np.zeros(M)\n",
        "            last_successful_active_set = tuple(sorted(act_idx_k)) if act_idx_k is not None else tuple()\n",
        "            Hk = pk.T @ Vk @ pk\n",
        "            try:\n",
        "                gHk = calculate_H_gradient(pk, w_k, R_alpha, SecondMoments_alpha_array, Ex, ExxT, tolerance, debug_print=debug_print)\n",
        "            except Exception as e:\n",
        "                result = OptimizationResult(False, f\"Outer iter {k+1}: Grad failed: {e}\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "                if return_history: history.append(iter_data); return (result, history)\n",
        "                else: return result\n",
        "\n",
        "            final_gHk = gHk if gHk is not None else np.full(M, np.nan)\n",
        "            last_successful_gHk = final_gHk if not np.isnan(final_gHk).any() else last_successful_gHk\n",
        "\n",
        "        if return_history: iter_data['H_k'] = Hk\n",
        "        if return_history: iter_data['grad_H_k_norm'] = np.linalg.norm(final_gHk) if not np.isnan(final_gHk).any() else np.nan\n",
        "\n",
        "        current_w = np.copy(w_k); current_H = Hk; current_pi = pk; current_lam = last_successful_lam; current_active_set = last_successful_active_set\n",
        "\n",
        "        if Hk >= best_H - tolerance*1000:\n",
        "             best_H = Hk; best_w = np.copy(w_k); best_pi = np.copy(pk); best_lam = np.copy(current_lam); best_active_set = current_active_set\n",
        "\n",
        "        if final_gHk is None or np.isnan(final_gHk).any():\n",
        "            result = OptimizationResult(False, f\"Outer iter {k+1}: Grad NaN.\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "            if return_history: history.append(iter_data); return (result, history)\n",
        "            else: return result\n",
        "\n",
        "        grad_norm = np.linalg.norm(final_gHk)\n",
        "        if grad_norm < tolerance * 10: sk = w_k; sk_idx = -1\n",
        "        else: sk_idx = np.argmax(final_gHk); sk = np.zeros(M); sk[sk_idx] = 1.0\n",
        "        if return_history: iter_data['s_k_index'] = sk_idx\n",
        "\n",
        "        if w_k is None:\n",
        "             result = OptimizationResult(False, f\"k={k} w_k None\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             if return_history: history.append(iter_data); return (result, history)\n",
        "             else: return result\n",
        "\n",
        "        fw_gap = final_gHk.T @ (w_k - sk)\n",
        "        last_successful_fw_gap = fw_gap\n",
        "        if return_history: iter_data['fw_gap'] = fw_gap\n",
        "        gamma = 2.0 / (k + 3.0)\n",
        "        if return_history: iter_data['gamma_k'] = gamma\n",
        "        if return_history: history.append(iter_data)\n",
        "\n",
        "        converged = False\n",
        "        if k >= force_iterations and not np.isnan(fw_gap):\n",
        "            if abs(fw_gap) <= fw_gap_tol: converged = True; conv_msg = f\"Converged (Gap {abs(fw_gap):.2e})\"\n",
        "        if converged:\n",
        "            # --- Final Inner Solve for Consistency ---\n",
        "            final_w = current_w\n",
        "            final_pi_result = current_pi\n",
        "            final_lam_result = current_lam\n",
        "            final_active_set_result = current_active_set\n",
        "            final_H_result = current_H\n",
        "            final_grad_result = final_gHk\n",
        "            final_fw_gap_result = fw_gap\n",
        "\n",
        "            try:\n",
        "                final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "                pi0_final, ok_final, _ = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance)\n",
        "                if ok_final:\n",
        "                    pi_s_final, lam_s_final, act_idx_final, _, _, inner_ok_final, msg_final = solve_inner_qp_active_set(\n",
        "                        final_Vk, R_alpha, mu_tilde, pi0_final, # Use stable pi0 for final check\n",
        "                        **inner_solver_args_for_loop\n",
        "                    )\n",
        "                    if inner_ok_final and pi_s_final is not None and lam_s_final is not None:\n",
        "                        final_pi_result = pi_s_final\n",
        "                        final_lam_result = lam_s_final\n",
        "                        final_active_set_result = tuple(sorted(act_idx_final)) if act_idx_final is not None else tuple()\n",
        "                        final_grad_result = calculate_H_gradient(final_pi_result, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance, debug_print=False)\n",
        "                        if final_grad_result is None or np.isnan(final_grad_result).any(): final_grad_result = np.zeros(M)\n",
        "                        grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                        if grad_norm_final < tolerance * 10: sk_final = final_w\n",
        "                        else: sk_idx_final = np.argmax(final_grad_result); sk_final = np.zeros(M); sk_final[sk_idx_final] = 1.0\n",
        "                        final_fw_gap_result = final_grad_result.T @ (final_w - sk_final) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                    else:\n",
        "                        warnings.warn(f\"Warning: Final inner QP solve failed after convergence: {msg_final}. Using last iteration's values.\")\n",
        "                else:\n",
        "                     warnings.warn(f\"Warning: Could not find feasible pi0 for final solve after convergence. Using last iteration's values.\")\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Error during final inner solve after convergence: {e}. Using last iteration's values.\")\n",
        "            # --- End of Final Inner Solve ---\n",
        "\n",
        "            result = OptimizationResult(True, conv_msg, w_opt=final_w, pi_opt=final_pi_result, lambda_opt=final_lam_result,\n",
        "                                        H_opt=final_H_result, grad_H_opt=final_grad_result, iterations=k + 1,\n",
        "                                        fw_gap=final_fw_gap_result, active_set_opt=final_active_set_result)\n",
        "            return (result, history) if return_history else result\n",
        "\n",
        "        # --- Prepare for next iteration ---\n",
        "        if w_k is None or sk is None:\n",
        "             result = OptimizationResult(False, f\"k={k} w_k/sk None before update\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             return (result, history) if return_history else result\n",
        "        w_k_next = (1.0 - gamma) * w_k + gamma * sk; w_k = project_to_simplex(w_k_next)\n",
        "        if w_k is None:\n",
        "             result = OptimizationResult(False, f\"k={k} proj None\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             return (result, history) if return_history else result\n",
        "        pi0 = pk # Use last inner solution as initial guess\n",
        "\n",
        "    # Max iter reached, perform final solve for consistency\n",
        "    final_w = current_w\n",
        "    final_pi_result = current_pi\n",
        "    final_lam_result = current_lam\n",
        "    final_active_set_result = current_active_set\n",
        "    final_H_result = current_H\n",
        "    final_grad_result = last_successful_gHk\n",
        "    final_fw_gap_result = last_successful_fw_gap\n",
        "\n",
        "    try:\n",
        "        final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "        pi0_final, ok_final, _ = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance) # Use stable pi0\n",
        "        if ok_final:\n",
        "            pi_s_final, lam_s_final, act_idx_final, _, _, inner_ok_final, msg_final = solve_inner_qp_active_set(\n",
        "                final_Vk, R_alpha, mu_tilde, pi0_final, # <-- Use stable pi0_final\n",
        "                **inner_solver_args_for_loop\n",
        "            )\n",
        "            if inner_ok_final and pi_s_final is not None and lam_s_final is not None:\n",
        "                final_pi_result = pi_s_final\n",
        "                final_lam_result = lam_s_final\n",
        "                final_active_set_result = tuple(sorted(act_idx_final)) if act_idx_final is not None else tuple()\n",
        "                # Recalculate gradient and gap\n",
        "                final_grad_result = calculate_H_gradient(final_pi_result, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance, debug_print=False)\n",
        "                if final_grad_result is None or np.isnan(final_grad_result).any(): final_grad_result = np.zeros(M)\n",
        "                grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                if grad_norm_final < tolerance * 10: sk_final = final_w\n",
        "                else: sk_idx_final = np.argmax(final_grad_result); sk_final = np.zeros(M); sk_final[sk_idx_final] = 1.0\n",
        "                final_fw_gap_result = final_grad_result.T @ (final_w - sk_final) if not np.isnan(final_grad_result).any() else np.nan\n",
        "            else:\n",
        "                warnings.warn(f\"Warning: Final inner QP solve failed after max iter: {msg_final}. Using last iteration's values.\")\n",
        "        else:\n",
        "             warnings.warn(f\"Warning: Could not find feasible pi0 for final solve after max iter. Using last iteration's values.\")\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error during final inner solve after max iter: {e}. Using last iteration's values.\")\n",
        "\n",
        "    if return_history:\n",
        "        grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "        if grad_norm_final < tolerance * 10: sk_idx_final = -1\n",
        "        else: sk_idx_final = np.argmax(final_grad_result) if not np.isnan(final_grad_result).any() else -1\n",
        "        history.append({\n",
        "            'k': max_outer_iter + 1, 'w_k': np.copy(final_w), 'H_k': final_H_result,\n",
        "            'grad_H_k_norm': grad_norm_final, 's_k_index': sk_idx_final, 'fw_gap': final_fw_gap_result,\n",
        "            'gamma_k': np.nan\n",
        "        })\n",
        "\n",
        "    result = OptimizationResult(True, f\"Max Iter ({max_outer_iter})\", w_opt=final_w, pi_opt=final_pi_result, lambda_opt=final_lam_result,\n",
        "                                H_opt=final_H_result, grad_H_opt=final_grad_result, iterations=max_outer_iter,\n",
        "                                fw_gap=final_fw_gap_result, active_set_opt=final_active_set_result)\n",
        "    return (result, history) if return_history else result\n",
        "\n",
        "# === generate_params_profile_switching  () ---\n",
        "def generate_params_profile_switching_symmetric(alpha, alpha_max, K=5, M=3,\n",
        "                                      R_base_sym=np.array([0.02, 0.01, 0.0, -0.01, -0.02]), # Neutral=0\n",
        "                                      sigma_base=np.array([0.18, 0.15, 0.20, 0.12, 0.10]),\n",
        "                                      Corr_base=np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]),\n",
        "                                      r_offset=0.03, # Good/Bad\n",
        "                                      s_factor=0.0,  # V^G=V^N=V^B \n",
        "                                      corr_factor=1.0, # Corr\n",
        "                                      corr_offset=0.0, # Corr\n",
        "                                      sigma_min_epsilon=1e-4, psd_tolerance=1e-9):\n",
        "    \"\"\" Generates symmetric parameters assuming V^G=V^N=V^B \"\"\"\n",
        "    assert K == len(R_base_sym) and K == len(sigma_base) and K == Corr_base.shape[0] and M == 3, \"Dimension mismatch\"\n",
        "\n",
        "    R_neutral = R_base_sym # Neutral expectation is the base\n",
        "    R_good = R_base_sym + r_offset\n",
        "    R_bad = R_base_sym - r_offset # Symmetric bad profile\n",
        "\n",
        "    # Assume V^G = V^N = V^B = V_base\n",
        "    sigma_neutral = np.maximum(sigma_min_epsilon, sigma_base)\n",
        "    Corr_neutral = make_psd(Corr_base, psd_tolerance)\n",
        "    V_base = np.diag(sigma_neutral) @ Corr_neutral @ np.diag(sigma_neutral)\n",
        "\n",
        "    # Calculate Sigma^m = V_base + r^m * r^m.T\n",
        "    Sigma_neutral = V_base + np.outer(R_neutral, R_neutral)\n",
        "    Sigma_good = V_base + np.outer(R_good, R_good)\n",
        "    Sigma_bad = V_base + np.outer(R_bad, R_bad)\n",
        "\n",
        "    # Apply profile switching based on alpha\n",
        "    beta = np.clip(alpha / alpha_max if alpha_max > 0 else (1.0 if alpha > 0 else 0.0), 0.0, 1.0)\n",
        "\n",
        "    R_alpha = np.zeros((K, M))\n",
        "    SecondMoments_a_array = np.zeros((M, K, K))\n",
        "\n",
        "    # Scenario 0: Mix Good and Neutral\n",
        "    R_alpha[:, 0] = (1 - beta) * R_good + beta * R_neutral\n",
        "    SecondMoments_a_array[0, :, :] = (1 - beta) * Sigma_good + beta * Sigma_neutral\n",
        "\n",
        "    # Scenario 1: Mix Bad and Good\n",
        "    R_alpha[:, 1] = (1 - beta) * R_bad + beta * R_good\n",
        "    SecondMoments_a_array[1, :, :] = (1 - beta) * Sigma_bad + beta * Sigma_good\n",
        "\n",
        "    # Scenario 2: Mix Neutral and Bad\n",
        "    R_alpha[:, 2] = (1 - beta) * R_neutral + beta * R_bad\n",
        "    SecondMoments_a_array[2, :, :] = (1 - beta) * Sigma_neutral + beta * Sigma_bad\n",
        "\n",
        "    return R_alpha, SecondMoments_a_array\n",
        "\n",
        "\n",
        "# ===  (ActiveSet) ===\n",
        "def analyze_alpha_full_results(alpha_range, param_gen_kwargs, optimizer_kwargs, mu_tilde):\n",
        "    \"\"\"  alpha  FWH*pi*grad H*lambda*active_set*  \"\"\"\n",
        "    results_over_alpha = []\n",
        "    K = param_gen_kwargs.get('K', 5); M = param_gen_kwargs.get('M', 3)\n",
        "\n",
        "    print(\"\\n--- Starting Full Results Analysis over Alpha Range (using Active Set Method) ---\")\n",
        "    total_alphas = len(alpha_range)\n",
        "    start_loop_time = time.time()\n",
        "\n",
        "    for idx, alpha in enumerate(alpha_range):\n",
        "        loop_start_time = time.time()\n",
        "        print(f\"\\rAnalyzing alpha = {alpha:.6f} ({idx+1}/{total_alphas}) ... \", end=\"\")\n",
        "\n",
        "        # ===  ===\n",
        "        #   \n",
        "        R_alpha, SecondMoments_alpha_array = generate_params_profile_switching_symmetric(alpha, **param_gen_kwargs)\n",
        "        alpha_result = {'alpha': alpha}\n",
        "\n",
        "        w_fw_default = np.full(M, np.nan)\n",
        "        H_star_fw = np.nan\n",
        "        pi_opt = np.full(K, np.nan)\n",
        "        grad_H_opt = np.full(M, np.nan)\n",
        "        lambda_opt = np.full(M, np.nan)\n",
        "        active_set_opt = tuple()\n",
        "\n",
        "        try:\n",
        "            # --- FW Optimizer (Active Set) ---\n",
        "            fw_result = frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde,\n",
        "                                              initial_w=None, return_history=False,\n",
        "                                              debug_print=False, # OFF\n",
        "                                              **optimizer_kwargs)\n",
        "            if fw_result.success:\n",
        "                 w_fw_default = fw_result.w_opt if fw_result.w_opt is not None else np.full(M, np.nan)\n",
        "                 H_star_fw = fw_result.H_opt if fw_result.H_opt is not None else np.nan\n",
        "                 pi_opt = fw_result.pi_opt if fw_result.pi_opt is not None else np.full(K, np.nan)\n",
        "                 grad_H_opt = fw_result.grad_H_opt if fw_result.grad_H_opt is not None else np.full(M, np.nan)\n",
        "                 lambda_opt = fw_result.lambda_opt if fw_result.lambda_opt is not None else np.full(M, np.nan)\n",
        "                 active_set_opt = fw_result.active_set_opt if fw_result.active_set_opt is not None else tuple()\n",
        "            else:\n",
        "                 w_fw_default = np.full(M, np.nan)\n",
        "                 H_star_fw = np.nan\n",
        "                 pi_opt = np.full(K, np.nan)\n",
        "                 grad_H_opt = np.full(M, np.nan)\n",
        "                 lambda_opt = np.full(M, np.nan)\n",
        "                 active_set_opt = tuple()\n",
        "                 print(f\"\\n  Warn: FW failed for alpha={alpha:.6f}: {fw_result.message}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n  Error during FW run for alpha={alpha:.6f}: {e}\")\n",
        "            w_fw_default = np.full(M, np.nan)\n",
        "            H_star_fw = np.nan\n",
        "            pi_opt = np.full(K, np.nan)\n",
        "            grad_H_opt = np.full(M, np.nan)\n",
        "            lambda_opt = np.full(M, np.nan)\n",
        "            active_set_opt = tuple()\n",
        "\n",
        "        alpha_result['w_fw_default'] = w_fw_default\n",
        "        alpha_result['H_star'] = H_star_fw\n",
        "        alpha_result['pi_opt'] = pi_opt\n",
        "        alpha_result['grad_H_opt'] = grad_H_opt\n",
        "        alpha_result['lambda_opt'] = lambda_opt\n",
        "        alpha_result['active_set_opt'] = active_set_opt\n",
        "\n",
        "        results_over_alpha.append(alpha_result)\n",
        "\n",
        "    print(\"\\n--- Finished Full Results Analysis ---\")\n",
        "    df_results = pd.DataFrame(results_over_alpha)\n",
        "    if PANDAS_AVAILABLE:\n",
        "        fw_vecs = np.stack([res.get('w_fw_default', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        pi_vecs = np.stack([res.get('pi_opt', np.full(K, np.nan)) for res in results_over_alpha])\n",
        "        grad_vecs = np.stack([res.get('grad_H_opt', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        lambda_vecs = np.stack([res.get('lambda_opt', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        active_sets = [str(res.get('active_set_opt', tuple())) for res in results_over_alpha]\n",
        "\n",
        "        for m in range(M):\n",
        "            df_results[f'w_fw_{m}'] = fw_vecs[:, m]\n",
        "            df_results[f'grad_H_{m}'] = grad_vecs[:, m]\n",
        "            df_results[f'lambda_{m}'] = lambda_vecs[:, m]\n",
        "        for k in range(K):\n",
        "            df_results[f'pi_{k}'] = pi_vecs[:, k]\n",
        "\n",
        "        df_results['active_set'] = active_sets\n",
        "\n",
        "        df_results = df_results.drop(columns=['w_fw_default', 'pi_opt', 'grad_H_opt', 'lambda_opt', 'active_set_opt'], errors='ignore')\n",
        "\n",
        "    return df_results if PANDAS_AVAILABLE else results_over_alpha\n",
        "\n",
        "\n",
        "# ===  ===\n",
        "if __name__ == '__main__':\n",
        "    start_time_main = time.time()\n",
        "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "    warnings.filterwarnings('ignore', category=UserWarning)\n",
        "    warnings.filterwarnings('ignore', category=OptimizeWarning)\n",
        "\n",
        "    # ---  ---\n",
        "    K = 5; M = 3;\n",
        "    #  mu_tilde  0.01  \n",
        "    mu_tilde = 0.01\n",
        "    alpha_max = 1.5 # beta\n",
        "\n",
        "    #   \n",
        "    param_gen_kwargs_symmetric = {\n",
        "        'K': K, 'M': M, 'alpha_max': alpha_max,\n",
        "        'R_base_sym': np.array([0.02, 0.01, 0.0, -0.01, -0.02]), # Neutral=0\n",
        "        'sigma_base': np.array([0.18, 0.15, 0.20, 0.12, 0.10]), # \n",
        "        'Corr_base': np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]), # \n",
        "        'r_offset': 0.03, # Good/Bad\n",
        "        's_factor': 0.0,  # V^G=V^N=V^B \n",
        "        'corr_factor': 1.0,\n",
        "        'corr_offset': 0.0,\n",
        "        'sigma_min_epsilon': 1e-4,\n",
        "        'psd_tolerance': 1e-9\n",
        "    }\n",
        "\n",
        "    solver_settings = { # \n",
        "        'max_outer_iter': 500, 'fw_gap_tol': 1e-9,\n",
        "        'inner_max_iter': 600,\n",
        "        'tolerance': 1e-11,\n",
        "        'psd_make_tolerance': 1e-9,\n",
        "        'qp_regularization': 1e-10,\n",
        "        'force_iterations': 20\n",
        "    }\n",
        "    #   alpha  (0.75 ) \n",
        "    alpha_start = 0.700\n",
        "    alpha_end = 0.800\n",
        "    alpha_step = 0.001\n",
        "    num_alpha_steps = int(round((alpha_end - alpha_start) / alpha_step)) + 1\n",
        "    alpha_range_analyze = np.linspace(alpha_start, alpha_end, num_alpha_steps)\n",
        "    unique_pt_tolerance = 1e-5 # KKT\n",
        "\n",
        "    output_csv_filename = f\"alpha_internal_sol_search_{alpha_start:.3f}_{alpha_end:.3f}_activeset_mu001_symmetric.csv\" # \n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"--- Searching for Internal Solution around alpha=0.75 [{alpha_start:.3f}, {alpha_end:.3f}] ---\")\n",
        "    print(f\"--- (Using ActiveSet, mu_tilde={mu_tilde}, Symmetric Params) ---\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # ---  ---\n",
        "    results_df = analyze_alpha_full_results(\n",
        "        alpha_range=alpha_range_analyze,\n",
        "        param_gen_kwargs=param_gen_kwargs_symmetric, # <-- \n",
        "        optimizer_kwargs=solver_settings,\n",
        "        mu_tilde=mu_tilde # <--  mu_tilde \n",
        "    )\n",
        "\n",
        "    # ---  & CSV & KKT ---\n",
        "    if PANDAS_AVAILABLE and isinstance(results_df, pd.DataFrame):\n",
        "        print(\"\\n--- Analysis Results Summary (DataFrame) ---\")\n",
        "        float_format_func = lambda x: f\"{x:.4e}\" if pd.notna(x) and isinstance(x, (float, np.number)) else x\n",
        "        pd.options.display.float_format = float_format_func\n",
        "        cols_to_show = ['alpha', 'H_star', 'active_set'] + \\\n",
        "                       [f'w_fw_{m}' for m in range(M) if f'w_fw_{m}' in results_df.columns] + \\\n",
        "                       [f'pi_{k}' for k in range(K) if f'pi_{k}' in results_df.columns] + \\\n",
        "                       [f'grad_H_{m}' for m in range(M) if f'grad_H_{m}' in results_df.columns] + \\\n",
        "                       [f'lambda_{m}' for m in range(M) if f'lambda_{m}' in results_df.columns]\n",
        "        cols_to_show = [col for col in cols_to_show if col in results_df.columns]\n",
        "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 200):\n",
        "             print(results_df[cols_to_show].to_string(index=False, na_rep='NaN'))\n",
        "        pd.reset_option('display.float_format')\n",
        "\n",
        "        # CSV ()\n",
        "        try:\n",
        "            df_to_save = results_df.copy()\n",
        "            cols_order = ['alpha', 'H_star', 'active_set'] + \\\n",
        "                         [f'w_fw_{m}' for m in range(M) if f'w_fw_{m}' in df_to_save.columns] + \\\n",
        "                         [f'pi_{k}' for k in range(K) if f'pi_{k}' in df_to_save.columns] + \\\n",
        "                         [f'grad_H_{m}' for m in range(M) if f'grad_H_{m}' in df_to_save.columns] + \\\n",
        "                         [f'lambda_{m}' for m in range(M) if f'lambda_{m}' in df_to_save.columns]\n",
        "            cols_order = [col for col in cols_order if col in df_to_save.columns]\n",
        "            df_to_save = df_to_save[cols_order]\n",
        "\n",
        "            df_to_save.to_csv(output_csv_filename, index=False, float_format='%.8e')\n",
        "            print(f\"\\nFull detailed results ({len(df_to_save)} points) saved to: {os.path.abspath(output_csv_filename)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError saving results to CSV: {e}\")\n",
        "\n",
        "        # KKT ()\n",
        "        print(\"\\n--- Quick KKT Check (Stationarity w.r.t. w) ---\")\n",
        "        kkt_violations = []\n",
        "        # unique_pt_tolerance  main \n",
        "        for index, row in results_df.iterrows():\n",
        "            alpha = row['alpha']\n",
        "            w_star = np.array([row.get(f'w_fw_{m}', np.nan) for m in range(M)])\n",
        "            grad_h = np.array([row.get(f'grad_H_{m}', np.nan) for m in range(M)])\n",
        "            active_set_str = row.get('active_set', '()')\n",
        "\n",
        "            if np.isnan(w_star).any() or np.isnan(grad_h).any() or np.isclose(np.sum(w_star), 0):\n",
        "                kkt_violations.append({\n",
        "                    'alpha': alpha,\n",
        "                    'active_set_report': active_set_str,\n",
        "                    'active_set_kkt': 'NaN',\n",
        "                    'max_violation': np.nan,\n",
        "                    'grad_consistency_violation': np.nan,\n",
        "                    'check_result': 'Skipped (NaN or zero w)'\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            check_tol = solver_settings['tolerance'] * 1e3\n",
        "\n",
        "            max_grad_h = np.max(grad_h)\n",
        "            nu_estimate = max_grad_h\n",
        "\n",
        "            stationarity_violation = 0.0\n",
        "            active_indices_kkt = set()\n",
        "            for m in range(M):\n",
        "                if w_star[m] > unique_pt_tolerance:\n",
        "                    active_indices_kkt.add(m)\n",
        "\n",
        "            active_grads = grad_h[list(active_indices_kkt)] if active_indices_kkt else []\n",
        "            grad_consistency_violation = 0.0\n",
        "            if len(active_grads) > 1:\n",
        "                grad_consistency_violation = np.max(active_grads) - np.min(active_grads)\n",
        "                if not np.isclose(grad_consistency_violation, 0.0, atol=check_tol):\n",
        "                     stationarity_violation = max(stationarity_violation, grad_consistency_violation)\n",
        "\n",
        "            for m in range(M):\n",
        "                if m not in active_indices_kkt:\n",
        "                    nu_for_check = np.min(active_grads) if active_grads else max_grad_h\n",
        "                    eta_m_estimate = nu_for_check - grad_h[m]\n",
        "                    if eta_m_estimate < -check_tol:\n",
        "                        stationarity_violation = max(stationarity_violation, abs(eta_m_estimate))\n",
        "\n",
        "            kkt_violations.append({\n",
        "                'alpha': alpha,\n",
        "                'active_set_report': active_set_str,\n",
        "                'active_set_kkt': str(tuple(sorted(active_indices_kkt))),\n",
        "                'max_violation': stationarity_violation,\n",
        "                'grad_consistency_violation': grad_consistency_violation,\n",
        "                'check_result': 'OK' if stationarity_violation < check_tol * 10 else 'WARN'\n",
        "            })\n",
        "\n",
        "        if PANDAS_AVAILABLE and kkt_violations:\n",
        "            df_kkt = pd.DataFrame(kkt_violations)\n",
        "            kkt_cols_order = ['alpha', 'active_set_report','active_set_kkt', 'max_violation', 'grad_consistency_violation', 'check_result']\n",
        "            kkt_cols_order = [col for col in kkt_cols_order if col in df_kkt.columns]\n",
        "            with pd.option_context('display.float_format', '{:.4e}'.format, 'display.max_rows', None): # \n",
        "                print(df_kkt[kkt_cols_order].to_string(index=False, na_rep='NaN'))\n",
        "            print(\"Notes on KKT Check:\")\n",
        "            print(\" - active_set_report: Active set reported by inner solver\")\n",
        "            print(\" - active_set_kkt: Active set inferred from w* > tol\")\n",
        "            print(\" - max_violation: Max KKT violation (dual infeasibility or comp. slackness based on eta estimate)\")\n",
        "            print(\" - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\")\n",
        "        else:\n",
        "            print(\"Could not perform KKT check or pandas not available.\")\n",
        "\n",
        "    elif isinstance(results_df, list):\n",
        "        print(\"\\n--- Analysis Results Summary (List) ---\")\n",
        "        # ()\n",
        "        print(\"\\n(Pandas not available, skipping CSV export)\")\n",
        "\n",
        "    end_time_main = time.time()\n",
        "    print(f\"\\nTotal execution time: {end_time_main - start_time_main:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go-s48yXwYb8",
        "outputId": "fbc11925-cd53-47e2-b726-cfffad78729f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "--- Searching for Internal Solution around alpha=0.75 [0.700, 0.800] ---\n",
            "--- (Using ActiveSet, mu_tilde=0.01, Symmetric Params) ---\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Starting Full Results Analysis over Alpha Range (using Active Set Method) ---\n",
            "Analyzing alpha = 0.800000 (101/101) ... \n",
            "--- Finished Full Results Analysis ---\n",
            "\n",
            "--- Analysis Results Summary (DataFrame) ---\n",
            "     alpha     H_star active_set     w_fw_0     w_fw_1     w_fw_2       pi_0       pi_1       pi_2       pi_3        pi_4   grad_H_0   grad_H_1   grad_H_2   lambda_0   lambda_1   lambda_2\n",
            "7.0000e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.0587e-01 0.0000e+00 8.9429e-02\n",
            "7.0100e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.0613e-01 0.0000e+00 8.9165e-02\n",
            "7.0200e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4613e-01 1.4916e-01 0.0000e+00\n",
            "7.0300e-01 1.9765e-03     (0, 2) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.0666e-01 0.0000e+00 8.8638e-02\n",
            "7.0400e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.0692e-01 0.0000e+00 8.8375e-02\n",
            "7.0500e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4595e-01 1.4934e-01 0.0000e+00\n",
            "7.0600e-01 1.9765e-03     (0, 2) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.0745e-01 0.0000e+00 8.7848e-02\n",
            "7.0700e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.0771e-01 0.0000e+00 8.7584e-02\n",
            "7.0800e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.0797e-01 0.0000e+00 8.7320e-02\n",
            "7.0900e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4571e-01 1.4958e-01 0.0000e+00\n",
            "7.1000e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.0850e-01 0.0000e+00 8.6793e-02\n",
            "7.1100e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4559e-01 1.4971e-01 0.0000e+00\n",
            "7.1200e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.0903e-01 0.0000e+00 8.6266e-02\n",
            "7.1300e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4546e-01 1.4983e-01 0.0000e+00\n",
            "7.1400e-01 1.9765e-03     (0, 2) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.0956e-01 0.0000e+00 8.5739e-02\n",
            "7.1500e-01 1.9765e-03     (0, 2) 1.4430e-03 7.9365e-01 2.0491e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.0982e-01 0.0000e+00 8.5476e-02\n",
            "7.1600e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1008e-01 0.0000e+00 8.5212e-02\n",
            "7.1700e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1035e-01 0.0000e+00 8.4949e-02\n",
            "7.1800e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1061e-01 0.0000e+00 8.4685e-02\n",
            "7.1900e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1087e-01 0.0000e+00 8.4422e-02\n",
            "7.2000e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1114e-01 0.0000e+00 8.4158e-02\n",
            "7.2100e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1140e-01 0.0000e+00 8.3895e-02\n",
            "7.2200e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1166e-01 0.0000e+00 8.3631e-02\n",
            "7.2300e-01 1.9765e-03     (0, 1) 1.4430e-03 1.7893e-01 8.1962e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4481e-01 1.5048e-01 0.0000e+00\n",
            "7.2400e-01 1.9765e-03     (0, 1) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4474e-01 1.5055e-01 0.0000e+00\n",
            "7.2500e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4468e-01 1.5062e-01 0.0000e+00\n",
            "7.2600e-01 1.9765e-03     (0, 2) 1.4430e-03 7.7633e-01 2.2222e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1272e-01 0.0000e+00 8.2577e-02\n",
            "7.2700e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1298e-01 0.0000e+00 8.2313e-02\n",
            "7.2800e-01 1.9765e-03     (0, 2) 6.4214e-01 3.5642e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1325e-01 0.0000e+00 8.2050e-02\n",
            "7.2900e-01 1.9765e-03     (0, 2) 7.5036e-02 9.2352e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1351e-01 0.0000e+00 8.1786e-02\n",
            "7.3000e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1377e-01 0.0000e+00 8.1523e-02\n",
            "7.3100e-01 1.9765e-03     (0, 2) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1404e-01 0.0000e+00 8.1259e-02\n",
            "7.3200e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4418e-01 1.5111e-01 0.0000e+00\n",
            "7.3300e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4411e-01 1.5118e-01 0.0000e+00\n",
            "7.3400e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1483e-01 0.0000e+00 8.0469e-02\n",
            "7.3500e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1509e-01 0.0000e+00 8.0205e-02\n",
            "7.3600e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4389e-01 1.5140e-01 0.0000e+00\n",
            "7.3700e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1562e-01 0.0000e+00 7.9678e-02\n",
            "7.3800e-01 1.9765e-03     (0, 1) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4374e-01 1.5155e-01 0.0000e+00\n",
            "7.3900e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4366e-01 1.5163e-01 0.0000e+00\n",
            "7.4000e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4359e-01 1.5171e-01 0.0000e+00\n",
            "7.4100e-01 1.9765e-03     (0, 1) 1.4430e-03 4.5599e-01 5.4257e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4351e-01 1.5178e-01 0.0000e+00\n",
            "7.4200e-01 1.9765e-03     (0, 1) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4343e-01 1.5186e-01 0.0000e+00\n",
            "7.4300e-01 1.9765e-03     (0, 2) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1720e-01 0.0000e+00 7.8097e-02\n",
            "7.4400e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1746e-01 0.0000e+00 7.7833e-02\n",
            "7.4500e-01 1.9765e-03     (0, 2) 1.4430e-03 8.1097e-01 1.8759e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1773e-01 0.0000e+00 7.7570e-02\n",
            "7.4600e-01 1.9765e-03     (0, 1) 4.8629e-01 5.1227e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4312e-01 1.5218e-01 0.0000e+00\n",
            "7.4700e-01 1.9765e-03     (0, 2) 1.4430e-03 8.8456e-01 1.1400e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1825e-01 0.0000e+00 7.7043e-02\n",
            "7.4800e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1852e-01 0.0000e+00 7.6779e-02\n",
            "7.4900e-01 1.9765e-03     (0, 2) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1878e-01 0.0000e+00 7.6516e-02\n",
            "7.5000e-01 1.9765e-03     (0, 2) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1904e-01 0.0000e+00 7.6252e-02\n",
            "7.5100e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1931e-01 0.0000e+00 7.5989e-02\n",
            "7.5200e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.1957e-01 0.0000e+00 7.5725e-02\n",
            "7.5300e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4254e-01 1.5276e-01 0.0000e+00\n",
            "7.5400e-01 1.9765e-03     (0, 2) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2010e-01 0.0000e+00 7.5198e-02\n",
            "7.5500e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4237e-01 1.5293e-01 0.0000e+00\n",
            "7.5600e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2062e-01 0.0000e+00 7.4671e-02\n",
            "7.5700e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4219e-01 1.5310e-01 0.0000e+00\n",
            "7.5800e-01 1.9765e-03     (0, 2) 1.1400e-01 8.8456e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2115e-01 0.0000e+00 7.4144e-02\n",
            "7.5900e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4202e-01 1.5328e-01 0.0000e+00\n",
            "7.6000e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2168e-01 0.0000e+00 7.3617e-02\n",
            "7.6100e-01 1.9765e-03     (0, 1) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4184e-01 1.5346e-01 0.0000e+00\n",
            "7.6200e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4175e-01 1.5355e-01 0.0000e+00\n",
            "7.6300e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2247e-01 0.0000e+00 7.2826e-02\n",
            "7.6400e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4156e-01 1.5373e-01 0.0000e+00\n",
            "7.6500e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4147e-01 1.5383e-01 0.0000e+00\n",
            "7.6600e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2326e-01 0.0000e+00 7.2036e-02\n",
            "7.6700e-01 1.9765e-03     (0, 1) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4128e-01 1.5402e-01 0.0000e+00\n",
            "7.6800e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2379e-01 0.0000e+00 7.1509e-02\n",
            "7.6900e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2405e-01 0.0000e+00 7.1245e-02\n",
            "7.7000e-01 1.9765e-03     (0, 1) 1.4430e-03 3.4776e-01 6.5079e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4099e-01 1.5431e-01 0.0000e+00\n",
            "7.7100e-01 1.9765e-03     (0, 1) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4089e-01 1.5441e-01 0.0000e+00\n",
            "7.7200e-01 1.9765e-03     (0, 1) 1.4430e-03 8.0231e-01 1.9625e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4079e-01 1.5451e-01 0.0000e+00\n",
            "7.7300e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4069e-01 1.5461e-01 0.0000e+00\n",
            "7.7400e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2537e-01 0.0000e+00 6.9927e-02\n",
            "7.7500e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4049e-01 1.5481e-01 0.0000e+00\n",
            "7.7600e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2589e-01 0.0000e+00 6.9400e-02\n",
            "7.7700e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4028e-01 1.5502e-01 0.0000e+00\n",
            "7.7800e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2642e-01 0.0000e+00 6.8873e-02\n",
            "7.7900e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.4007e-01 1.5523e-01 0.0000e+00\n",
            "7.8000e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2695e-01 0.0000e+00 6.8346e-02\n",
            "7.8100e-01 1.9765e-03     (0, 1) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3986e-01 1.5544e-01 0.0000e+00\n",
            "7.8200e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3975e-01 1.5555e-01 0.0000e+00\n",
            "7.8300e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3964e-01 1.5566e-01 0.0000e+00\n",
            "7.8400e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2800e-01 0.0000e+00 6.7292e-02\n",
            "7.8500e-01 1.9765e-03     (0, 1) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3941e-01 1.5588e-01 0.0000e+00\n",
            "7.8600e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2853e-01 0.0000e+00 6.6765e-02\n",
            "7.8700e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3919e-01 1.5611e-01 0.0000e+00\n",
            "7.8800e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3907e-01 1.5622e-01 0.0000e+00\n",
            "7.8900e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2932e-01 0.0000e+00 6.5975e-02\n",
            "7.9000e-01 1.9765e-03     (0, 2) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.2958e-01 0.0000e+00 6.5711e-02\n",
            "7.9100e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3872e-01 1.5657e-01 0.0000e+00\n",
            "7.9200e-01 1.9765e-03     (0, 1) 1.4430e-03 1.4430e-03 9.9711e-01 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3860e-01 1.5669e-01 0.0000e+00\n",
            "7.9300e-01 1.9765e-03     (0, 1) 8.5859e-01 1.3997e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3848e-01 1.5681e-01 0.0000e+00\n",
            "7.9400e-01 1.9765e-03     (0, 1) 1.4430e-03 9.9711e-01 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3836e-01 1.5693e-01 0.0000e+00\n",
            "7.9500e-01 1.9765e-03     (0, 2) 9.8846e-01 1.0101e-02 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.3090e-01 0.0000e+00 6.4393e-02\n",
            "7.9600e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3811e-01 1.5718e-01 0.0000e+00\n",
            "7.9700e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3799e-01 1.5731e-01 0.0000e+00\n",
            "7.9800e-01 1.9765e-03     (0, 1) 9.2352e-01 1.4430e-03 7.5036e-02 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3786e-01 1.5743e-01 0.0000e+00\n",
            "7.9900e-01 1.9765e-03     (0, 2) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 3.3196e-01 0.0000e+00 6.3339e-02\n",
            "8.0000e-01 1.9765e-03     (0, 1) 9.9711e-01 1.4430e-03 1.4430e-03 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8765e-03 1.8765e-03 1.8765e-03 2.3761e-01 1.5769e-01 0.0000e+00\n",
            "\n",
            "Full detailed results (101 points) saved to: /content/alpha_internal_sol_search_0.700_0.800_activeset_mu001_symmetric.csv\n",
            "\n",
            "--- Quick KKT Check (Stationarity w.r.t. w) ---\n",
            "     alpha active_set_report active_set_kkt  max_violation  grad_consistency_violation check_result\n",
            "7.0000e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  8.6736e-19           OK\n",
            "7.0100e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.0200e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.0300e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.0400e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.0500e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.0600e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  0.0000e+00           OK\n",
            "7.0700e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.0800e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  0.0000e+00           OK\n",
            "7.0900e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.1000e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  0.0000e+00           OK\n",
            "7.1100e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  8.6736e-19           OK\n",
            "7.1200e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  8.6736e-19           OK\n",
            "7.1300e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.1400e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.1500e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.1600e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.1700e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.1800e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.1900e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.2000e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  0.0000e+00           OK\n",
            "7.2100e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.2200e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.2300e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.2400e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  0.0000e+00           OK\n",
            "7.2500e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.2600e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  0.0000e+00           OK\n",
            "7.2700e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.2800e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.2900e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.3000e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.3100e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.3200e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.3300e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.3400e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.3500e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.3600e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.3700e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.3800e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.3900e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.4000e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.4100e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.4200e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  1.0842e-18           OK\n",
            "7.4300e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.4400e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.4500e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.4600e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.4700e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  0.0000e+00           OK\n",
            "7.4800e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  0.0000e+00           OK\n",
            "7.4900e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.5000e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.5100e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.5200e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.5300e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.5400e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.5500e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.5600e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.5700e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.5800e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.5900e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.6000e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  1.0842e-18           OK\n",
            "7.6100e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.6200e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  8.6736e-19           OK\n",
            "7.6300e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.6400e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.6500e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.6600e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.6700e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.6800e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.6900e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.7000e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.7100e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.7200e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.7300e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.7400e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.7500e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  8.6736e-19           OK\n",
            "7.7600e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.7700e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.7800e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.7900e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.8000e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  0.0000e+00           OK\n",
            "7.8100e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.8200e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.8300e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.8400e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.8500e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  0.0000e+00           OK\n",
            "7.8600e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.8700e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.8800e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.8900e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.9000e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.9100e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.9200e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  8.6736e-19           OK\n",
            "7.9300e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.9400e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.9500e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  4.3368e-19           OK\n",
            "7.9600e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.9700e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "7.9800e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  2.1684e-19           OK\n",
            "7.9900e-01            (0, 2)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "8.0000e-01            (0, 1)      (0, 1, 2)     0.0000e+00                  6.5052e-19           OK\n",
            "Notes on KKT Check:\n",
            " - active_set_report: Active set reported by inner solver\n",
            " - active_set_kkt: Active set inferred from w* > tol\n",
            " - max_violation: Max KKT violation (dual infeasibility or comp. slackness based on eta estimate)\n",
            " - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\n",
            "\n",
            "Total execution time: 3.73 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "4_14_find_internal_solution_quasi_symmetric.py\n",
        "\n",
        "Attempts to find an internal solution w* by using quasi-symmetric parameters\n",
        "(slight difference in Vm) around alpha=0.75, testing various mu_tilde values.\n",
        "Uses the Active Set method for the inner QP solve.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from scipy.linalg import solve, LinAlgError, eigh, inv, qr\n",
        "from scipy.optimize import linprog, OptimizeWarning\n",
        "import traceback\n",
        "import time\n",
        "import itertools\n",
        "import os\n",
        "import math\n",
        "from collections import Counter # \n",
        "\n",
        "# pandas \n",
        "try:\n",
        "    import pandas as pd\n",
        "    PANDAS_AVAILABLE = True\n",
        "    pd.set_option('display.width', 160)\n",
        "    pd.set_option('display.float_format', '{:.6e}'.format) # \n",
        "    pd.set_option('display.max_rows', 200) # \n",
        "except ImportError:\n",
        "    PANDAS_AVAILABLE = False\n",
        "    print(\"Warning: pandas library not found. Output formatting will be basic. CSV export disabled.\")\n",
        "\n",
        "# ===  ===\n",
        "DEFAULT_TOLERANCE = 1e-9\n",
        "\n",
        "# --- OptimizationResult  ---\n",
        "class OptimizationResult:\n",
        "    def __init__(self, success, message, w_opt=None, pi_opt=None, lambda_opt=None,\n",
        "                 H_opt=None, grad_H_opt=None, iterations=None, fw_gap=None,\n",
        "                 active_set_opt=None):\n",
        "        self.success = success; self.message = message; self.w_opt = w_opt; self.pi_opt = pi_opt\n",
        "        self.lambda_opt = lambda_opt; self.H_opt = H_opt; self.grad_H_opt = grad_H_opt\n",
        "        self.iterations = iterations; self.fw_gap = fw_gap\n",
        "        self.active_set_opt = active_set_opt\n",
        "\n",
        "# --- find_feasible_initial_pi  ---\n",
        "def find_feasible_initial_pi(R, mu_tilde, K, tolerance=1e-8):\n",
        "    M = R.shape[1]; c = np.zeros(K + 1); c[K] = 1.0\n",
        "    A_ub = np.hstack((-R.T, -np.ones((M, 1)))); b_ub = -mu_tilde * np.ones(M)\n",
        "    bounds = [(None, None)] * K + [(0, None)]; opts = {'tol': tolerance, 'disp': False, 'presolve': True}\n",
        "    result = None; methods_to_try = ['highs', 'highs-ipm', 'highs-ds', 'simplex']\n",
        "    for method in methods_to_try:\n",
        "        try:\n",
        "            with warnings.catch_warnings(): warnings.filterwarnings(\"ignore\"); result = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=method, options=opts)\n",
        "            if result.success: break\n",
        "        except ValueError: continue\n",
        "        except Exception as e: return None, False, f\"Phase 1 LP failed: {e}\"\n",
        "    if result is None or not result.success: msg = result.message if result else \"No solver\"; status = result.status if result else -1; return None, False, f\"Phase 1 LP solver failed: {msg} (status={status})\"\n",
        "    s = result.x[K]; pi = result.x[:K]\n",
        "    if np.isnan(pi).any(): return None, False, \"Phase 1 LP resulted in NaN values for pi.\"\n",
        "    if s <= tolerance * 1000:\n",
        "        G = -R.T; h = -mu_tilde * np.ones(M); violation = np.max(G @ pi - h)\n",
        "        if violation <= tolerance * 10000: return pi, True, f\"Phase 1 OK (s*={s:.1e}, vio={violation:.1e})\"\n",
        "        else: return pi, True, f\"Phase 1 OK (s*={s:.1e}), WARN: violation {violation:.1e}\"\n",
        "    else: return None, False, f\"Phase 1 Infeasible (s* = {s:.1e})\"\n",
        "\n",
        "# --- solve_kkt_system  ---\n",
        "def solve_kkt_system(Q, G_W, g, tolerance=DEFAULT_TOLERANCE):\n",
        "    K = Q.shape[0]; n_act = G_W.shape[0] if G_W is not None and G_W.ndim == 2 and G_W.shape[0] > 0 else 0\n",
        "    if n_act == 0:\n",
        "        try: p = solve(Q, -g, assume_a='sym'); l = np.array([])\n",
        "        except LinAlgError: return None, None, False\n",
        "        res_norm = np.linalg.norm(Q @ p + g); g_norm = np.linalg.norm(g); solved_ok = res_norm <= tolerance * 1e3 * (1 + g_norm)\n",
        "        return p, l, solved_ok\n",
        "    else:\n",
        "        kkt_mat = None; rhs = None\n",
        "        try: kkt_mat = np.block([[Q, G_W.T], [G_W, np.zeros((n_act, n_act))]]) ; rhs = np.concatenate([-g, np.zeros(n_act)])\n",
        "        except ValueError as e: return None, None, False\n",
        "        try: sol = solve(kkt_mat, rhs, assume_a='sym'); p = sol[:K]; l = sol[K:]\n",
        "        except LinAlgError: return None, None, False\n",
        "        except ValueError as e: return None, None, False\n",
        "        res_norm = np.linalg.norm(kkt_mat @ sol - rhs); rhs_norm = np.linalg.norm(rhs); solved_ok = res_norm <= tolerance * 1e3 * (1 + rhs_norm)\n",
        "        return p, l, solved_ok\n",
        "\n",
        "# --- solve_inner_qp_active_set  ---\n",
        "def solve_inner_qp_active_set(Vw, R, mu_tilde, initial_pi, max_iter=350, tolerance=DEFAULT_TOLERANCE, regularization_epsilon=1e-10):\n",
        "    K = Vw.shape[0]; M = R.shape[1]; Q_reg = 2 * Vw + 2 * regularization_epsilon * np.eye(K); G = -R.T; h = -mu_tilde * np.ones(M)\n",
        "    if initial_pi is None: return None, None, None, None, None, False, \"No initial pi\"\n",
        "    pi_k = np.copy(initial_pi); lam_opt = np.zeros(M); W = set()\n",
        "    active_tol = tolerance * 10\n",
        "    initial_violations = G @ pi_k - h; W = set(j for j, viol in enumerate(initial_violations) if viol > -active_tol)\n",
        "    active_indices_opt = None; kkt_matrix_opt = None\n",
        "    if np.any(initial_violations > active_tol * 10): warnings.warn(f\"Initial pi infeasible (max viol: {np.max(initial_violations):.2e}).\")\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        g_k = Q_reg @ pi_k; act = sorted(list(W)); n_act = len(act); G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "        p_k, lam_Wk, solved = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "        if not solved or p_k is None: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: KKT solve failed. ActiveSet={act}\"\n",
        "        if np.linalg.norm(p_k) <= tolerance * 10 * (1 + np.linalg.norm(pi_k)):\n",
        "            is_optimal_point = True; blocking_constraint_idx = -1; min_negative_lambda = float('inf')\n",
        "            dual_feas_tol = -tolerance * 10\n",
        "            if W:\n",
        "                if lam_Wk is None or len(lam_Wk) != n_act: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: lam_Wk inconsistent? ActiveSet={act}\"\n",
        "                lambda_map = dict(zip(act, lam_Wk))\n",
        "                for constraint_idx, lagrange_multiplier in lambda_map.items():\n",
        "                    if lagrange_multiplier < dual_feas_tol: is_optimal_point = False;\n",
        "                    if lagrange_multiplier < min_negative_lambda: min_negative_lambda = lagrange_multiplier; blocking_constraint_idx = constraint_idx\n",
        "            if is_optimal_point:\n",
        "                lam_opt.fill(0.0);\n",
        "                if W and len(lam_Wk) == n_act: lam_opt[act] = np.maximum(lam_Wk, 0)\n",
        "                final_infeas = np.max(G @ pi_k - h); msg = f\"Optimal found at iter {i+1}.\"\n",
        "                if final_infeas > active_tol: msg += f\" (WARN: violation {final_infeas:.1e})\"\n",
        "                active_indices_opt = act\n",
        "                return pi_k, lam_opt, active_indices_opt, None, Q_reg / 2.0, True, msg\n",
        "            else:\n",
        "                if blocking_constraint_idx in W: W.remove(blocking_constraint_idx); continue\n",
        "                else: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Neg lambda idx {blocking_constraint_idx}, not in W={act}\"\n",
        "        else:\n",
        "            alpha_k = 1.0; blocking_constraint_idx = -1; min_step_length = float('inf')\n",
        "            step_tol = tolerance * 10\n",
        "            for j in range(M):\n",
        "                if j not in W:\n",
        "                    constraint_gradient_dot_p = G[j, :] @ p_k\n",
        "                    if constraint_gradient_dot_p > step_tol:\n",
        "                        distance_to_boundary = h[j] - (G[j, :] @ pi_k)\n",
        "                        if abs(constraint_gradient_dot_p) > 1e-15:\n",
        "                            alpha_j = distance_to_boundary / constraint_gradient_dot_p\n",
        "                            step_j = max(0.0, alpha_j)\n",
        "                            if step_j < min_step_length:\n",
        "                                min_step_length = step_j; blocking_constraint_idx = j\n",
        "            alpha_k = min(1.0, min_step_length); pi_k += alpha_k * p_k\n",
        "            if alpha_k < 1.0 - step_tol and blocking_constraint_idx != -1:\n",
        "                if blocking_constraint_idx not in W: W.add(blocking_constraint_idx)\n",
        "            continue\n",
        "    msg = f\"Max iter ({max_iter}) reached.\"; final_infeas = np.max(G @ pi_k - h)\n",
        "    if final_infeas > active_tol * 10: return None, None, None, None, None, False, f\"{msg} Final infeasible. ActiveSet={sorted(list(W))}\"\n",
        "    act = sorted(list(W)); n_act = len(act); G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "    is_likely_optimal = False; active_constraints_opt = act\n",
        "    g_k = Q_reg @ pi_k; p_f, lam_f, solved_f = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "    final_lambda_estimate = np.zeros(M)\n",
        "    if solved_f and p_f is not None and np.linalg.norm(p_f) <= tolerance * 100 * (1 + np.linalg.norm(pi_k)):\n",
        "        if n_act > 0 and lam_f is not None and len(lam_f) == n_act:\n",
        "             try: final_lambda_estimate[act] = lam_f\n",
        "             except IndexError: pass\n",
        "        active_lambdas = final_lambda_estimate[act] if n_act > 0 else np.array([])\n",
        "        if n_act == 0 or np.all(active_lambdas >= -tolerance * 100): is_likely_optimal = True; msg += \" Final KKT check approx OK.\"\n",
        "        else: msg += \" Final KKT check fails (dual infeasible).\"\n",
        "    else: msg += \" Final KKT check fails (stationarity or solve error).\"\n",
        "    lam_opt = final_lambda_estimate\n",
        "    return pi_k, lam_opt, active_constraints_opt, None, Q_reg / 2.0, is_likely_optimal, msg\n",
        "\n",
        "# --- make_psd  ---\n",
        "def make_psd(matrix, tolerance=1e-8):\n",
        "    sym = (matrix + matrix.T) / 2\n",
        "    try: eigenvalues, eigenvectors = np.linalg.eigh(sym); min_eigenvalue = np.min(eigenvalues)\n",
        "    except LinAlgError: warnings.warn(\"...\"); return sym\n",
        "    if min_eigenvalue < tolerance: eigenvalues[eigenvalues < tolerance] = tolerance; psd_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T; return (psd_matrix + psd_matrix.T) / 2\n",
        "    else: return sym\n",
        "\n",
        "# --- calculate_Vw  ---\n",
        "def calculate_Vw(w, R, SecondMoments_a_array, tolerance=DEFAULT_TOLERANCE, psd_tolerance=1e-8):\n",
        "    K, M = R.shape; w_sum = np.sum(w); w_norm = w\n",
        "    EwX = R @ w_norm; EwXXT = np.zeros((K, K));\n",
        "    for m in range(M): EwXXT += w_norm[m] * SecondMoments_a_array[m]\n",
        "    Vw = EwXXT - np.outer(EwX, EwX); Vw_psd = make_psd(Vw, psd_tolerance)\n",
        "    return Vw_psd, EwX, EwXXT\n",
        "\n",
        "# --- calculate_H_gradient  () ---\n",
        "def calculate_H_gradient(pi_star, w, R, SecondMoments_a_array, EwX, EwXXT, tolerance=1e-9, debug_print=False):\n",
        "    M = w.shape[0]; K = R.shape[0]; grad = np.zeros(M)\n",
        "    norm_tolerance = 1e-12 # Allow very small pi_star\n",
        "\n",
        "    if pi_star is None or EwX is None:\n",
        "        if debug_print: print(\"DEBUG grad_H: Returning NaN due to None input (pi_star or EwX).\")\n",
        "        return np.full(M, np.nan)\n",
        "    if np.isnan(pi_star).any() or np.isinf(pi_star).any():\n",
        "         if debug_print: print(\"DEBUG grad_H: Returning NaN because pi_star contains NaN/Inf.\")\n",
        "         return np.full(M, np.nan)\n",
        "    pi_norm = np.linalg.norm(pi_star)\n",
        "    if pi_norm < norm_tolerance:\n",
        "         if debug_print: print(f\"DEBUG grad_H: Returning NaN because pi_star norm {pi_norm:.2e} < {norm_tolerance:.1e}\")\n",
        "         return np.full(M, np.nan)\n",
        "    if np.isnan(EwX).any() or np.isinf(EwX).any():\n",
        "        if debug_print: print(\"DEBUG grad_H: Returning NaN because EwX contains NaN/Inf.\")\n",
        "        return np.full(M, np.nan)\n",
        "\n",
        "    try:\n",
        "        pi_T_EwX = pi_star.T @ EwX\n",
        "        if np.isnan(pi_T_EwX) or np.isinf(pi_T_EwX):\n",
        "             if debug_print: print(f\"DEBUG grad_H: pi_T_EwX is NaN/Inf: {pi_T_EwX}\")\n",
        "             return np.full(M, np.nan)\n",
        "\n",
        "        for j in range(M):\n",
        "            Sigma_j = SecondMoments_a_array[j]; r_j = R[:, j]\n",
        "            pi_T_Sigma_j_pi = pi_star.T @ Sigma_j @ pi_star\n",
        "            pi_T_r_j = pi_star.T @ r_j\n",
        "\n",
        "            if np.isnan(pi_T_Sigma_j_pi) or np.isinf(pi_T_Sigma_j_pi):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_Sigma_j_pi is NaN/Inf: {pi_T_Sigma_j_pi}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "            if np.isnan(pi_T_r_j) or np.isinf(pi_T_r_j):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_r_j is NaN/Inf: {pi_T_r_j}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "\n",
        "            term2 = 2 * pi_T_r_j * pi_T_EwX\n",
        "            if np.isnan(term2) or np.isinf(term2):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): term2 (2*pi_T_r_j*pi_T_EwX) is NaN/Inf: {term2}, pi_T_r_j={pi_T_r_j}, pi_T_EwX={pi_T_EwX}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "\n",
        "            grad[j] = pi_T_Sigma_j_pi - term2\n",
        "            if np.isnan(grad[j]) or np.isinf(grad[j]):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): final grad[{j}] is NaN/Inf\")\n",
        "\n",
        "    except Exception as e_calc:\n",
        "         print(f\"ERROR in calculate_H_gradient calculation: {e_calc}\")\n",
        "         print(traceback.format_exc())\n",
        "         return np.full(M, np.nan)\n",
        "\n",
        "    if np.any(np.isnan(grad)) or np.any(np.isinf(grad)):\n",
        "        warnings.warn(f\"NaN or Inf detected in calculated gradient for w={w}\")\n",
        "        if debug_print: print(f\"DEBUG grad_H: Final check found NaN/Inf in gradient: {grad}\")\n",
        "        return np.full(M, np.nan)\n",
        "    return grad\n",
        "\n",
        "# --- project_to_simplex  ---\n",
        "def project_to_simplex(v, z=1):\n",
        "    n_features = v.shape[0];\n",
        "    if n_features == 0: return np.array([])\n",
        "    v_arr = np.asarray(v)\n",
        "    if np.all(v_arr >= -1e-9) and np.isclose(np.sum(v_arr), z): return np.maximum(v_arr, 0)\n",
        "    u = np.sort(v_arr)[::-1]; cssv = np.cumsum(u) - z; ind = np.arange(n_features) + 1; cond = u - cssv / ind > 0\n",
        "    if np.any(cond): rho = ind[cond][-1]; theta = cssv[rho - 1] / float(rho); w = np.maximum(v_arr - theta, 0)\n",
        "    else:\n",
        "         w = np.zeros(n_features)\n",
        "         if z > 0: w[np.argmax(v_arr)] = z\n",
        "    w_sum = np.sum(w)\n",
        "    if not np.isclose(w_sum, z):\n",
        "        if w_sum > 1e-9: w = w * (z / w_sum)\n",
        "        elif z > 0 :\n",
        "            w = np.zeros(n_features)\n",
        "            w[np.argmax(v_arr)] = z\n",
        "    return np.maximum(w, 0)\n",
        "\n",
        "# --- frank_wolfe_optimizer  (ActiveSet) ---\n",
        "def frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde, initial_w=None, max_outer_iter=250, fw_gap_tol=1e-7, inner_max_iter=350, tolerance=1e-9, psd_make_tolerance=1e-8, qp_regularization=1e-10, debug_print=False, force_iterations=0, return_history=False):\n",
        "    K, M = R_alpha.shape\n",
        "    if initial_w is None: w_k = np.ones(M) / M\n",
        "    else: w_k = project_to_simplex(np.copy(initial_w))\n",
        "    if w_k is None:\n",
        "        result = OptimizationResult(False, \"Initial projection failed\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "    fw_gap = float('inf'); best_w = np.copy(w_k); best_pi = None; best_lam = np.zeros(M); best_H = -float('inf'); final_gHk = np.zeros(M); best_active_set = None\n",
        "    pi0, ok, p1msg = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance)\n",
        "    if not ok:\n",
        "        result = OptimizationResult(False, f\"Phase 1 failed: {p1msg}\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "    current_w=np.copy(w_k); current_H=-float('inf'); current_pi=None; current_lam=None; current_active_set=None\n",
        "    inner_solver_args_for_loop = {'max_iter': inner_max_iter, 'tolerance': tolerance, 'regularization_epsilon': qp_regularization}\n",
        "    history = []\n",
        "\n",
        "    last_successful_pi = pi0\n",
        "    last_successful_lam = np.zeros(M)\n",
        "    last_successful_active_set = tuple()\n",
        "    last_successful_gHk = np.zeros(M)\n",
        "    last_successful_fw_gap = float('inf')\n",
        "\n",
        "    for k in range(max_outer_iter):\n",
        "        iter_data = {'k': k + 1}\n",
        "        if return_history: iter_data['w_k'] = np.copy(w_k)\n",
        "        try: Vk, Ex, ExxT = calculate_Vw(w_k, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "        except Exception as e:\n",
        "            result = OptimizationResult(False, f\"Outer iter {k+1}: Vw failed: {e}\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, active_set_opt=best_active_set)\n",
        "            if return_history: history.append(iter_data); return (result, history)\n",
        "            else: return result\n",
        "\n",
        "        # === Use Active Set for inner solve ===\n",
        "        pi_init_inner = last_successful_pi if last_successful_pi is not None else pi0\n",
        "        pk, lk, act_idx_k, _, Vw_reg_k, inner_ok, inner_msg = solve_inner_qp_active_set(\n",
        "            Vk, R_alpha, mu_tilde, pi_init_inner, **inner_solver_args_for_loop\n",
        "        )\n",
        "        # ===================================\n",
        "\n",
        "        if not inner_ok or pk is None:\n",
        "            warnings.warn(f\"Outer iter {k+1}: Inner QP failed: {inner_msg}. Using last successful state if available.\")\n",
        "            if last_successful_pi is None:\n",
        "                 result = OptimizationResult(False, f\"Outer iter {k+1}: Inner QP failed and no prior success: {inner_msg}\",\n",
        "                                             w_opt=current_w, iterations=k)\n",
        "                 if return_history: history.append(iter_data); return (result, history)\n",
        "                 else: return result\n",
        "            pk = last_successful_pi\n",
        "            lk = last_successful_lam\n",
        "            act_idx_k = list(last_successful_active_set)\n",
        "            final_gHk = last_successful_gHk\n",
        "            fw_gap = last_successful_fw_gap\n",
        "            try: Hk = pk.T @ Vk @ pk\n",
        "            except: Hk = current_H\n",
        "        else:\n",
        "            last_successful_pi = pk\n",
        "            last_successful_lam = lk if lk is not None else np.zeros(M)\n",
        "            last_successful_active_set = tuple(sorted(act_idx_k)) if act_idx_k is not None else tuple()\n",
        "            Hk = pk.T @ Vk @ pk\n",
        "            try:\n",
        "                gHk = calculate_H_gradient(pk, w_k, R_alpha, SecondMoments_alpha_array, Ex, ExxT, tolerance, debug_print=debug_print)\n",
        "            except Exception as e:\n",
        "                result = OptimizationResult(False, f\"Outer iter {k+1}: Grad failed: {e}\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "                if return_history: history.append(iter_data); return (result, history)\n",
        "                else: return result\n",
        "\n",
        "            final_gHk = gHk if gHk is not None else np.full(M, np.nan)\n",
        "            last_successful_gHk = final_gHk if not np.isnan(final_gHk).any() else last_successful_gHk\n",
        "\n",
        "        if return_history: iter_data['H_k'] = Hk\n",
        "        if return_history: iter_data['grad_H_k_norm'] = np.linalg.norm(final_gHk) if not np.isnan(final_gHk).any() else np.nan\n",
        "\n",
        "        current_w = np.copy(w_k); current_H = Hk; current_pi = pk; current_lam = last_successful_lam; current_active_set = last_successful_active_set\n",
        "\n",
        "        if Hk >= best_H - tolerance*1000:\n",
        "             best_H = Hk; best_w = np.copy(w_k); best_pi = np.copy(pk); best_lam = np.copy(current_lam); best_active_set = current_active_set\n",
        "\n",
        "        if final_gHk is None or np.isnan(final_gHk).any():\n",
        "            result = OptimizationResult(False, f\"Outer iter {k+1}: Grad NaN.\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "            if return_history: history.append(iter_data); return (result, history)\n",
        "            else: return result\n",
        "\n",
        "        grad_norm = np.linalg.norm(final_gHk)\n",
        "        if grad_norm < tolerance * 10: sk = w_k; sk_idx = -1\n",
        "        else: sk_idx = np.argmax(final_gHk); sk = np.zeros(M); sk[sk_idx] = 1.0\n",
        "        if return_history: iter_data['s_k_index'] = sk_idx\n",
        "\n",
        "        if w_k is None:\n",
        "             result = OptimizationResult(False, f\"k={k} w_k None\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             if return_history: history.append(iter_data); return (result, history)\n",
        "             else: return result\n",
        "\n",
        "        fw_gap = final_gHk.T @ (w_k - sk)\n",
        "        last_successful_fw_gap = fw_gap\n",
        "        if return_history: iter_data['fw_gap'] = fw_gap\n",
        "        gamma = 2.0 / (k + 3.0)\n",
        "        if return_history: iter_data['gamma_k'] = gamma\n",
        "        if return_history: history.append(iter_data)\n",
        "\n",
        "        converged = False\n",
        "        if k >= force_iterations and not np.isnan(fw_gap):\n",
        "            if abs(fw_gap) <= fw_gap_tol: converged = True; conv_msg = f\"Converged (Gap {abs(fw_gap):.2e})\"\n",
        "        if converged:\n",
        "            # --- Final Inner Solve for Consistency ---\n",
        "            final_w = current_w\n",
        "            final_pi_result = current_pi\n",
        "            final_lam_result = current_lam\n",
        "            final_active_set_result = current_active_set\n",
        "            final_H_result = current_H\n",
        "            final_grad_result = final_gHk\n",
        "            final_fw_gap_result = fw_gap\n",
        "\n",
        "            try:\n",
        "                final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "                pi0_final, ok_final, _ = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance)\n",
        "                if ok_final:\n",
        "                    pi_s_final, lam_s_final, act_idx_final, _, _, inner_ok_final, msg_final = solve_inner_qp_active_set(\n",
        "                        final_Vk, R_alpha, mu_tilde, pi0_final, # Use stable pi0 for final check\n",
        "                        **inner_solver_args_for_loop\n",
        "                    )\n",
        "                    if inner_ok_final and pi_s_final is not None and lam_s_final is not None:\n",
        "                        final_pi_result = pi_s_final\n",
        "                        final_lam_result = lam_s_final\n",
        "                        final_active_set_result = tuple(sorted(act_idx_final)) if act_idx_final is not None else tuple()\n",
        "                        final_grad_result = calculate_H_gradient(final_pi_result, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance, debug_print=False)\n",
        "                        if final_grad_result is None or np.isnan(final_grad_result).any(): final_grad_result = np.zeros(M)\n",
        "                        grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                        if grad_norm_final < tolerance * 10: sk_final = final_w\n",
        "                        else: sk_idx_final = np.argmax(final_grad_result); sk_final = np.zeros(M); sk_final[sk_idx_final] = 1.0\n",
        "                        final_fw_gap_result = final_grad_result.T @ (final_w - sk_final) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                    else:\n",
        "                        warnings.warn(f\"Warning: Final inner QP solve failed after convergence: {msg_final}. Using last iteration's values.\")\n",
        "                else:\n",
        "                     warnings.warn(f\"Warning: Could not find feasible pi0 for final solve after convergence. Using last iteration's values.\")\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Error during final inner solve after convergence: {e}. Using last iteration's values.\")\n",
        "            # --- End of Final Inner Solve ---\n",
        "\n",
        "            result = OptimizationResult(True, conv_msg, w_opt=final_w, pi_opt=final_pi_result, lambda_opt=final_lam_result,\n",
        "                                        H_opt=final_H_result, grad_H_opt=final_grad_result, iterations=k + 1,\n",
        "                                        fw_gap=final_fw_gap_result, active_set_opt=final_active_set_result)\n",
        "            return (result, history) if return_history else result\n",
        "\n",
        "        # --- Prepare for next iteration ---\n",
        "        if w_k is None or sk is None:\n",
        "             result = OptimizationResult(False, f\"k={k} w_k/sk None before update\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             return (result, history) if return_history else result\n",
        "        w_k_next = (1.0 - gamma) * w_k + gamma * sk; w_k = project_to_simplex(w_k_next)\n",
        "        if w_k is None:\n",
        "             result = OptimizationResult(False, f\"k={k} proj None\", w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, active_set_opt=best_active_set)\n",
        "             return (result, history) if return_history else result\n",
        "        pi0 = pk # Use last inner solution as initial guess\n",
        "\n",
        "    # Max iter reached, perform final solve for consistency\n",
        "    final_w = current_w\n",
        "    final_pi_result = current_pi\n",
        "    final_lam_result = current_lam\n",
        "    final_active_set_result = current_active_set\n",
        "    final_H_result = current_H\n",
        "    final_grad_result = last_successful_gHk\n",
        "    final_fw_gap_result = last_successful_fw_gap\n",
        "\n",
        "    try:\n",
        "        final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "        pi0_final, ok_final, _ = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance) # Use stable pi0\n",
        "        if ok_final:\n",
        "            pi_s_final, lam_s_final, act_idx_final, _, _, inner_ok_final, msg_final = solve_inner_qp_active_set(\n",
        "                final_Vk, R_alpha, mu_tilde, pi0_final, # <-- Use stable pi0_final\n",
        "                **inner_solver_args_for_loop\n",
        "            )\n",
        "            if inner_ok_final and pi_s_final is not None and lam_s_final is not None:\n",
        "                final_pi_result = pi_s_final\n",
        "                final_lam_result = lam_s_final\n",
        "                final_active_set_result = tuple(sorted(act_idx_final)) if act_idx_final is not None else tuple()\n",
        "                # Recalculate gradient and gap\n",
        "                final_grad_result = calculate_H_gradient(final_pi_result, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance, debug_print=False)\n",
        "                if final_grad_result is None or np.isnan(final_grad_result).any(): final_grad_result = np.zeros(M)\n",
        "                grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "                if grad_norm_final < tolerance * 10: sk_final = final_w\n",
        "                else: sk_idx_final = np.argmax(final_grad_result); sk_final = np.zeros(M); sk_final[sk_idx_final] = 1.0\n",
        "                final_fw_gap_result = final_grad_result.T @ (final_w - sk_final) if not np.isnan(final_grad_result).any() else np.nan\n",
        "            else:\n",
        "                warnings.warn(f\"Warning: Final inner QP solve failed after max iter: {msg_final}. Using last iteration's values.\")\n",
        "        else:\n",
        "             warnings.warn(f\"Warning: Could not find feasible pi0 for final solve after max iter. Using last iteration's values.\")\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error during final inner solve after max iter: {e}. Using last iteration's values.\")\n",
        "\n",
        "    if return_history:\n",
        "        grad_norm_final = np.linalg.norm(final_grad_result) if not np.isnan(final_grad_result).any() else np.nan\n",
        "        if grad_norm_final < tolerance * 10: sk_idx_final = -1\n",
        "        else: sk_idx_final = np.argmax(final_grad_result) if not np.isnan(final_grad_result).any() else -1\n",
        "        history.append({\n",
        "            'k': max_outer_iter + 1, 'w_k': np.copy(final_w), 'H_k': final_H_result,\n",
        "            'grad_H_k_norm': grad_norm_final, 's_k_index': sk_idx_final, 'fw_gap': final_fw_gap_result,\n",
        "            'gamma_k': np.nan\n",
        "        })\n",
        "\n",
        "    result = OptimizationResult(True, f\"Max Iter ({max_outer_iter})\", w_opt=final_w, pi_opt=final_pi_result, lambda_opt=final_lam_result,\n",
        "                                H_opt=final_H_result, grad_H_opt=final_grad_result, iterations=max_outer_iter,\n",
        "                                fw_gap=final_fw_gap_result, active_set_opt=final_active_set_result)\n",
        "    return (result, history) if return_history else result\n",
        "\n",
        "\n",
        "# --- generate_params_profile_switching_symmetric  ( + Vm) ---\n",
        "def generate_params_profile_switching_symmetric(alpha, alpha_max, K=5, M=3,\n",
        "                                      R_base_sym=np.array([0.02, 0.01, 0.0, -0.01, -0.02]),\n",
        "                                      sigma_base=np.array([0.18, 0.15, 0.20, 0.12, 0.10]),\n",
        "                                      Corr_base=np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]),\n",
        "                                      r_offset=0.03,\n",
        "                                      #  Vm  \n",
        "                                      s_factor_g = 0.001, # good/neutral vs base\n",
        "                                      s_factor_b = -0.001, # bad vs base\n",
        "                                      # \n",
        "                                      sigma_min_epsilon=1e-4, psd_tolerance=1e-9):\n",
        "    \"\"\" Generates symmetric parameters with slight differences in Vm \"\"\"\n",
        "    assert K == len(R_base_sym) and K == len(sigma_base) and K == Corr_base.shape[0] and M == 3, \"Dimension mismatch\"\n",
        "\n",
        "    R_neutral = R_base_sym\n",
        "    R_good = R_base_sym + r_offset\n",
        "    R_bad = R_base_sym - r_offset\n",
        "\n",
        "    # Base covariance\n",
        "    sigma_neutral = np.maximum(sigma_min_epsilon, sigma_base)\n",
        "    Corr_neutral = make_psd(Corr_base, psd_tolerance)\n",
        "    V_base = np.diag(sigma_neutral) @ Corr_neutral @ np.diag(sigma_neutral)\n",
        "\n",
        "    # Covariances with slight differences\n",
        "    sigma_good_adj = np.maximum(sigma_min_epsilon, sigma_base * (1 + s_factor_g))\n",
        "    V_good = np.diag(sigma_good_adj) @ Corr_neutral @ np.diag(sigma_good_adj) # Assume same Corr for simplicity\n",
        "\n",
        "    sigma_bad_adj = np.maximum(sigma_min_epsilon, sigma_base * (1 + s_factor_b))\n",
        "    V_bad = np.diag(sigma_bad_adj) @ Corr_neutral @ np.diag(sigma_bad_adj) # Assume same Corr for simplicity\n",
        "\n",
        "    V_neutral = V_base # V^N = V_base\n",
        "\n",
        "    # Calculate Sigma^m = V^m + r^m * r^m.T\n",
        "    Sigma_neutral = V_neutral + np.outer(R_neutral, R_neutral)\n",
        "    Sigma_good = V_good + np.outer(R_good, R_good)\n",
        "    Sigma_bad = V_bad + np.outer(R_bad, R_bad)\n",
        "\n",
        "    # Apply profile switching based on alpha\n",
        "    beta = np.clip(alpha / alpha_max if alpha_max > 0 else (1.0 if alpha > 0 else 0.0), 0.0, 1.0)\n",
        "\n",
        "    R_alpha = np.zeros((K, M))\n",
        "    SecondMoments_a_array = np.zeros((M, K, K))\n",
        "\n",
        "    # Scenario 0: Mix Good and Neutral\n",
        "    R_alpha[:, 0] = (1 - beta) * R_good + beta * R_neutral\n",
        "    SecondMoments_a_array[0, :, :] = (1 - beta) * Sigma_good + beta * Sigma_neutral\n",
        "\n",
        "    # Scenario 1: Mix Bad and Good\n",
        "    R_alpha[:, 1] = (1 - beta) * R_bad + beta * R_good\n",
        "    SecondMoments_a_array[1, :, :] = (1 - beta) * Sigma_bad + beta * Sigma_good\n",
        "\n",
        "    # Scenario 2: Mix Neutral and Bad\n",
        "    R_alpha[:, 2] = (1 - beta) * R_neutral + beta * R_bad\n",
        "    SecondMoments_a_array[2, :, :] = (1 - beta) * Sigma_neutral + beta * Sigma_bad\n",
        "\n",
        "    return R_alpha, SecondMoments_a_array\n",
        "\n",
        "\n",
        "# ===  (ActiveSet) ===\n",
        "def analyze_alpha_full_results(alpha_range, param_gen_kwargs, optimizer_kwargs, mu_tilde):\n",
        "    \"\"\"  alpha  FWH*pi*grad H*lambda*active_set*  \"\"\"\n",
        "    results_over_alpha = []\n",
        "    K = param_gen_kwargs.get('K', 5); M = param_gen_kwargs.get('M', 3)\n",
        "\n",
        "    print(\"\\n--- Starting Full Results Analysis over Alpha Range (using Active Set Method) ---\")\n",
        "    total_alphas = len(alpha_range)\n",
        "    start_loop_time = time.time()\n",
        "\n",
        "    for idx, alpha in enumerate(alpha_range):\n",
        "        loop_start_time = time.time()\n",
        "        print(f\"\\rAnalyzing alpha = {alpha:.6f} ({idx+1}/{total_alphas}) ... \", end=\"\")\n",
        "\n",
        "        # ===  ===\n",
        "        #  () \n",
        "        R_alpha, SecondMoments_alpha_array = generate_params_profile_switching_symmetric(alpha, **param_gen_kwargs)\n",
        "        alpha_result = {'alpha': alpha}\n",
        "\n",
        "        w_fw_default = np.full(M, np.nan)\n",
        "        H_star_fw = np.nan\n",
        "        pi_opt = np.full(K, np.nan)\n",
        "        grad_H_opt = np.full(M, np.nan)\n",
        "        lambda_opt = np.full(M, np.nan)\n",
        "        active_set_opt = tuple()\n",
        "\n",
        "        try:\n",
        "            # --- FW Optimizer (Active Set) ---\n",
        "            fw_result = frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde,\n",
        "                                              initial_w=None, return_history=False,\n",
        "                                              debug_print=False, # OFF\n",
        "                                              **optimizer_kwargs)\n",
        "            if fw_result.success:\n",
        "                 w_fw_default = fw_result.w_opt if fw_result.w_opt is not None else np.full(M, np.nan)\n",
        "                 H_star_fw = fw_result.H_opt if fw_result.H_opt is not None else np.nan\n",
        "                 pi_opt = fw_result.pi_opt if fw_result.pi_opt is not None else np.full(K, np.nan)\n",
        "                 grad_H_opt = fw_result.grad_H_opt if fw_result.grad_H_opt is not None else np.full(M, np.nan)\n",
        "                 lambda_opt = fw_result.lambda_opt if fw_result.lambda_opt is not None else np.full(M, np.nan)\n",
        "                 active_set_opt = fw_result.active_set_opt if fw_result.active_set_opt is not None else tuple()\n",
        "            else:\n",
        "                 w_fw_default = np.full(M, np.nan)\n",
        "                 H_star_fw = np.nan\n",
        "                 pi_opt = np.full(K, np.nan)\n",
        "                 grad_H_opt = np.full(M, np.nan)\n",
        "                 lambda_opt = np.full(M, np.nan)\n",
        "                 active_set_opt = tuple()\n",
        "                 print(f\"\\n  Warn: FW failed for alpha={alpha:.6f}: {fw_result.message}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n  Error during FW run for alpha={alpha:.6f}: {e}\")\n",
        "            w_fw_default = np.full(M, np.nan)\n",
        "            H_star_fw = np.nan\n",
        "            pi_opt = np.full(K, np.nan)\n",
        "            grad_H_opt = np.full(M, np.nan)\n",
        "            lambda_opt = np.full(M, np.nan)\n",
        "            active_set_opt = tuple()\n",
        "\n",
        "        alpha_result['w_fw_default'] = w_fw_default\n",
        "        alpha_result['H_star'] = H_star_fw\n",
        "        alpha_result['pi_opt'] = pi_opt\n",
        "        alpha_result['grad_H_opt'] = grad_H_opt\n",
        "        alpha_result['lambda_opt'] = lambda_opt\n",
        "        alpha_result['active_set_opt'] = active_set_opt\n",
        "\n",
        "        results_over_alpha.append(alpha_result)\n",
        "\n",
        "    print(\"\\n--- Finished Full Results Analysis ---\")\n",
        "    df_results = pd.DataFrame(results_over_alpha)\n",
        "    if PANDAS_AVAILABLE:\n",
        "        fw_vecs = np.stack([res.get('w_fw_default', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        pi_vecs = np.stack([res.get('pi_opt', np.full(K, np.nan)) for res in results_over_alpha])\n",
        "        grad_vecs = np.stack([res.get('grad_H_opt', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        lambda_vecs = np.stack([res.get('lambda_opt', np.full(M, np.nan)) for res in results_over_alpha])\n",
        "        active_sets = [str(res.get('active_set_opt', tuple())) for res in results_over_alpha]\n",
        "\n",
        "        for m in range(M):\n",
        "            df_results[f'w_fw_{m}'] = fw_vecs[:, m]\n",
        "            df_results[f'grad_H_{m}'] = grad_vecs[:, m]\n",
        "            df_results[f'lambda_{m}'] = lambda_vecs[:, m]\n",
        "        for k in range(K):\n",
        "            df_results[f'pi_{k}'] = pi_vecs[:, k]\n",
        "\n",
        "        df_results['active_set'] = active_sets\n",
        "\n",
        "        df_results = df_results.drop(columns=['w_fw_default', 'pi_opt', 'grad_H_opt', 'lambda_opt', 'active_set_opt'], errors='ignore')\n",
        "\n",
        "    return df_results if PANDAS_AVAILABLE else results_over_alpha\n",
        "\n",
        "\n",
        "# ===  ===\n",
        "if __name__ == '__main__':\n",
        "    start_time_main = time.time()\n",
        "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "    warnings.filterwarnings('ignore', category=UserWarning)\n",
        "    warnings.filterwarnings('ignore', category=OptimizeWarning)\n",
        "\n",
        "    # ---  ---\n",
        "    K = 5; M = 3;\n",
        "    #  mu_tilde  0.01  \n",
        "    mu_tilde = 0.01\n",
        "    alpha_max = 1.5\n",
        "\n",
        "    #   \n",
        "    param_gen_kwargs_symmetric_mild = {\n",
        "        'K': K, 'M': M, 'alpha_max': alpha_max,\n",
        "        'R_base_sym': np.array([0.02, 0.01, 0.0, -0.01, -0.02]), # Neutral=0\n",
        "        'sigma_base': np.array([0.18, 0.15, 0.20, 0.12, 0.10]),\n",
        "        'Corr_base': np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]),\n",
        "        'r_offset': 0.03,\n",
        "        's_factor_g': 0.001,  # Vm \n",
        "        's_factor_b': -0.001, # Vm \n",
        "        'sigma_min_epsilon': 1e-4,\n",
        "        'psd_tolerance': 1e-9\n",
        "    }\n",
        "\n",
        "    solver_settings = { # \n",
        "        'max_outer_iter': 500, 'fw_gap_tol': 1e-9,\n",
        "        'inner_max_iter': 600,\n",
        "        'tolerance': 1e-11,\n",
        "        'psd_make_tolerance': 1e-9,\n",
        "        'qp_regularization': 1e-10, # \n",
        "        'force_iterations': 20\n",
        "    }\n",
        "    #   alpha  (0.75 ) \n",
        "    alpha_start = 0.700\n",
        "    alpha_end = 0.800\n",
        "    alpha_step = 0.001\n",
        "    num_alpha_steps = int(round((alpha_end - alpha_start) / alpha_step)) + 1\n",
        "    alpha_range_analyze = np.linspace(alpha_start, alpha_end, num_alpha_steps)\n",
        "    unique_pt_tolerance = 1e-5 # KKT\n",
        "\n",
        "    output_csv_filename = f\"alpha_internal_sol_search_{alpha_start:.3f}_{alpha_end:.3f}_activeset_mu001_quasi_sym.csv\" # \n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"--- Searching for Internal Solution around alpha=0.75 [{alpha_start:.3f}, {alpha_end:.3f}] ---\")\n",
        "    print(f\"--- (Using ActiveSet, mu_tilde={mu_tilde}, Quasi-Symmetric Params) ---\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # ---  ---\n",
        "    results_df = analyze_alpha_full_results(\n",
        "        alpha_range=alpha_range_analyze,\n",
        "        param_gen_kwargs=param_gen_kwargs_symmetric_mild, # <-- ()\n",
        "        optimizer_kwargs=solver_settings,\n",
        "        mu_tilde=mu_tilde\n",
        "    )\n",
        "\n",
        "    # ---  & CSV & KKT ---\n",
        "    if PANDAS_AVAILABLE and isinstance(results_df, pd.DataFrame):\n",
        "        print(\"\\n--- Analysis Results Summary (DataFrame) ---\")\n",
        "        float_format_func = lambda x: f\"{x:.4e}\" if pd.notna(x) and isinstance(x, (float, np.number)) else x\n",
        "        pd.options.display.float_format = float_format_func\n",
        "        cols_to_show = ['alpha', 'H_star', 'active_set'] + \\\n",
        "                       [f'w_fw_{m}' for m in range(M) if f'w_fw_{m}' in results_df.columns] + \\\n",
        "                       [f'pi_{k}' for k in range(K) if f'pi_{k}' in results_df.columns] + \\\n",
        "                       [f'grad_H_{m}' for m in range(M) if f'grad_H_{m}' in results_df.columns] + \\\n",
        "                       [f'lambda_{m}' for m in range(M) if f'lambda_{m}' in results_df.columns]\n",
        "        cols_to_show = [col for col in cols_to_show if col in results_df.columns]\n",
        "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 200): # \n",
        "             print(results_df[cols_to_show].to_string(index=False, na_rep='NaN'))\n",
        "        pd.reset_option('display.float_format')\n",
        "\n",
        "        # CSV ()\n",
        "        try:\n",
        "            df_to_save = results_df.copy()\n",
        "            cols_order = ['alpha', 'H_star', 'active_set'] + \\\n",
        "                         [f'w_fw_{m}' for m in range(M) if f'w_fw_{m}' in df_to_save.columns] + \\\n",
        "                         [f'pi_{k}' for k in range(K) if f'pi_{k}' in df_to_save.columns] + \\\n",
        "                         [f'grad_H_{m}' for m in range(M) if f'grad_H_{m}' in df_to_save.columns] + \\\n",
        "                         [f'lambda_{m}' for m in range(M) if f'lambda_{m}' in df_to_save.columns]\n",
        "            cols_order = [col for col in cols_order if col in df_to_save.columns]\n",
        "            df_to_save = df_to_save[cols_order]\n",
        "\n",
        "            df_to_save.to_csv(output_csv_filename, index=False, float_format='%.8e')\n",
        "            print(f\"\\nFull detailed results ({len(df_to_save)} points) saved to: {os.path.abspath(output_csv_filename)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError saving results to CSV: {e}\")\n",
        "\n",
        "        # KKT ()\n",
        "        print(\"\\n--- Quick KKT Check (Stationarity w.r.t. w) ---\")\n",
        "        kkt_violations = []\n",
        "        # unique_pt_tolerance  main \n",
        "        for index, row in results_df.iterrows():\n",
        "            alpha = row['alpha']\n",
        "            w_star = np.array([row.get(f'w_fw_{m}', np.nan) for m in range(M)])\n",
        "            grad_h = np.array([row.get(f'grad_H_{m}', np.nan) for m in range(M)])\n",
        "            active_set_str = row.get('active_set', '()')\n",
        "\n",
        "            if np.isnan(w_star).any() or np.isnan(grad_h).any() or np.isclose(np.sum(w_star), 0):\n",
        "                kkt_violations.append({\n",
        "                    'alpha': alpha,\n",
        "                    'active_set_report': active_set_str,\n",
        "                    'active_set_kkt': 'NaN',\n",
        "                    'max_violation': np.nan,\n",
        "                    'grad_consistency_violation': np.nan,\n",
        "                    'check_result': 'Skipped (NaN or zero w)'\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            check_tol = solver_settings['tolerance'] * 1e3\n",
        "\n",
        "            max_grad_h = np.max(grad_h)\n",
        "            nu_estimate = max_grad_h\n",
        "\n",
        "            stationarity_violation = 0.0\n",
        "            active_indices_kkt = set()\n",
        "            for m in range(M):\n",
        "                if w_star[m] > unique_pt_tolerance:\n",
        "                    active_indices_kkt.add(m)\n",
        "\n",
        "            active_grads = grad_h[list(active_indices_kkt)] if active_indices_kkt else []\n",
        "            grad_consistency_violation = 0.0\n",
        "            if len(active_grads) > 1:\n",
        "                grad_consistency_violation = np.max(active_grads) - np.min(active_grads)\n",
        "                if not np.isclose(grad_consistency_violation, 0.0, atol=check_tol):\n",
        "                     stationarity_violation = max(stationarity_violation, grad_consistency_violation)\n",
        "\n",
        "            for m in range(M):\n",
        "                if m not in active_indices_kkt:\n",
        "                    nu_for_check = np.min(active_grads) if active_grads else max_grad_h\n",
        "                    eta_m_estimate = nu_for_check - grad_h[m]\n",
        "                    if eta_m_estimate < -check_tol:\n",
        "                        stationarity_violation = max(stationarity_violation, abs(eta_m_estimate))\n",
        "\n",
        "            kkt_violations.append({\n",
        "                'alpha': alpha,\n",
        "                'active_set_report': active_set_str,\n",
        "                'active_set_kkt': str(tuple(sorted(active_indices_kkt))),\n",
        "                'max_violation': stationarity_violation,\n",
        "                'grad_consistency_violation': grad_consistency_violation,\n",
        "                'check_result': 'OK' if stationarity_violation < check_tol * 10 else 'WARN'\n",
        "            })\n",
        "\n",
        "        if PANDAS_AVAILABLE and kkt_violations:\n",
        "            df_kkt = pd.DataFrame(kkt_violations)\n",
        "            kkt_cols_order = ['alpha', 'active_set_report','active_set_kkt', 'max_violation', 'grad_consistency_violation', 'check_result']\n",
        "            kkt_cols_order = [col for col in kkt_cols_order if col in df_kkt.columns]\n",
        "            with pd.option_context('display.float_format', '{:.4e}'.format, 'display.max_rows', None): # \n",
        "                print(df_kkt[kkt_cols_order].to_string(index=False, na_rep='NaN'))\n",
        "            print(\"Notes on KKT Check:\")\n",
        "            print(\" - active_set_report: Active set reported by inner solver\")\n",
        "            print(\" - active_set_kkt: Active set inferred from w* > tol\")\n",
        "            print(\" - max_violation: Max KKT violation (dual infeasibility or comp. slackness based on eta estimate)\")\n",
        "            print(\" - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\")\n",
        "        else:\n",
        "            print(\"Could not perform KKT check or pandas not available.\")\n",
        "\n",
        "    elif isinstance(results_df, list):\n",
        "        print(\"\\n--- Analysis Results Summary (List) ---\")\n",
        "        # ()\n",
        "        print(\"\\n(Pandas not available, skipping CSV export)\")\n",
        "\n",
        "    end_time_main = time.time()\n",
        "    print(f\"\\nTotal execution time: {end_time_main - start_time_main:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfehjyaQ0WFz",
        "outputId": "53ed3083-99fa-48ab-990a-d6b499286a5e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------\n",
            "--- Searching for Internal Solution around alpha=0.75 [0.700, 0.800] ---\n",
            "--- (Using ActiveSet, mu_tilde=0.01, Quasi-Symmetric Params) ---\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Starting Full Results Analysis over Alpha Range (using Active Set Method) ---\n",
            "Analyzing alpha = 0.800000 (101/101) ... \n",
            "--- Finished Full Results Analysis ---\n",
            "\n",
            "--- Analysis Results Summary (DataFrame) ---\n",
            "     alpha     H_star active_set     w_fw_0     w_fw_1     w_fw_2       pi_0       pi_1       pi_2       pi_3        pi_4   grad_H_0   grad_H_1   grad_H_2   lambda_0   lambda_1   lambda_2\n",
            "7.0000e-01 1.9786e-03     (0, 1) 9.9969e-01 1.5540e-04 1.5540e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8762e-03 1.8746e-03 2.4651e-01 1.4921e-01 0.0000e+00\n",
            "7.0100e-01 1.9786e-03     (0, 2) 9.9969e-01 1.5540e-04 1.5540e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8762e-03 1.8746e-03 3.0646e-01 0.0000e+00 8.9260e-02\n",
            "7.0200e-01 1.9786e-03     (0, 2) 9.9969e-01 1.5540e-04 1.5540e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8762e-03 1.8746e-03 3.0672e-01 0.0000e+00 8.8996e-02\n",
            "7.0300e-01 1.9786e-03     (0, 2) 9.9969e-01 1.5540e-04 1.5540e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8762e-03 1.8746e-03 3.0698e-01 0.0000e+00 8.8732e-02\n",
            "7.0400e-01 1.9786e-03     (0, 2) 9.9969e-01 1.5540e-04 1.5540e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8762e-03 1.8746e-03 3.0725e-01 0.0000e+00 8.8468e-02\n",
            "7.0500e-01 1.9786e-03     (0, 2) 9.9969e-01 1.5540e-04 1.5540e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8762e-03 1.8746e-03 3.0751e-01 0.0000e+00 8.8204e-02\n",
            "7.0600e-01 1.9786e-03     (0, 2) 9.9969e-01 1.5540e-04 1.5540e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8762e-03 1.8746e-03 3.0777e-01 0.0000e+00 8.7941e-02\n",
            "7.0700e-01 1.9786e-03     (0, 2) 9.9969e-01 1.5540e-04 1.5540e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8763e-03 1.8746e-03 3.0804e-01 0.0000e+00 8.7677e-02\n",
            "7.0800e-01 1.9786e-03     (0, 2) 9.9969e-01 1.5540e-04 1.5540e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8763e-03 1.8746e-03 3.0830e-01 0.0000e+00 8.7413e-02\n",
            "7.0900e-01 1.9786e-03     (0, 2) 9.9969e-01 1.5540e-04 1.5540e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8763e-03 1.8746e-03 3.0856e-01 0.0000e+00 8.7149e-02\n",
            "7.1000e-01 1.9786e-03     (0, 2) 9.9969e-01 1.5540e-04 1.5540e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8763e-03 1.8746e-03 3.0883e-01 0.0000e+00 8.6885e-02\n",
            "7.1100e-01 1.9786e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8763e-03 1.8746e-03 3.0909e-01 0.0000e+00 8.6621e-02\n",
            "7.1200e-01 1.9786e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8763e-03 1.8746e-03 3.0935e-01 0.0000e+00 8.6357e-02\n",
            "7.1300e-01 1.9786e-03     (0, 1) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8786e-03 1.8763e-03 1.8746e-03 2.4572e-01 1.4999e-01 0.0000e+00\n",
            "7.1400e-01 1.9785e-03     (0, 1) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 2.4566e-01 1.5005e-01 0.0000e+00\n",
            "7.1500e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 3.1014e-01 0.0000e+00 8.5565e-02\n",
            "7.1600e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 3.1041e-01 0.0000e+00 8.5301e-02\n",
            "7.1700e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 3.1067e-01 0.0000e+00 8.5037e-02\n",
            "7.1800e-01 1.9785e-03     (0, 1) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 2.4540e-01 1.5031e-01 0.0000e+00\n",
            "7.1900e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 3.1120e-01 0.0000e+00 8.4510e-02\n",
            "7.2000e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 3.1146e-01 0.0000e+00 8.4246e-02\n",
            "7.2100e-01 1.9785e-03     (0, 1) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 2.4520e-01 1.5050e-01 0.0000e+00\n",
            "7.2200e-01 1.9785e-03     (0, 1) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 2.4513e-01 1.5057e-01 0.0000e+00\n",
            "7.2300e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 3.1225e-01 0.0000e+00 8.3454e-02\n",
            "7.2400e-01 1.9785e-03     (0, 1) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 2.4500e-01 1.5071e-01 0.0000e+00\n",
            "7.2500e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8763e-03 1.8746e-03 3.1278e-01 0.0000e+00 8.2926e-02\n",
            "7.2600e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8746e-03 3.1304e-01 0.0000e+00 8.2662e-02\n",
            "7.2700e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8746e-03 3.1330e-01 0.0000e+00 8.2398e-02\n",
            "7.2800e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8746e-03 3.1357e-01 0.0000e+00 8.2134e-02\n",
            "7.2900e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8746e-03 3.1383e-01 0.0000e+00 8.1870e-02\n",
            "7.3000e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8746e-03 3.1409e-01 0.0000e+00 8.1607e-02\n",
            "7.3100e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8746e-03 3.1436e-01 0.0000e+00 8.1343e-02\n",
            "7.3200e-01 1.9785e-03     (0, 1) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 2.4443e-01 1.5127e-01 0.0000e+00\n",
            "7.3300e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 3.1488e-01 0.0000e+00 8.0815e-02\n",
            "7.3400e-01 1.9785e-03     (0, 2) 9.9968e-01 1.6026e-04 1.6026e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 3.1515e-01 0.0000e+00 8.0551e-02\n",
            "7.3500e-01 1.9785e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 3.1541e-01 0.0000e+00 8.0287e-02\n",
            "7.3600e-01 1.9785e-03     (0, 1) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 2.4414e-01 1.5156e-01 0.0000e+00\n",
            "7.3700e-01 1.9785e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 3.1594e-01 0.0000e+00 7.9759e-02\n",
            "7.3800e-01 1.9785e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 3.1620e-01 0.0000e+00 7.9495e-02\n",
            "7.3900e-01 1.9785e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 3.1646e-01 0.0000e+00 7.9231e-02\n",
            "7.4000e-01 1.9785e-03     (0, 1) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 2.4384e-01 1.5186e-01 0.0000e+00\n",
            "7.4100e-01 1.9785e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 3.1699e-01 0.0000e+00 7.8704e-02\n",
            "7.4200e-01 1.9785e-03     (0, 1) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 2.4368e-01 1.5201e-01 0.0000e+00\n",
            "7.4300e-01 1.9785e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 3.1752e-01 0.0000e+00 7.8176e-02\n",
            "7.4400e-01 1.9785e-03     (0, 1) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8764e-03 1.8745e-03 2.4352e-01 1.5217e-01 0.0000e+00\n",
            "7.4500e-01 1.9785e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8765e-03 1.8745e-03 3.1805e-01 0.0000e+00 7.7648e-02\n",
            "7.4600e-01 1.9785e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8765e-03 1.8745e-03 3.1831e-01 0.0000e+00 7.7384e-02\n",
            "7.4700e-01 1.9785e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8765e-03 1.8745e-03 3.1857e-01 0.0000e+00 7.7120e-02\n",
            "7.4800e-01 1.9785e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8765e-03 1.8745e-03 3.1884e-01 0.0000e+00 7.6856e-02\n",
            "7.4900e-01 1.9785e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8765e-03 1.8745e-03 3.1910e-01 0.0000e+00 7.6592e-02\n",
            "7.5000e-01 1.9785e-03     (0, 1) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8765e-03 1.8745e-03 2.4303e-01 1.5266e-01 0.0000e+00\n",
            "7.5100e-01 1.9785e-03     (0, 1) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8785e-03 1.8765e-03 1.8745e-03 2.4295e-01 1.5274e-01 0.0000e+00\n",
            "7.5200e-01 1.9784e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 3.1989e-01 0.0000e+00 7.5801e-02\n",
            "7.5300e-01 1.9784e-03     (0, 1) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 2.4278e-01 1.5291e-01 0.0000e+00\n",
            "7.5400e-01 1.9784e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 3.2042e-01 0.0000e+00 7.5273e-02\n",
            "7.5500e-01 1.9784e-03     (0, 1) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 2.4261e-01 1.5308e-01 0.0000e+00\n",
            "7.5600e-01 1.9784e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 3.2094e-01 0.0000e+00 7.4745e-02\n",
            "7.5700e-01 1.9784e-03     (0, 1) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 2.4243e-01 1.5325e-01 0.0000e+00\n",
            "7.5800e-01 1.9784e-03     (0, 2) 9.9967e-01 1.6534e-04 1.6534e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 3.2147e-01 0.0000e+00 7.4217e-02\n",
            "7.5900e-01 1.9784e-03     (0, 2) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 3.2173e-01 0.0000e+00 7.3953e-02\n",
            "7.6000e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 2.4217e-01 1.5352e-01 0.0000e+00\n",
            "7.6100e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 2.4207e-01 1.5361e-01 0.0000e+00\n",
            "7.6200e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 2.4198e-01 1.5370e-01 0.0000e+00\n",
            "7.6300e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8765e-03 1.8745e-03 2.4189e-01 1.5379e-01 0.0000e+00\n",
            "7.6400e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8745e-03 2.4180e-01 1.5389e-01 0.0000e+00\n",
            "7.6500e-01 1.9784e-03     (0, 2) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8745e-03 3.2331e-01 0.0000e+00 7.2370e-02\n",
            "7.6600e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8745e-03 2.4161e-01 1.5407e-01 0.0000e+00\n",
            "7.6700e-01 1.9784e-03     (0, 2) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8745e-03 3.2384e-01 0.0000e+00 7.1842e-02\n",
            "7.6800e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8745e-03 2.4142e-01 1.5426e-01 0.0000e+00\n",
            "7.6900e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8745e-03 2.4132e-01 1.5436e-01 0.0000e+00\n",
            "7.7000e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 2.4122e-01 1.5446e-01 0.0000e+00\n",
            "7.7100e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 2.4112e-01 1.5456e-01 0.0000e+00\n",
            "7.7200e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 2.4102e-01 1.5466e-01 0.0000e+00\n",
            "7.7300e-01 1.9784e-03     (0, 2) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 3.2542e-01 0.0000e+00 7.0259e-02\n",
            "7.7400e-01 1.9784e-03     (0, 2) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 3.2568e-01 0.0000e+00 6.9995e-02\n",
            "7.7500e-01 1.9784e-03     (0, 2) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 3.2595e-01 0.0000e+00 6.9731e-02\n",
            "7.7600e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 2.4062e-01 1.5506e-01 0.0000e+00\n",
            "7.7700e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 2.4051e-01 1.5516e-01 0.0000e+00\n",
            "7.7800e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 2.4041e-01 1.5527e-01 0.0000e+00\n",
            "7.7900e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 2.4030e-01 1.5538e-01 0.0000e+00\n",
            "7.8000e-01 1.9784e-03     (0, 2) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 3.2726e-01 0.0000e+00 6.8412e-02\n",
            "7.8100e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 2.4009e-01 1.5559e-01 0.0000e+00\n",
            "7.8200e-01 1.9784e-03     (0, 1) 9.9966e-01 1.7068e-04 1.7068e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8766e-03 1.8744e-03 2.3998e-01 1.5570e-01 0.0000e+00\n",
            "7.8300e-01 1.9784e-03     (0, 1) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8767e-03 1.8744e-03 2.3987e-01 1.5581e-01 0.0000e+00\n",
            "7.8400e-01 1.9784e-03     (0, 2) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8767e-03 1.8744e-03 3.2832e-01 0.0000e+00 6.7356e-02\n",
            "7.8500e-01 1.9784e-03     (0, 1) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8767e-03 1.8744e-03 2.3964e-01 1.5603e-01 0.0000e+00\n",
            "7.8600e-01 1.9784e-03     (0, 1) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8767e-03 1.8744e-03 2.3953e-01 1.5614e-01 0.0000e+00\n",
            "7.8700e-01 1.9784e-03     (0, 1) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8767e-03 1.8744e-03 2.3942e-01 1.5626e-01 0.0000e+00\n",
            "7.8800e-01 1.9784e-03     (0, 2) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8767e-03 1.8744e-03 3.2937e-01 0.0000e+00 6.6301e-02\n",
            "7.8900e-01 1.9783e-03     (0, 2) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8784e-03 1.8767e-03 1.8744e-03 3.2963e-01 0.0000e+00 6.6037e-02\n",
            "7.9000e-01 1.9783e-03     (0, 1) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8783e-03 1.8767e-03 1.8744e-03 2.3907e-01 1.5660e-01 0.0000e+00\n",
            "7.9100e-01 1.9783e-03     (0, 1) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8783e-03 1.8767e-03 1.8744e-03 2.3895e-01 1.5672e-01 0.0000e+00\n",
            "7.9200e-01 1.9783e-03     (0, 1) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8783e-03 1.8767e-03 1.8744e-03 2.3883e-01 1.5684e-01 0.0000e+00\n",
            "7.9300e-01 1.9783e-03     (0, 1) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8783e-03 1.8767e-03 1.8744e-03 2.3871e-01 1.5696e-01 0.0000e+00\n",
            "7.9400e-01 1.9783e-03     (0, 1) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8783e-03 1.8767e-03 1.8744e-03 2.3859e-01 1.5708e-01 0.0000e+00\n",
            "7.9500e-01 1.9783e-03     (0, 2) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8783e-03 1.8767e-03 1.8744e-03 3.3121e-01 0.0000e+00 6.4454e-02\n",
            "7.9600e-01 1.9783e-03     (0, 2) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8783e-03 1.8767e-03 1.8744e-03 3.3148e-01 0.0000e+00 6.4190e-02\n",
            "7.9700e-01 1.9783e-03     (0, 1) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8783e-03 1.8767e-03 1.8744e-03 2.3821e-01 1.5745e-01 0.0000e+00\n",
            "7.9800e-01 1.9783e-03     (0, 1) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8783e-03 1.8767e-03 1.8744e-03 2.3809e-01 1.5758e-01 0.0000e+00\n",
            "7.9900e-01 1.9783e-03     (0, 2) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8783e-03 1.8767e-03 1.8744e-03 3.3227e-01 0.0000e+00 6.3398e-02\n",
            "8.0000e-01 1.9783e-03     (0, 2) 9.9965e-01 1.7627e-04 1.7627e-04 1.4671e-01 1.2588e-01 1.5709e-02 4.1068e-03 -2.9241e-01 1.8783e-03 1.8767e-03 1.8744e-03 3.3253e-01 0.0000e+00 6.3135e-02\n",
            "\n",
            "Full detailed results (101 points) saved to: /content/alpha_internal_sol_search_0.700_0.800_activeset_mu001_quasi_sym.csv\n",
            "\n",
            "--- Quick KKT Check (Stationarity w.r.t. w) ---\n",
            "     alpha active_set_report active_set_kkt  max_violation  grad_consistency_violation check_result\n",
            "7.0000e-01            (0, 1)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.0100e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.0200e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.0300e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.0400e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.0500e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.0600e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.0700e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.0800e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.0900e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.1000e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.1100e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.1200e-01            (0, 2)      (0, 1, 2)     3.9531e-06                  3.9531e-06         WARN\n",
            "7.1300e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.1400e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.1500e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.1600e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.1700e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.1800e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.1900e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.2000e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.2100e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.2200e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.2300e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.2400e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.2500e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.2600e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.2700e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.2800e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.2900e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.3000e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.3100e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.3200e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.3300e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.3400e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.3500e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.3600e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.3700e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.3800e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.3900e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.4000e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.4100e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.4200e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.4300e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.4400e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.4500e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.4600e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.4700e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.4800e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.4900e-01            (0, 2)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.5000e-01            (0, 1)      (0, 1, 2)     3.9530e-06                  3.9530e-06         WARN\n",
            "7.5100e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.5200e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.5300e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.5400e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.5500e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.5600e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.5700e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.5800e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.5900e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.6000e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.6100e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.6200e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.6300e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.6400e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.6500e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.6600e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.6700e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.6800e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.6900e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.7000e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.7100e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.7200e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.7300e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.7400e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.7500e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.7600e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.7700e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.7800e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.7900e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.8000e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.8100e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.8200e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.8300e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.8400e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.8500e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.8600e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.8700e-01            (0, 1)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.8800e-01            (0, 2)      (0, 1, 2)     3.9529e-06                  3.9529e-06         WARN\n",
            "7.8900e-01            (0, 2)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "7.9000e-01            (0, 1)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "7.9100e-01            (0, 1)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "7.9200e-01            (0, 1)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "7.9300e-01            (0, 1)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "7.9400e-01            (0, 1)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "7.9500e-01            (0, 2)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "7.9600e-01            (0, 2)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "7.9700e-01            (0, 1)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "7.9800e-01            (0, 1)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "7.9900e-01            (0, 2)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "8.0000e-01            (0, 2)      (0, 1, 2)     3.9528e-06                  3.9528e-06         WARN\n",
            "Notes on KKT Check:\n",
            " - active_set_report: Active set reported by inner solver\n",
            " - active_set_kkt: Active set inferred from w* > tol\n",
            " - max_violation: Max KKT violation (dual infeasibility or comp. slackness based on eta estimate)\n",
            " - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\n",
            "\n",
            "Total execution time: 11.38 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "5_1_find_internal_solution_mu025_quasi_symmetric.py\n",
        "\n",
        "Attempts to find an internal solution w* by using quasi-symmetric parameters\n",
        "(slight difference in Vm) around alpha=0.75, testing mu_tilde = 0.025.\n",
        "Uses the Active Set method for the inner QP solve.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from scipy.linalg import solve, LinAlgError, eigh, inv, qr\n",
        "from scipy.optimize import linprog, OptimizeWarning\n",
        "import traceback\n",
        "import time\n",
        "import itertools\n",
        "import os\n",
        "import math\n",
        "from collections import Counter # \n",
        "\n",
        "# pandas \n",
        "try:\n",
        "    import pandas as pd\n",
        "    PANDAS_AVAILABLE = True\n",
        "    pd.set_option('display.width', 180) # \n",
        "    pd.set_option('display.float_format', '{:.6e}'.format) # \n",
        "    pd.set_option('display.max_rows', 300) # \n",
        "except ImportError:\n",
        "    PANDAS_AVAILABLE = False\n",
        "    print(\"Warning: pandas library not found. Output formatting will be basic. CSV export disabled.\")\n",
        "\n",
        "# ===  () ===\n",
        "DEFAULT_TOLERANCE = 1e-9\n",
        "\n",
        "# --- OptimizationResult  ---\n",
        "class OptimizationResult:\n",
        "    def __init__(self, success, message, w_opt=None, pi_opt=None, lambda_opt=None,\n",
        "                 H_opt=None, grad_H_opt=None, iterations=None, fw_gap=None,\n",
        "                 active_set_opt=None):\n",
        "        self.success = success; self.message = message; self.w_opt = w_opt; self.pi_opt = pi_opt\n",
        "        self.lambda_opt = lambda_opt; self.H_opt = H_opt; self.grad_H_opt = grad_H_opt\n",
        "        self.iterations = iterations; self.fw_gap = fw_gap\n",
        "        self.active_set_opt = active_set_opt\n",
        "\n",
        "# --- find_feasible_initial_pi  ---\n",
        "def find_feasible_initial_pi(R, mu_tilde, K, tolerance=1e-8):\n",
        "    M = R.shape[1]; c = np.zeros(K + 1); c[K] = 1.0\n",
        "    A_ub = np.hstack((-R.T, -np.ones((M, 1)))); b_ub = -mu_tilde * np.ones(M)\n",
        "    bounds = [(None, None)] * K + [(0, None)]; opts = {'tol': tolerance, 'disp': False, 'presolve': True}\n",
        "    result = None; methods_to_try = ['highs', 'highs-ipm', 'highs-ds', 'simplex'] # Removed 'interior-point' as it's deprecated and caused issues\n",
        "    for method in methods_to_try:\n",
        "        try:\n",
        "            # Suppress specific warnings during linprog call if needed\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\", OptimizeWarning) # Ignore presolve warnings etc.\n",
        "                result = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=method, options=opts)\n",
        "            if result.success: break\n",
        "        except ValueError: # Handle potential issues like KKT matrix singular\n",
        "            continue\n",
        "        except Exception as e: # Catch other unexpected errors\n",
        "            print(f\"\\n  Phase 1 LP Exception ({method}): {e}\") # Print error for debugging\n",
        "            return None, False, f\"Phase 1 LP failed ({method}): {e}\" # Return specific error\n",
        "    # Check result status carefully\n",
        "    if result is None or not result.success:\n",
        "        msg = result.message if result else \"No solver succeeded\"\n",
        "        status = result.status if result else -1\n",
        "        return None, False, f\"Phase 1 LP solver failed: {msg} (status={status}, method={method if result else 'N/A'})\"\n",
        "\n",
        "    s = result.x[K]; pi = result.x[:K]\n",
        "\n",
        "    if np.isnan(pi).any(): # Explicit NaN check\n",
        "        return None, False, \"Phase 1 LP resulted in NaN values for pi.\"\n",
        "\n",
        "    # Check feasibility more robustly\n",
        "    G = -R.T; h = -mu_tilde * np.ones(M); violation = np.max(G @ pi - h)\n",
        "    feasibility_tol = tolerance * 10000 # Tolerance for feasibility check\n",
        "\n",
        "    # Phase 1 goal is s=0. Check if s is close to zero.\n",
        "    if s <= tolerance * 100: # s should ideally be zero\n",
        "        if violation <= feasibility_tol:\n",
        "             return pi, True, f\"Phase 1 OK (s*={s:.1e}, max_viol={violation:.1e})\"\n",
        "        else:\n",
        "             # s is small, but initial pi is still infeasible - might happen with loose tol\n",
        "             return pi, True, f\"Phase 1 OK (s*={s:.1e}), WARN: Initial violation {violation:.1e} > {feasibility_tol:.1e}\"\n",
        "    else:\n",
        "        # s is significantly positive, means the original constraints are likely infeasible\n",
        "        return None, False, f\"Phase 1 Likely Infeasible (s* = {s:.1e} > {tolerance*100:.1e}), max_viol={violation:.1e}\"\n",
        "\n",
        "\n",
        "# --- solve_kkt_system  ---\n",
        "def solve_kkt_system(Q, G_W, g, tolerance=DEFAULT_TOLERANCE):\n",
        "    K = Q.shape[0]; n_act = G_W.shape[0] if G_W is not None and G_W.ndim == 2 and G_W.shape[0] > 0 else 0\n",
        "    if n_act == 0:\n",
        "        try:\n",
        "            # Use 'auto' or specify based on Q properties if known (e.g., 'pos')\n",
        "            # Added check_finite=False for potential minor non-finite inputs\n",
        "            p = solve(Q, -g, assume_a='sym', check_finite=False)\n",
        "            l = np.array([]) # No active constraints, lambda is empty\n",
        "        except LinAlgError: # Matrix might be singular or nearly singular\n",
        "            return None, None, False\n",
        "        except ValueError as e: # Input contains NaN, infinity or is too large\n",
        "             print(f\"DEBUG KKT (n_act=0) ValueError: {e}\")\n",
        "             return None, None, False\n",
        "\n",
        "        # Check residual norm after solve\n",
        "        if p is None or np.isnan(p).any() or np.isinf(p).any(): return None, None, False\n",
        "        res_norm = np.linalg.norm(Q @ p + g); g_norm = np.linalg.norm(g)\n",
        "        # Increased tolerance slightly for numerical stability\n",
        "        solved_ok = res_norm <= tolerance * 1e4 * (1 + g_norm)\n",
        "        return p, l, solved_ok\n",
        "    else:\n",
        "        kkt_mat = None; rhs = None\n",
        "        try:\n",
        "            # Ensure G_W is correctly shaped before blocking\n",
        "            if G_W.ndim != 2 or G_W.shape[1] != K:\n",
        "                 print(f\"DEBUG KKT (n_act>0) G_W shape error: {G_W.shape}\")\n",
        "                 return None, None, False\n",
        "            kkt_mat = np.block([[Q, G_W.T], [G_W, np.zeros((n_act, n_act))]])\n",
        "            rhs = np.concatenate([-g, np.zeros(n_act)])\n",
        "        except ValueError as e: # If shapes mismatch during block creation\n",
        "            print(f\"DEBUG KKT (n_act>0) Block ValueError: {e}\")\n",
        "            return None, None, False\n",
        "\n",
        "        try:\n",
        "            # Added check_finite=False\n",
        "            sol = solve(kkt_mat, rhs, assume_a='sym', check_finite=False)\n",
        "            p = sol[:K]; l = sol[K:]\n",
        "        except LinAlgError: # KKT matrix might be singular\n",
        "            # Attempt pseudo-inverse as a fallback? Or just fail.\n",
        "            # For now, just fail. Add pinv later if needed.\n",
        "            # print(f\"DEBUG KKT (n_act>0) LinAlgError. Cond: {np.linalg.cond(kkt_mat)}\") # Check condition number\n",
        "            return None, None, False\n",
        "        except ValueError as e: # Input contains NaN, infinity or is too large\n",
        "            print(f\"DEBUG KKT (n_act>0) ValueError: {e}\")\n",
        "            return None, None, False\n",
        "\n",
        "        # Check residual norm after solve\n",
        "        if sol is None or np.isnan(sol).any() or np.isinf(sol).any(): return None, None, False\n",
        "        res_norm = np.linalg.norm(kkt_mat @ sol - rhs); rhs_norm = np.linalg.norm(rhs)\n",
        "        # Increased tolerance slightly for numerical stability\n",
        "        solved_ok = res_norm <= tolerance * 1e4 * (1 + rhs_norm)\n",
        "        return p, l, solved_ok\n",
        "\n",
        "# --- solve_inner_qp_active_set  () ---\n",
        "def solve_inner_qp_active_set(Vw, R, mu_tilde, initial_pi, max_iter=350, tolerance=DEFAULT_TOLERANCE, regularization_epsilon=1e-10):\n",
        "    K = Vw.shape[0]; M = R.shape[1];\n",
        "    # Ensure Vw is finite before regularization\n",
        "    if not np.all(np.isfinite(Vw)): return None, None, None, None, None, False, \"QP fail: Vw contains NaN/Inf.\"\n",
        "    # Regularize Q matrix (ensure positive definite)\n",
        "    Q_reg = 2 * Vw + 2 * regularization_epsilon * np.eye(K)\n",
        "    # Check condition number of Q_reg\n",
        "    cond_Q = np.linalg.cond(Q_reg)\n",
        "    if cond_Q > 1/tolerance: # If condition number is too high\n",
        "        warnings.warn(f\"QP Warning: Condition number of Q_reg is high ({cond_Q:.2e}). Regularization might be insufficient.\")\n",
        "        # Consider increasing regularization if this happens often\n",
        "        # Q_reg += 2 * (regularization_epsilon * 10) * np.eye(K)\n",
        "\n",
        "    G = -R.T; h = -mu_tilde * np.ones(M)\n",
        "\n",
        "    if initial_pi is None or not np.all(np.isfinite(initial_pi)):\n",
        "        return None, None, None, None, None, False, \"QP fail: Initial pi is None or contains NaN/Inf.\"\n",
        "\n",
        "    pi_k = np.copy(initial_pi)\n",
        "    lam_opt = np.zeros(M)\n",
        "    W = set() # Active set (indices of constraints treated as equalities)\n",
        "\n",
        "    # Tolerance for constraint activation/violation checks\n",
        "    active_tol = tolerance * 100 # Increased tolerance slightly\n",
        "\n",
        "    # Initial check for feasibility and active set determination\n",
        "    initial_violations = G @ pi_k - h\n",
        "    W = set(j for j, viol in enumerate(initial_violations) if viol > -active_tol) # Activate constraints violated or close to boundary\n",
        "    max_initial_violation = np.max(initial_violations)\n",
        "    if max_initial_violation > active_tol * 10: # Allow small initial infeasibility\n",
        "        warnings.warn(f\"QP Warning: Initial pi infeasible (max viol: {max_initial_violation:.2e} > {active_tol * 10:.1e}). Active set: {sorted(list(W))}\")\n",
        "        # Attempt to project initial pi onto feasible region? Might be complex. Start anyway.\n",
        "\n",
        "    active_indices_opt = None # Store the final active set indices\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        g_k = Q_reg @ pi_k # Gradient of objective at pi_k\n",
        "        if not np.all(np.isfinite(g_k)): return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Gradient g_k contains NaN/Inf.\"\n",
        "\n",
        "        act = sorted(list(W))\n",
        "        n_act = len(act)\n",
        "        G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "\n",
        "        # Solve KKT system for search direction p_k and Lagrange multipliers lam_Wk\n",
        "        p_k, lam_Wk, solved = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "\n",
        "        # Check if KKT solve was successful\n",
        "        if not solved or p_k is None:\n",
        "            return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: KKT solve failed. ActiveSet={act}. Cond(Q)={cond_Q:.1e}\"\n",
        "\n",
        "        # Check for convergence: If search direction p_k is close to zero\n",
        "        p_norm = np.linalg.norm(p_k)\n",
        "        pi_k_norm = np.linalg.norm(pi_k)\n",
        "        if p_norm <= tolerance * 100 * (1 + pi_k_norm): # Increased tolerance\n",
        "            # Check optimality conditions (Lagrange multipliers for active constraints)\n",
        "            is_optimal_point = True\n",
        "            blocking_constraint_idx = -1\n",
        "            min_negative_lambda = float('inf')\n",
        "            # Tolerance for checking non-negativity of lambda\n",
        "            dual_feas_tol = -tolerance * 100 # Allow slightly negative lambda\n",
        "\n",
        "            if W: # If there are active constraints\n",
        "                if lam_Wk is None or len(lam_Wk) != n_act:\n",
        "                    return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: lam_Wk inconsistent? ActiveSet={act}\"\n",
        "                if not np.all(np.isfinite(lam_Wk)):\n",
        "                     return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: lam_Wk contains NaN/Inf. ActiveSet={act}\"\n",
        "\n",
        "                lambda_map = dict(zip(act, lam_Wk))\n",
        "                for constraint_idx, lagrange_multiplier in lambda_map.items():\n",
        "                    if lagrange_multiplier < dual_feas_tol:\n",
        "                        is_optimal_point = False\n",
        "                        # Find the index of the constraint with the most negative multiplier\n",
        "                        if lagrange_multiplier < min_negative_lambda:\n",
        "                            min_negative_lambda = lagrange_multiplier\n",
        "                            blocking_constraint_idx = constraint_idx\n",
        "\n",
        "            if is_optimal_point:\n",
        "                # Optimal solution found\n",
        "                lam_opt.fill(0.0)\n",
        "                if W and lam_Wk is not None and len(lam_Wk) == n_act:\n",
        "                    # Assign non-negative multipliers\n",
        "                    try:\n",
        "                        lam_opt[act] = np.maximum(lam_Wk, 0)\n",
        "                    except IndexError: # Should not happen if act and lam_Wk are consistent\n",
        "                        return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Index error assigning lambda. ActiveSet={act}\"\n",
        "\n",
        "                # Final check of feasibility\n",
        "                final_infeas = np.max(G @ pi_k - h)\n",
        "                msg = f\"Optimal found at iter {i+1}.\"\n",
        "                if final_infeas > active_tol: # Check against activation tolerance\n",
        "                    msg += f\" (WARN: Final violation {final_infeas:.1e})\"\n",
        "                active_indices_opt = act # Store the active set at optimum\n",
        "                return pi_k, lam_opt, active_indices_opt, None, Q_reg / 2.0, True, msg\n",
        "            else:\n",
        "                # Not optimal: remove the constraint with the most negative multiplier\n",
        "                if blocking_constraint_idx in W:\n",
        "                    W.remove(blocking_constraint_idx)\n",
        "                    # Continue to the next iteration with the updated active set\n",
        "                    continue\n",
        "                else:\n",
        "                    # Should not happen if blocking_constraint_idx was found correctly\n",
        "                    return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Negative lambda idx {blocking_constraint_idx}, not in W={act}\"\n",
        "        else:\n",
        "            # Non-zero search direction p_k: Move along p_k\n",
        "            alpha_k = 1.0 # Initial step length (full step)\n",
        "            blocking_constraint_idx = -1\n",
        "            min_step_length = float('inf')\n",
        "            # Tolerance for checking if a step is blocked by a constraint\n",
        "            step_tol = tolerance * 100 # Increased tolerance\n",
        "\n",
        "            # Check inactive constraints to see if any block the step\n",
        "            for j in range(M):\n",
        "                if j not in W: # Check only inactive constraints\n",
        "                    constraint_gradient_dot_p = G[j, :] @ p_k\n",
        "                    # If moving along p_k violates constraint j (G[j,:] @ (pi_k + alpha*p_k) > h[j])\n",
        "                    # This corresponds to G[j,:] @ p_k > 0 if constraint j is currently G[j,:] @ pi_k <= h[j]\n",
        "                    if constraint_gradient_dot_p > step_tol: # Potential to violate\n",
        "                        # Calculate distance to the boundary of constraint j\n",
        "                        distance_to_boundary = h[j] - (G[j, :] @ pi_k)\n",
        "                        if abs(constraint_gradient_dot_p) > 1e-15: # Avoid division by zero\n",
        "                            alpha_j = distance_to_boundary / constraint_gradient_dot_p\n",
        "                            # Ensure step is non-negative (should be if distance_to_boundary >= 0)\n",
        "                            step_j = max(0.0, alpha_j)\n",
        "                            # Find the smallest step length that hits a boundary\n",
        "                            if step_j < min_step_length:\n",
        "                                min_step_length = step_j\n",
        "                                blocking_constraint_idx = j\n",
        "                        # else: if constraint_gradient_dot_p is near zero, this constraint won't block significantly\n",
        "\n",
        "            # Determine the actual step length: minimum of full step (1.0) and blocking step\n",
        "            alpha_k = min(1.0, min_step_length)\n",
        "\n",
        "            # Update the current solution\n",
        "            pi_k += alpha_k * p_k\n",
        "\n",
        "            # If the step was blocked (alpha_k < 1.0), add the blocking constraint to the active set\n",
        "            # Check if alpha_k is significantly less than 1\n",
        "            if alpha_k < 1.0 - step_tol and blocking_constraint_idx != -1:\n",
        "                if blocking_constraint_idx not in W:\n",
        "                    W.add(blocking_constraint_idx)\n",
        "\n",
        "            # Continue to the next iteration\n",
        "            continue\n",
        "\n",
        "    # Max iterations reached\n",
        "    msg = f\"Max iter ({max_iter}) reached.\"\n",
        "    # Check final feasibility\n",
        "    final_infeas = np.max(G @ pi_k - h)\n",
        "    if final_infeas > active_tol * 10: # Check against a larger tolerance\n",
        "        return None, None, None, None, None, False, f\"{msg} Final infeasible ({final_infeas:.1e} > {active_tol*10:.1e}). ActiveSet={sorted(list(W))}\"\n",
        "\n",
        "    # Try to assess optimality at the final point (even if max_iter reached)\n",
        "    act = sorted(list(W))\n",
        "    n_act = len(act)\n",
        "    G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "    is_likely_optimal = False\n",
        "    active_constraints_opt = act\n",
        "\n",
        "    g_k = Q_reg @ pi_k # Final gradient\n",
        "    if not np.all(np.isfinite(g_k)): return None, None, None, None, None, False, f\"{msg} Final gradient g_k contains NaN/Inf.\"\n",
        "\n",
        "    p_f, lam_f, solved_f = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "    final_lambda_estimate = np.zeros(M)\n",
        "\n",
        "    # Check if the final point satisfies approximate KKT conditions\n",
        "    # Check stationarity (p_f should be near zero)\n",
        "    if solved_f and p_f is not None and np.linalg.norm(p_f) <= tolerance * 1000 * (1 + np.linalg.norm(pi_k)): # Looser check\n",
        "        # Check dual feasibility (lambda for active constraints should be non-negative)\n",
        "        if n_act > 0:\n",
        "            if lam_f is not None and len(lam_f) == n_act and np.all(np.isfinite(lam_f)):\n",
        "                 try:\n",
        "                     final_lambda_estimate[act] = lam_f\n",
        "                 except IndexError:\n",
        "                     pass # Should not happen\n",
        "                 active_lambdas = final_lambda_estimate[act]\n",
        "                 if np.all(active_lambdas >= -tolerance * 1000): # Allow slightly negative lambda\n",
        "                     is_likely_optimal = True\n",
        "                     msg += \" Final KKT check approx OK.\"\n",
        "                 else:\n",
        "                     min_lam = np.min(active_lambdas)\n",
        "                     msg += f\" Final KKT check fails (dual infeasible, min_lam={min_lam:.1e}).\"\n",
        "            else:\n",
        "                 msg += \" Final KKT check fails (lam_f invalid or NaN/Inf).\"\n",
        "        else: # No active constraints, optimality requires gradient = 0 (checked by p_f ~ 0)\n",
        "             is_likely_optimal = True\n",
        "             msg += \" Final KKT check approx OK (unconstrained).\"\n",
        "    else:\n",
        "        p_norm_f = np.linalg.norm(p_f) if p_f is not None else np.nan\n",
        "        msg += f\" Final KKT check fails (stationarity error, p_norm={p_norm_f:.1e} or solve error).\"\n",
        "\n",
        "    # Return the final solution, even if potentially suboptimal due to max_iter\n",
        "    lam_opt = np.maximum(final_lambda_estimate, 0) # Ensure non-negativity for return\n",
        "    return pi_k, lam_opt, active_constraints_opt, None, Q_reg / 2.0, is_likely_optimal, msg\n",
        "\n",
        "# --- make_psd  ---\n",
        "def make_psd(matrix, tolerance=1e-8):\n",
        "    if not np.all(np.isfinite(matrix)):\n",
        "         warnings.warn(\"make_psd: Input matrix contains NaN/Inf. Returning as is after symmetrization.\")\n",
        "         sym = (matrix + matrix.T) / 2.0 # Attempt symmetrization anyway\n",
        "         return sym # Or should return None or raise error?\n",
        "    sym = (matrix + matrix.T) / 2.0\n",
        "    try:\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(sym)\n",
        "        min_eigenvalue = np.min(eigenvalues)\n",
        "        if min_eigenvalue < tolerance:\n",
        "            # Shift eigenvalues to be at least `tolerance`\n",
        "            eigenvalues[eigenvalues < tolerance] = tolerance\n",
        "            # Reconstruct the matrix\n",
        "            psd_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n",
        "            # Ensure symmetry again due to potential numerical errors\n",
        "            return (psd_matrix + psd_matrix.T) / 2.0\n",
        "        else:\n",
        "            # Already PSD (or positive definite)\n",
        "            return sym\n",
        "    except LinAlgError:\n",
        "        warnings.warn(\"make_psd: eigh failed, possibly due to non-finite values or extreme scaling. Returning symmetrized input.\")\n",
        "        return sym # Fallback to returning the symmetrized matrix\n",
        "\n",
        "\n",
        "# --- calculate_Vw  ---\n",
        "def calculate_Vw(w, R, SecondMoments_a_array, tolerance=DEFAULT_TOLERANCE, psd_tolerance=1e-8):\n",
        "    K, M = R.shape\n",
        "    # Ensure w sums to 1 (or close enough) and non-negative\n",
        "    w_norm = project_to_simplex(w) # Project w onto simplex for safety\n",
        "    if not np.isclose(np.sum(w_norm), 1.0):\n",
        "         raise ValueError(f\"calculate_Vw: w does not sum to 1 after projection: {np.sum(w_norm)}\")\n",
        "\n",
        "    # Check inputs for NaN/Inf\n",
        "    if not np.all(np.isfinite(R)): raise ValueError(\"calculate_Vw: R contains NaN/Inf.\")\n",
        "    if not np.all(np.isfinite(SecondMoments_a_array)): raise ValueError(\"calculate_Vw: SecondMoments_a_array contains NaN/Inf.\")\n",
        "\n",
        "    EwX = R @ w_norm\n",
        "    EwXXT = np.zeros((K, K))\n",
        "    for m in range(M):\n",
        "        EwXXT += w_norm[m] * SecondMoments_a_array[m]\n",
        "\n",
        "    # Check calculated moments for NaN/Inf\n",
        "    if not np.all(np.isfinite(EwX)): raise ValueError(\"calculate_Vw: EwX calculation resulted in NaN/Inf.\")\n",
        "    if not np.all(np.isfinite(EwXXT)): raise ValueError(\"calculate_Vw: EwXXT calculation resulted in NaN/Inf.\")\n",
        "\n",
        "    Vw = EwXXT - np.outer(EwX, EwX)\n",
        "    if not np.all(np.isfinite(Vw)): raise ValueError(\"calculate_Vw: Vw calculation resulted in NaN/Inf.\")\n",
        "\n",
        "    Vw_psd = make_psd(Vw, psd_tolerance)\n",
        "    # Final check after PSD making\n",
        "    if not np.all(np.isfinite(Vw_psd)): raise ValueError(\"calculate_Vw: Vw_psd after make_psd contains NaN/Inf.\")\n",
        "\n",
        "    return Vw_psd, EwX, EwXXT\n",
        "\n",
        "# --- calculate_H_gradient  ( + NaN) ---\n",
        "def calculate_H_gradient(pi_star, w, R, SecondMoments_a_array, EwX, EwXXT, tolerance=1e-9, debug_print=False):\n",
        "    M = w.shape[0]; K = R.shape[0]; grad = np.zeros(M)\n",
        "    norm_tolerance = 1e-12 # Allow very small pi_star\n",
        "\n",
        "    # --- Input Validation ---\n",
        "    if pi_star is None: return np.full(M, np.nan) # Grad cannot be computed\n",
        "    if not np.all(np.isfinite(pi_star)): return np.full(M, np.nan)\n",
        "    if not np.all(np.isfinite(w)): return np.full(M, np.nan)\n",
        "    if not np.all(np.isfinite(R)): return np.full(M, np.nan)\n",
        "    if not np.all(np.isfinite(SecondMoments_a_array)): return np.full(M, np.nan)\n",
        "    if EwX is None or not np.all(np.isfinite(EwX)): return np.full(M, np.nan)\n",
        "    if EwXXT is None or not np.all(np.isfinite(EwXXT)): return np.full(M, np.nan)\n",
        "    # --- End Input Validation ---\n",
        "\n",
        "    pi_norm = np.linalg.norm(pi_star)\n",
        "    # If pi_star is numerically zero, gradient is effectively zero (or undefined), return NaN to signal issue\n",
        "    if pi_norm < norm_tolerance:\n",
        "        if debug_print: print(f\"DEBUG grad_H: Returning NaN because pi_star norm {pi_norm:.2e} < {norm_tolerance:.1e}\")\n",
        "        return np.full(M, np.nan)\n",
        "\n",
        "    try:\n",
        "        pi_T_EwX = pi_star.T @ EwX\n",
        "        if not np.isfinite(pi_T_EwX):\n",
        "             if debug_print: print(f\"DEBUG grad_H: pi_T_EwX is NaN/Inf: {pi_T_EwX}\")\n",
        "             return np.full(M, np.nan)\n",
        "\n",
        "        for j in range(M):\n",
        "            Sigma_j = SecondMoments_a_array[j]\n",
        "            r_j = R[:, j]\n",
        "\n",
        "            # Term 1: pi^T * Sigma_j * pi\n",
        "            pi_T_Sigma_j_pi = pi_star.T @ Sigma_j @ pi_star\n",
        "            if not np.isfinite(pi_T_Sigma_j_pi):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_Sigma_j_pi is NaN/Inf: {pi_T_Sigma_j_pi}\")\n",
        "                 grad[j] = np.nan; continue # Mark as NaN and skip to next component\n",
        "\n",
        "            # Intermediate term: pi^T * r_j\n",
        "            pi_T_r_j = pi_star.T @ r_j\n",
        "            if not np.isfinite(pi_T_r_j):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_r_j is NaN/Inf: {pi_T_r_j}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "\n",
        "            # Term 2: 2 * (pi^T * r_j) * (pi^T * EwX)\n",
        "            term2 = 2 * pi_T_r_j * pi_T_EwX\n",
        "            if not np.isfinite(term2):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): term2 (2*pi_T_r_j*pi_T_EwX) is NaN/Inf: {term2}, pi_T_r_j={pi_T_r_j}, pi_T_EwX={pi_T_EwX}\")\n",
        "                 grad[j] = np.nan; continue\n",
        "\n",
        "            # Gradient component j = Term 1 - Term 2\n",
        "            grad[j] = pi_T_Sigma_j_pi - term2\n",
        "            if not np.isfinite(grad[j]): # Final check for the component\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): final grad[{j}] is NaN/Inf\")\n",
        "                 # Keep grad[j] as NaN if calculation resulted in NaN/Inf\n",
        "\n",
        "    except Exception as e_calc:\n",
        "         print(f\"\\nERROR in calculate_H_gradient calculation: {e_calc}\")\n",
        "         print(traceback.format_exc())\n",
        "         return np.full(M, np.nan) # Return NaN array on unexpected error\n",
        "\n",
        "    # Final check on the whole gradient vector\n",
        "    if np.any(~np.isfinite(grad)):\n",
        "        # warnings.warn(f\"NaN or Inf detected in calculated gradient for w={w}. Grad={grad}\")\n",
        "        if debug_print: print(f\"DEBUG grad_H: Final check found NaN/Inf in gradient: {grad}\")\n",
        "        # Return the gradient vector possibly containing NaNs\n",
        "        return grad # Don't replace with all NaNs if only some failed\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "# --- project_to_simplex  ---\n",
        "def project_to_simplex(v, z=1):\n",
        "    n_features = v.shape[0]\n",
        "    if n_features == 0: return np.array([]) # Handle empty input\n",
        "\n",
        "    # Ensure v is a numpy array and finite\n",
        "    v_arr = np.asarray(v)\n",
        "    if not np.all(np.isfinite(v_arr)):\n",
        "        # Handle non-finite values: Option 1: Raise error\n",
        "        # raise ValueError(\"Input vector contains NaN or Inf.\")\n",
        "        # Option 2: Try to recover (e.g., replace with 0 or average), but this can be problematic.\n",
        "        # Option 3: Return a default valid projection (e.g., uniform) - might hide issues.\n",
        "        # Let's return uniform for now, with a warning.\n",
        "        warnings.warn(\"project_to_simplex: Input vector contains NaN/Inf. Returning uniform projection.\")\n",
        "        return np.full(n_features, z / n_features) if n_features > 0 else np.array([])\n",
        "\n",
        "\n",
        "    # Check if already on simplex (within tolerance)\n",
        "    if np.all(v_arr >= -1e-9) and np.isclose(np.sum(v_arr), z):\n",
        "        return np.maximum(v_arr, 0) # Ensure non-negativity strictly\n",
        "\n",
        "    # Use the standard projection algorithm (Micchelli, etc.)\n",
        "    u = np.sort(v_arr)[::-1] # Sort in descending order\n",
        "    cssv = np.cumsum(u) - z\n",
        "    ind = np.arange(n_features) + 1\n",
        "    cond = u - cssv / ind > 0\n",
        "\n",
        "    # Handle the case where no rho is found (e.g., all elements <= 0 for z > 0)\n",
        "    if np.any(cond):\n",
        "        rho = ind[cond][-1]\n",
        "        theta = cssv[rho - 1] / float(rho)\n",
        "        w = np.maximum(v_arr - theta, 0)\n",
        "    else:\n",
        "        # This case might happen if v has negative components and z=1.\n",
        "        # Project onto the closest point, which might be a vertex.\n",
        "        # A simple fallback: assign total weight to the max element if z > 0.\n",
        "         w = np.zeros(n_features)\n",
        "         if z > 0:\n",
        "             w[np.argmax(v_arr)] = z # Assign weight to the largest component index\n",
        "\n",
        "    # Normalize to ensure sum is exactly z, handling potential floating point errors\n",
        "    w_sum = np.sum(w)\n",
        "    if not np.isclose(w_sum, z):\n",
        "        if w_sum > 1e-9: # Avoid division by zero if sum is tiny\n",
        "            w = w * (z / w_sum)\n",
        "        elif z > 0 : # If sum is near zero but should be positive z\n",
        "            # Fallback again: Assign total weight to the max element's index from original v\n",
        "            w = np.zeros(n_features)\n",
        "            w[np.argmax(v_arr)] = z\n",
        "        # If z=0, w should be all zeros, which is likely covered.\n",
        "\n",
        "    # Final check for non-negativity\n",
        "    return np.maximum(w, 0)\n",
        "\n",
        "\n",
        "# --- frank_wolfe_optimizer  (ActiveSet) ---\n",
        "def frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde, initial_w=None, max_outer_iter=250, fw_gap_tol=1e-7, inner_max_iter=350, tolerance=1e-9, psd_make_tolerance=1e-8, qp_regularization=1e-10, debug_print=False, force_iterations=0, return_history=False):\n",
        "    K, M = R_alpha.shape\n",
        "    if initial_w is None: w_k = np.ones(M) / M\n",
        "    else: w_k = project_to_simplex(np.copy(initial_w))\n",
        "\n",
        "    if w_k is None or not np.all(np.isfinite(w_k)): # Check initial w_k\n",
        "        result = OptimizationResult(False, \"Initial w_k invalid after projection\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "\n",
        "    fw_gap = float('inf')\n",
        "    best_w = np.copy(w_k)\n",
        "    best_pi = None\n",
        "    best_lam = np.zeros(M)\n",
        "    best_H = -float('inf')\n",
        "    best_grad_H = np.full(M, np.nan)\n",
        "    best_active_set = None\n",
        "    history = []\n",
        "\n",
        "    # --- Phase 1: Find initial feasible pi_0 ---\n",
        "    pi0, ok, p1msg = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance)\n",
        "    if not ok:\n",
        "        result = OptimizationResult(False, f\"Phase 1 failed: {p1msg}\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "    if pi0 is None or not np.all(np.isfinite(pi0)): # Check pi0 validity\n",
        "        result = OptimizationResult(False, \"Phase 1 returned invalid pi0\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "    # --- End Phase 1 ---\n",
        "\n",
        "    # Initialize tracking variables\n",
        "    current_w = np.copy(w_k)\n",
        "    current_H = -float('inf')\n",
        "    current_pi = np.copy(pi0) # Start with feasible pi0\n",
        "    current_lam = np.zeros(M)\n",
        "    current_active_set = tuple()\n",
        "    current_grad_H = np.full(M, np.nan)\n",
        "    current_fw_gap = float('inf')\n",
        "\n",
        "    # QP solver args\n",
        "    inner_solver_args_for_loop = {'max_iter': inner_max_iter, 'tolerance': tolerance, 'regularization_epsilon': qp_regularization}\n",
        "\n",
        "    last_successful_pi = np.copy(pi0) # Store the last valid pi found\n",
        "\n",
        "    for k in range(max_outer_iter):\n",
        "        iter_data = {'k': k + 1, 'w_k': np.copy(w_k)} if return_history else {}\n",
        "\n",
        "        # --- Calculate Vw and related terms ---\n",
        "        try:\n",
        "            Vk, Ex, ExxT = calculate_Vw(w_k, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "            if not np.all(np.isfinite(Vk)): raise ValueError(\"Vk contains NaN/Inf\")\n",
        "            if not np.all(np.isfinite(Ex)): raise ValueError(\"Ex contains NaN/Inf\")\n",
        "        except Exception as e:\n",
        "            msg = f\"Outer iter {k+1}: Vw calculation failed: {e}. Using best known state.\"\n",
        "            warnings.warn(msg)\n",
        "            result = OptimizationResult(False, msg, w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, grad_H_opt=best_grad_H, active_set_opt=best_active_set, iterations=k)\n",
        "            if return_history: history.append(iter_data); return (result, history)\n",
        "            else: return result\n",
        "        # --- End Calculate Vw ---\n",
        "\n",
        "        # --- Inner QP Solve (using Active Set) ---\n",
        "        # Use the last successful pi as the initial guess for the inner solver\n",
        "        pi_init_inner = last_successful_pi if last_successful_pi is not None else pi0\n",
        "\n",
        "        # Check if pi_init_inner is valid before passing\n",
        "        if pi_init_inner is None or not np.all(np.isfinite(pi_init_inner)):\n",
        "             warnings.warn(f\"Outer iter {k+1}: Invalid pi_init_inner. Reverting to pi0.\")\n",
        "             pi_init_inner = pi0 # Fallback to the initial feasible point\n",
        "             if pi_init_inner is None or not np.all(np.isfinite(pi_init_inner)): # If pi0 is also invalid, fail\n",
        "                  msg = f\"Outer iter {k+1}: Both last successful pi and pi0 are invalid. Cannot solve inner QP.\"\n",
        "                  result = OptimizationResult(False, msg, w_opt=best_w, iterations=k)\n",
        "                  if return_history: history.append(iter_data); return (result, history)\n",
        "                  else: return result\n",
        "\n",
        "        pk, lk, act_idx_k, _, _, inner_ok, inner_msg = solve_inner_qp_active_set(\n",
        "            Vk, R_alpha, mu_tilde, pi_init_inner, **inner_solver_args_for_loop\n",
        "        )\n",
        "        # --- End Inner QP Solve ---\n",
        "\n",
        "        # --- Process Inner QP Results ---\n",
        "        if not inner_ok or pk is None or not np.all(np.isfinite(pk)):\n",
        "            warnings.warn(f\"Outer iter {k+1}: Inner QP failed or returned invalid pi: {inner_msg}. Trying fallback.\")\n",
        "            # Fallback 1: Try solving QP with the default pi0\n",
        "            if pi_init_inner is not pi0: # Avoid re-solving if already tried pi0\n",
        "                pi0_check, _, _, _, _, inner_ok_pi0, inner_msg_pi0 = solve_inner_qp_active_set(Vk, R_alpha, mu_tilde, pi0, **inner_solver_args_for_loop)\n",
        "                if inner_ok_pi0 and pi0_check is not None and np.all(np.isfinite(pi0_check)):\n",
        "                    warnings.warn(f\"Outer iter {k+1}: Inner QP succeeded with pi0 after failing with last_successful_pi.\")\n",
        "                    pk = pi0_check\n",
        "                    # Need to recalculate lk, act_idx_k if using pi0_check - Requires another KKT solve or accepting potentially inaccurate lambda/active set from solve_inner_qp_active_set\n",
        "                    # For simplicity, let's re-use the results from the successful call if possible, or reset them\n",
        "                    # This part is tricky - ideally solve_inner_qp returns consistent results\n",
        "                    lk = np.zeros(M) # Reset lambda as it's unreliable\n",
        "                    act_idx_k = [] # Reset active set\n",
        "                    inner_ok = True # Mark as ok for gradient calculation\n",
        "                else:\n",
        "                    warnings.warn(f\"Outer iter {k+1}: Inner QP also failed with pi0: {inner_msg_pi0}. Stopping.\")\n",
        "                    msg = f\"Outer iter {k+1}: Inner QP failed with both initial guesses. {inner_msg}\"\n",
        "                    result = OptimizationResult(False, msg, w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, grad_H_opt=best_grad_H, active_set_opt=best_active_set, iterations=k)\n",
        "                    if return_history: history.append(iter_data); return (result, history)\n",
        "                    else: return result\n",
        "            else: # Already tried pi0 or pi_init_inner was pi0\n",
        "                msg = f\"Outer iter {k+1}: Inner QP failed: {inner_msg}. Stopping.\"\n",
        "                result = OptimizationResult(False, msg, w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, grad_H_opt=best_grad_H, active_set_opt=best_active_set, iterations=k)\n",
        "                if return_history: history.append(iter_data); return (result, history)\n",
        "                else: return result\n",
        "\n",
        "        # Inner QP succeeded (either initially or with fallback)\n",
        "        last_successful_pi = np.copy(pk) # Update last successful pi\n",
        "        Hk = pk.T @ Vk @ pk\n",
        "        current_pi = np.copy(pk)\n",
        "        current_lam = lk if lk is not None and np.all(np.isfinite(lk)) else np.zeros(M)\n",
        "        current_active_set = tuple(sorted(act_idx_k)) if act_idx_k is not None else tuple()\n",
        "        current_H = Hk\n",
        "\n",
        "        # --- Calculate Gradient of H ---\n",
        "        try:\n",
        "            gHk = calculate_H_gradient(pk, w_k, R_alpha, SecondMoments_alpha_array, Ex, ExxT, tolerance, debug_print=debug_print)\n",
        "            if gHk is None or np.any(~np.isfinite(gHk)):\n",
        "                raise ValueError(\"Gradient calculation returned None or NaN/Inf\")\n",
        "            current_grad_H = gHk\n",
        "        except Exception as e:\n",
        "            msg = f\"Outer iter {k+1}: Gradient calculation failed: {e}. Stopping.\"\n",
        "            warnings.warn(msg)\n",
        "            result = OptimizationResult(False, msg, w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, grad_H_opt=best_grad_H, active_set_opt=best_active_set, iterations=k)\n",
        "            if return_history: history.append(iter_data); return (result, history)\n",
        "            else: return result\n",
        "        # --- End Calculate Gradient ---\n",
        "\n",
        "        if return_history:\n",
        "             iter_data['H_k'] = Hk\n",
        "             iter_data['grad_H_k_norm'] = np.linalg.norm(current_grad_H)\n",
        "             iter_data['pi_k'] = np.copy(pk)\n",
        "             iter_data['lam_k'] = np.copy(current_lam)\n",
        "             iter_data['active_set_k'] = current_active_set\n",
        "\n",
        "        # --- Update Best Solution Found So Far ---\n",
        "        # Use a small tolerance for comparing H values\n",
        "        if Hk > best_H + tolerance * 1e-1: # If significantly better or first valid H\n",
        "            best_H = Hk; best_w = np.copy(w_k); best_pi = np.copy(pk)\n",
        "            best_lam = np.copy(current_lam); best_grad_H = np.copy(current_grad_H)\n",
        "            best_active_set = current_active_set\n",
        "        elif np.isclose(Hk, best_H, atol=tolerance*1e-1, rtol=tolerance*1e-1): # If close to best H\n",
        "             # Optional: Update if w is substantially different? Or keep the first one found.\n",
        "             # Keep the current best for simplicity unless Hk is clearly better.\n",
        "             pass\n",
        "\n",
        "        # --- Frank-Wolfe Step ---\n",
        "        # Find descent direction s_k (vertex maximizing linear approximation)\n",
        "        grad_norm = np.linalg.norm(current_grad_H)\n",
        "        # Check if gradient is too small (potential convergence or numerical issue)\n",
        "        if grad_norm < tolerance * 100: # Increased tolerance\n",
        "             sk = w_k # Stay at current point if gradient is negligible\n",
        "             sk_idx = -1 # Indicate no specific vertex direction\n",
        "        else:\n",
        "             sk_idx = np.argmax(current_grad_H)\n",
        "             sk = np.zeros(M); sk[sk_idx] = 1.0\n",
        "        if return_history: iter_data['s_k_index'] = sk_idx\n",
        "\n",
        "        # Calculate FW gap\n",
        "        fw_gap = current_grad_H.T @ (w_k - sk)\n",
        "        current_fw_gap = fw_gap # Update current gap\n",
        "        if return_history: iter_data['fw_gap'] = fw_gap\n",
        "\n",
        "        # --- Check Convergence ---\n",
        "        converged = False\n",
        "        # Check gap after force_iterations, ensure gap is finite\n",
        "        if k >= force_iterations and np.isfinite(fw_gap):\n",
        "            if abs(fw_gap) <= fw_gap_tol:\n",
        "                 converged = True\n",
        "                 conv_msg = f\"Converged (Gap {abs(fw_gap):.2e} <= {fw_gap_tol:.1e})\"\n",
        "\n",
        "        if converged:\n",
        "            # --- Final Check/Refinement ---\n",
        "            # Optional: Run one last inner QP solve at the converged w_k\n",
        "            # to ensure pi, lambda, grad_H are consistent with the final w.\n",
        "            final_w = np.copy(w_k)\n",
        "            final_pi = np.copy(current_pi)\n",
        "            final_lam = np.copy(current_lam)\n",
        "            final_active_set = current_active_set\n",
        "            final_H = current_H\n",
        "            final_grad = np.copy(current_grad_H)\n",
        "            final_gap = current_fw_gap\n",
        "\n",
        "            try:\n",
        "                final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "                # Use a known feasible point (pi0) for the final consistency check solve\n",
        "                pi_f, lam_f, act_f, _, _, ok_f, msg_f = solve_inner_qp_active_set(\n",
        "                    final_Vk, R_alpha, mu_tilde, pi0, # Use stable pi0\n",
        "                    **inner_solver_args_for_loop\n",
        "                )\n",
        "                if ok_f and pi_f is not None and np.all(np.isfinite(pi_f)):\n",
        "                     final_pi = pi_f # Update pi\n",
        "                     final_lam = lam_f if lam_f is not None and np.all(np.isfinite(lam_f)) else np.zeros(M)\n",
        "                     final_active_set = tuple(sorted(act_f)) if act_f is not None else tuple()\n",
        "                     final_H = final_pi.T @ final_Vk @ final_pi # Recalculate H\n",
        "\n",
        "                     # Recalculate gradient and gap for final consistency\n",
        "                     try:\n",
        "                         final_grad = calculate_H_gradient(final_pi, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance)\n",
        "                         if final_grad is None or np.any(~np.isfinite(final_grad)): final_grad = np.full(M, np.nan) # Handle potential NaN\n",
        "                         if np.any(np.isfinite(final_grad)): # Check if grad is valid before gap calc\n",
        "                              grad_norm_f = np.linalg.norm(final_grad[np.isfinite(final_grad)]) # Norm of finite components\n",
        "                              if grad_norm_f < tolerance * 100: sk_f = final_w\n",
        "                              else: sk_idx_f = np.nanargmax(np.nan_to_num(final_grad, nan=-np.inf)); sk_f = np.zeros(M); sk_f[sk_idx_f] = 1.0\n",
        "                              final_gap = final_grad.T @ (final_w - sk_f) if np.all(np.isfinite(final_grad)) else np.nan\n",
        "                         else: final_gap = np.nan\n",
        "                     except Exception as e_grad:\n",
        "                         warnings.warn(f\"Final gradient calculation failed after convergence: {e_grad}\")\n",
        "                         final_grad = np.full(M, np.nan)\n",
        "                         final_gap = np.nan\n",
        "                else:\n",
        "                    warnings.warn(f\"Warning: Final inner QP solve failed after convergence ({msg_f}). Using values from last iteration.\")\n",
        "                    # Keep values from last iteration (before final check)\n",
        "            except Exception as e_final:\n",
        "                warnings.warn(f\"Error during final consistency check after convergence: {e_final}. Using values from last iteration.\")\n",
        "            # --- End Final Check ---\n",
        "\n",
        "            result = OptimizationResult(True, conv_msg, w_opt=final_w, pi_opt=final_pi, lambda_opt=final_lam,\n",
        "                                        H_opt=final_H, grad_H_opt=final_grad, iterations=k + 1,\n",
        "                                        fw_gap=final_gap, active_set_opt=final_active_set)\n",
        "            if return_history: history.append(iter_data); return (result, history)\n",
        "            else: return result\n",
        "        # --- End Convergence Check ---\n",
        "\n",
        "        # --- Prepare for next iteration ---\n",
        "        gamma = 2.0 / (k + 3.0) # Standard step size\n",
        "        if return_history: iter_data['gamma_k'] = gamma\n",
        "        if return_history: history.append(iter_data)\n",
        "\n",
        "        w_k_next = (1.0 - gamma) * w_k + gamma * sk\n",
        "        w_k = project_to_simplex(w_k_next) # Project back onto simplex\n",
        "\n",
        "        if w_k is None or not np.all(np.isfinite(w_k)): # Check validity after update\n",
        "             msg = f\"Outer iter {k+1}: w_k became invalid after update/projection. Stopping.\"\n",
        "             result = OptimizationResult(False, msg, w_opt=best_w, pi_opt=best_pi, H_opt=best_H, lambda_opt=best_lam, grad_H_opt=best_grad_H, active_set_opt=best_active_set, iterations=k+1)\n",
        "             if return_history: return (result, history) # Return history up to failure\n",
        "             else: return result\n",
        "        # Update pi0 for next inner solve warm start (use the latest successful pi)\n",
        "        pi0 = last_successful_pi # Note: already updated above if QP was successful\n",
        "\n",
        "\n",
        "    # --- Max Iterations Reached ---\n",
        "    # Perform final consistency check similar to convergence case\n",
        "    final_w = np.copy(w_k)\n",
        "    final_pi = np.copy(current_pi)\n",
        "    final_lam = np.copy(current_lam)\n",
        "    final_active_set = current_active_set\n",
        "    final_H = current_H\n",
        "    final_grad = np.copy(current_grad_H)\n",
        "    final_gap = current_fw_gap\n",
        "\n",
        "    try:\n",
        "        final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "        # Use stable pi0 for final solve\n",
        "        pi_f, lam_f, act_f, _, _, ok_f, msg_f = solve_inner_qp_active_set(\n",
        "            final_Vk, R_alpha, mu_tilde, pi0,\n",
        "            **inner_solver_args_for_loop\n",
        "        )\n",
        "        if ok_f and pi_f is not None and np.all(np.isfinite(pi_f)):\n",
        "            final_pi = pi_f\n",
        "            final_lam = lam_f if lam_f is not None and np.all(np.isfinite(lam_f)) else np.zeros(M)\n",
        "            final_active_set = tuple(sorted(act_f)) if act_f is not None else tuple()\n",
        "            final_H = final_pi.T @ final_Vk @ final_pi\n",
        "            try:\n",
        "                final_grad = calculate_H_gradient(final_pi, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance)\n",
        "                if final_grad is None or np.any(~np.isfinite(final_grad)): final_grad = np.full(M, np.nan)\n",
        "                if np.any(np.isfinite(final_grad)):\n",
        "                     grad_norm_f = np.linalg.norm(final_grad[np.isfinite(final_grad)])\n",
        "                     if grad_norm_f < tolerance * 100: sk_f = final_w\n",
        "                     else: sk_idx_f = np.nanargmax(np.nan_to_num(final_grad, nan=-np.inf)); sk_f = np.zeros(M); sk_f[sk_idx_f] = 1.0\n",
        "                     final_gap = final_grad.T @ (final_w - sk_f) if np.all(np.isfinite(final_grad)) else np.nan\n",
        "                else: final_gap = np.nan\n",
        "            except Exception as e_grad:\n",
        "                 warnings.warn(f\"Final gradient calculation failed after max_iter: {e_grad}\")\n",
        "                 final_grad = np.full(M, np.nan)\n",
        "                 final_gap = np.nan\n",
        "        else:\n",
        "            warnings.warn(f\"Warning: Final inner QP solve failed after max_iter ({msg_f}). Using values from last iteration.\")\n",
        "    except Exception as e_final:\n",
        "        warnings.warn(f\"Error during final consistency check after max_iter: {e_final}. Using values from last iteration.\")\n",
        "\n",
        "    # Add final state to history if requested\n",
        "    if return_history:\n",
        "        grad_norm_final = np.linalg.norm(final_grad[np.isfinite(final_grad)]) if np.any(np.isfinite(final_grad)) else np.nan\n",
        "        if grad_norm_final < tolerance * 100: sk_idx_final = -1\n",
        "        else: sk_idx_final = np.nanargmax(np.nan_to_num(final_grad, nan=-np.inf)) if np.any(np.isfinite(final_grad)) else -1\n",
        "        history.append({\n",
        "            'k': max_outer_iter + 1, 'w_k': np.copy(final_w), 'H_k': final_H,\n",
        "            'grad_H_k_norm': grad_norm_final, 's_k_index': sk_idx_final, 'fw_gap': final_gap,\n",
        "            'gamma_k': np.nan, 'pi_k': np.copy(final_pi), 'lam_k': np.copy(final_lam), 'active_set_k': final_active_set\n",
        "        })\n",
        "\n",
        "    # Return result indicating max iterations reached\n",
        "    result = OptimizationResult(True, f\"Max Iter ({max_outer_iter}) reached\", w_opt=final_w, pi_opt=final_pi, lambda_opt=final_lam,\n",
        "                                H_opt=final_H, grad_H_opt=final_grad, iterations=max_outer_iter,\n",
        "                                fw_gap=final_gap, active_set_opt=final_active_set)\n",
        "    return (result, history) if return_history else result\n",
        "\n",
        "# --- generate_params_profile_switching_symmetric  ( + Vm) ---\n",
        "def generate_params_profile_switching_symmetric(alpha, alpha_max, K=5, M=3,\n",
        "                                      R_base_sym=np.array([0.02, 0.01, 0.0, -0.01, -0.02]),\n",
        "                                      sigma_base=np.array([0.18, 0.15, 0.20, 0.12, 0.10]),\n",
        "                                      Corr_base=np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]),\n",
        "                                      r_offset=0.03,\n",
        "                                      #  Vm  \n",
        "                                      s_factor_g = 0.001, # good/neutral vs base variance factor\n",
        "                                      s_factor_b = -0.001, # bad vs base variance factor\n",
        "                                      # \n",
        "                                      sigma_min_epsilon=1e-4, psd_tolerance=1e-9):\n",
        "    \"\"\" Generates quasi-symmetric parameters with slight differences in Vm \"\"\"\n",
        "    assert K == len(R_base_sym) and K == len(sigma_base) and K == Corr_base.shape[0] and M == 3, \"Dimension mismatch\"\n",
        "\n",
        "    # --- Define base profiles (Good, Neutral, Bad) ---\n",
        "    R_neutral = R_base_sym\n",
        "    R_good = R_base_sym + r_offset\n",
        "    R_bad = R_base_sym - r_offset\n",
        "\n",
        "    # Ensure base correlation matrix is PSD\n",
        "    Corr_neutral_psd = make_psd(Corr_base, psd_tolerance)\n",
        "\n",
        "    # Base covariance for Neutral profile\n",
        "    sigma_neutral_eff = np.maximum(sigma_min_epsilon, sigma_base)\n",
        "    V_neutral = np.diag(sigma_neutral_eff) @ Corr_neutral_psd @ np.diag(sigma_neutral_eff)\n",
        "\n",
        "    # Covariances with slight differences for Good and Bad profiles\n",
        "    # Good profile: Slightly higher volatility\n",
        "    sigma_good_adj = np.maximum(sigma_min_epsilon, sigma_base * (1 + s_factor_g))\n",
        "    V_good = np.diag(sigma_good_adj) @ Corr_neutral_psd @ np.diag(sigma_good_adj)\n",
        "\n",
        "    # Bad profile: Slightly lower volatility (or adjust factors as needed)\n",
        "    sigma_bad_adj = np.maximum(sigma_min_epsilon, sigma_base * (1 + s_factor_b))\n",
        "    V_bad = np.diag(sigma_bad_adj) @ Corr_neutral_psd @ np.diag(sigma_bad_adj)\n",
        "    # --- End base profiles ---\n",
        "\n",
        "    # --- Calculate Second Moments (Sigma^m = V^m + r^m * r^m^T) ---\n",
        "    Sigma_neutral = V_neutral + np.outer(R_neutral, R_neutral)\n",
        "    Sigma_good = V_good + np.outer(R_good, R_good)\n",
        "    Sigma_bad = V_bad + np.outer(R_bad, R_bad)\n",
        "    # --- End Second Moments ---\n",
        "\n",
        "    # --- Apply profile switching based on alpha ---\n",
        "    beta = np.clip(alpha / alpha_max if alpha_max > 0 else (1.0 if alpha > 0 else 0.0), 0.0, 1.0)\n",
        "\n",
        "    R_alpha = np.zeros((K, M))\n",
        "    SecondMoments_a_array = np.zeros((M, K, K))\n",
        "\n",
        "    # Scenario 0 (m=0): Mix Good and Neutral (alpha moves from Good towards Neutral)\n",
        "    R_alpha[:, 0] = (1 - beta) * R_good + beta * R_neutral\n",
        "    SecondMoments_a_array[0, :, :] = (1 - beta) * Sigma_good + beta * Sigma_neutral\n",
        "\n",
        "    # Scenario 1 (m=1): Mix Bad and Good (alpha moves from Bad towards Good)\n",
        "    R_alpha[:, 1] = (1 - beta) * R_bad + beta * R_good\n",
        "    SecondMoments_a_array[1, :, :] = (1 - beta) * Sigma_bad + beta * Sigma_good\n",
        "\n",
        "    # Scenario 2 (m=2): Mix Neutral and Bad (alpha moves from Neutral towards Bad)\n",
        "    R_alpha[:, 2] = (1 - beta) * R_neutral + beta * R_bad\n",
        "    SecondMoments_a_array[2, :, :] = (1 - beta) * Sigma_neutral + beta * Sigma_bad\n",
        "    # --- End profile switching ---\n",
        "\n",
        "    # Final check for validity\n",
        "    if not np.all(np.isfinite(R_alpha)): raise ValueError(\"Generated R_alpha contains NaN/Inf.\")\n",
        "    if not np.all(np.isfinite(SecondMoments_a_array)): raise ValueError(\"Generated SecondMoments_a_array contains NaN/Inf.\")\n",
        "\n",
        "    return R_alpha, SecondMoments_a_array\n",
        "\n",
        "\n",
        "# ===  (ActiveSet) ===\n",
        "def analyze_alpha_full_results(alpha_range, param_gen_kwargs, optimizer_kwargs, mu_tilde):\n",
        "    \"\"\"  alpha  FWH*pi*grad H*lambda*active_set*  \"\"\"\n",
        "    results_over_alpha = []\n",
        "    K = param_gen_kwargs.get('K', 5)\n",
        "    M = param_gen_kwargs.get('M', 3)\n",
        "    if K is None or M is None:\n",
        "         raise ValueError(\"K and M must be specified in param_gen_kwargs\")\n",
        "\n",
        "    print(f\"\\n--- Starting Full Results Analysis over Alpha Range [{alpha_range[0]:.4f}, {alpha_range[-1]:.4f}] ---\")\n",
        "    print(f\"--- (Using Active Set Method, mu_tilde={mu_tilde}) ---\")\n",
        "    total_alphas = len(alpha_range)\n",
        "    start_loop_time = time.time()\n",
        "    successful_runs = 0\n",
        "    failed_alphas = []\n",
        "\n",
        "    for idx, alpha in enumerate(alpha_range):\n",
        "        loop_start_time = time.time()\n",
        "        print(f\"\\rAnalyzing alpha = {alpha:.6f} ({idx+1}/{total_alphas}) ... \", end=\"\")\n",
        "\n",
        "        alpha_result = {'alpha': alpha}\n",
        "        fw_success = False\n",
        "        fw_message = \"Analysis not run\"\n",
        "        w_fw_default = np.full(M, np.nan)\n",
        "        H_star_fw = np.nan\n",
        "        pi_opt = np.full(K, np.nan)\n",
        "        grad_H_opt = np.full(M, np.nan)\n",
        "        lambda_opt = np.full(M, np.nan)\n",
        "        active_set_opt = tuple()\n",
        "        fw_gap_final = np.nan\n",
        "        iterations_final = 0\n",
        "\n",
        "        try:\n",
        "            # ===  ===\n",
        "            R_alpha, SecondMoments_alpha_array = generate_params_profile_switching_symmetric(alpha, **param_gen_kwargs)\n",
        "            # Check parameter validity immediately\n",
        "            if not np.all(np.isfinite(R_alpha)) or not np.all(np.isfinite(SecondMoments_alpha_array)):\n",
        "                 raise ValueError(\"Parameter generation resulted in NaN/Inf.\")\n",
        "\n",
        "            # --- FW Optimizer (Active Set) ---\n",
        "            fw_result = frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde,\n",
        "                                              initial_w=None, return_history=False,\n",
        "                                              debug_print=False, # Keep debug off for loops\n",
        "                                              **optimizer_kwargs)\n",
        "\n",
        "            fw_success = fw_result.success\n",
        "            fw_message = fw_result.message\n",
        "            iterations_final = fw_result.iterations if fw_result.iterations is not None else 0\n",
        "\n",
        "            if fw_result.success:\n",
        "                 w_fw_default = fw_result.w_opt if fw_result.w_opt is not None else np.full(M, np.nan)\n",
        "                 H_star_fw = fw_result.H_opt if fw_result.H_opt is not None else np.nan\n",
        "                 pi_opt = fw_result.pi_opt if fw_result.pi_opt is not None else np.full(K, np.nan)\n",
        "                 grad_H_opt = fw_result.grad_H_opt if fw_result.grad_H_opt is not None else np.full(M, np.nan)\n",
        "                 lambda_opt = fw_result.lambda_opt if fw_result.lambda_opt is not None else np.full(M, np.nan)\n",
        "                 active_set_opt = fw_result.active_set_opt if fw_result.active_set_opt is not None else tuple()\n",
        "                 fw_gap_final = fw_result.fw_gap if fw_result.fw_gap is not None else np.nan\n",
        "                 successful_runs += 1\n",
        "            else:\n",
        "                 # Store NaN for results if FW failed, but still record alpha\n",
        "                 w_fw_default = np.full(M, np.nan)\n",
        "                 H_star_fw = np.nan\n",
        "                 pi_opt = np.full(K, np.nan)\n",
        "                 grad_H_opt = np.full(M, np.nan)\n",
        "                 lambda_opt = np.full(M, np.nan)\n",
        "                 active_set_opt = tuple()\n",
        "                 fw_gap_final = np.nan\n",
        "                 failed_alphas.append(alpha)\n",
        "                 # Print failure message immediately\n",
        "                 print(f\"\\n  FW failed for alpha={alpha:.6f}: {fw_result.message}\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n  Error during analysis for alpha={alpha:.6f}: {e}\")\n",
        "            # Print traceback for detailed debugging\n",
        "            # traceback.print_exc()\n",
        "            fw_success = False\n",
        "            fw_message = f\"Exception: {e}\"\n",
        "            w_fw_default = np.full(M, np.nan)\n",
        "            H_star_fw = np.nan\n",
        "            pi_opt = np.full(K, np.nan)\n",
        "            grad_H_opt = np.full(M, np.nan)\n",
        "            lambda_opt = np.full(M, np.nan)\n",
        "            active_set_opt = tuple()\n",
        "            fw_gap_final = np.nan\n",
        "            failed_alphas.append(alpha)\n",
        "\n",
        "        # Store results for this alpha value\n",
        "        alpha_result['success'] = fw_success\n",
        "        alpha_result['message'] = fw_message\n",
        "        alpha_result['w_fw_default'] = w_fw_default\n",
        "        alpha_result['H_star'] = H_star_fw\n",
        "        alpha_result['pi_opt'] = pi_opt\n",
        "        alpha_result['grad_H_opt'] = grad_H_opt\n",
        "        alpha_result['lambda_opt'] = lambda_opt\n",
        "        alpha_result['active_set_opt'] = active_set_opt\n",
        "        alpha_result['fw_gap'] = fw_gap_final\n",
        "        alpha_result['iterations'] = iterations_final\n",
        "\n",
        "        results_over_alpha.append(alpha_result)\n",
        "        loop_end_time = time.time()\n",
        "        # Optional: print timing per loop\n",
        "        # print(f\" (took {loop_end_time - loop_start_time:.2f}s)\")\n",
        "\n",
        "    # --- Post-loop summary ---\n",
        "    end_loop_time = time.time()\n",
        "    print(f\"\\n--- Finished Full Results Analysis ({total_alphas} points) ---\")\n",
        "    print(f\"Total time: {end_loop_time - start_loop_time:.2f} seconds\")\n",
        "    print(f\"Successful runs: {successful_runs}/{total_alphas}\")\n",
        "    if failed_alphas:\n",
        "        print(f\"Failed alphas: {failed_alphas}\")\n",
        "\n",
        "    # --- Convert to DataFrame ---\n",
        "    if not PANDAS_AVAILABLE:\n",
        "        print(\"Pandas not available. Returning results as list of dictionaries.\")\n",
        "        return results_over_alpha\n",
        "\n",
        "    df_results = pd.DataFrame(results_over_alpha)\n",
        "\n",
        "    # Expand vector columns if they exist and are not all NaN\n",
        "    if 'w_fw_default' in df_results.columns:\n",
        "        fw_vecs = np.stack(df_results['w_fw_default'].fillna(pd.Series([np.full(M, np.nan)] * len(df_results))).values)\n",
        "        if np.any(np.isfinite(fw_vecs)):\n",
        "            for m in range(M): df_results[f'w_fw_{m}'] = fw_vecs[:, m]\n",
        "\n",
        "    if 'pi_opt' in df_results.columns:\n",
        "        pi_vecs = np.stack(df_results['pi_opt'].fillna(pd.Series([np.full(K, np.nan)] * len(df_results))).values)\n",
        "        if np.any(np.isfinite(pi_vecs)):\n",
        "            for k in range(K): df_results[f'pi_{k}'] = pi_vecs[:, k]\n",
        "\n",
        "    if 'grad_H_opt' in df_results.columns:\n",
        "        grad_vecs = np.stack(df_results['grad_H_opt'].fillna(pd.Series([np.full(M, np.nan)] * len(df_results))).values)\n",
        "        if np.any(np.isfinite(grad_vecs)):\n",
        "            for m in range(M): df_results[f'grad_H_{m}'] = grad_vecs[:, m]\n",
        "\n",
        "    if 'lambda_opt' in df_results.columns:\n",
        "        lambda_vecs = np.stack(df_results['lambda_opt'].fillna(pd.Series([np.full(M, np.nan)] * len(df_results))).values)\n",
        "        if np.any(np.isfinite(lambda_vecs)):\n",
        "            for m in range(M): df_results[f'lambda_{m}'] = lambda_vecs[:, m]\n",
        "\n",
        "    # Convert active set tuple to string for display/CSV\n",
        "    if 'active_set_opt' in df_results.columns:\n",
        "        df_results['active_set'] = df_results['active_set_opt'].apply(lambda x: str(x) if x is not None else '()')\n",
        "\n",
        "    # Drop original vector/tuple columns\n",
        "    cols_to_drop = ['w_fw_default', 'pi_opt', 'grad_H_opt', 'lambda_opt', 'active_set_opt']\n",
        "    df_results = df_results.drop(columns=[col for col in cols_to_drop if col in df_results.columns], errors='ignore')\n",
        "\n",
        "    return df_results\n",
        "\n",
        "\n",
        "# ===  ===\n",
        "if __name__ == '__main__':\n",
        "    start_time_main = time.time()\n",
        "    # Set warning filters\n",
        "    warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered') # Ignore common numpy warnings\n",
        "    warnings.filterwarnings('ignore', category=UserWarning) # Ignore other user warnings\n",
        "    warnings.filterwarnings('ignore', category=OptimizeWarning) # Ignore SciPy Optimize warnings\n",
        "\n",
        "    # ---  ---\n",
        "    K = 5; M = 3;\n",
        "    #  mu_tilde  0.025  \n",
        "    mu_tilde = 0.025\n",
        "    alpha_max = 1.5\n",
        "\n",
        "    #   \n",
        "    # (generate_params_profile_switching_symmetric  Vm )\n",
        "    param_gen_kwargs_quasi_symmetric = {\n",
        "        'K': K, 'M': M, 'alpha_max': alpha_max,\n",
        "        'R_base_sym': np.array([0.02, 0.01, 0.0, -0.01, -0.02]), # Neutral=0\n",
        "        'sigma_base': np.array([0.18, 0.15, 0.20, 0.12, 0.10]),\n",
        "        'Corr_base': np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]),\n",
        "        'r_offset': 0.03,\n",
        "        's_factor_g': 0.001,  # Vm \n",
        "        's_factor_b': -0.001, # Vm \n",
        "        'sigma_min_epsilon': 1e-4,\n",
        "        'psd_tolerance': 1e-9\n",
        "    }\n",
        "\n",
        "    # FW Optimizer  Inner QP  ()\n",
        "    solver_settings = {\n",
        "        'max_outer_iter': 500, 'fw_gap_tol': 1e-9,\n",
        "        'inner_max_iter': 600, # Increase inner iterations slightly\n",
        "        'tolerance': 1e-11,   # Numerical tolerances\n",
        "        'psd_make_tolerance': 1e-9,\n",
        "        'qp_regularization': 1e-10, # Regularization for inner QP\n",
        "        'force_iterations': 50 # Run at least 50 iterations before checking FW gap\n",
        "    }\n",
        "\n",
        "    #   alpha  (0.75 ) \n",
        "    alpha_start = 0.700\n",
        "    alpha_end = 0.800\n",
        "    num_alpha_steps = 101 #  0.001 \n",
        "    alpha_range_analyze = np.linspace(alpha_start, alpha_end, num_alpha_steps)\n",
        "\n",
        "    # KKT\n",
        "    unique_pt_tolerance = 1e-4 # Threshold to consider w_m > 0 for KKT check\n",
        "\n",
        "    # \n",
        "    output_csv_filename = f\"alpha_internal_sol_search_{alpha_start:.3f}_{alpha_end:.3f}_activeset_mu{int(mu_tilde*1000):03d}_quasi_sym.csv\"\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"--- Searching for Internal Solution around alpha=0.75 [{alpha_start:.3f}, {alpha_end:.3f}] ---\")\n",
        "    print(f\"--- (Using ActiveSet, mu_tilde={mu_tilde}, Quasi-Symmetric Params) ---\")\n",
        "    print(f\"--- Output CSV: {output_csv_filename} ---\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # ---  ---\n",
        "    results_df = analyze_alpha_full_results(\n",
        "        alpha_range=alpha_range_analyze,\n",
        "        param_gen_kwargs=param_gen_kwargs_quasi_symmetric, # <-- \n",
        "        optimizer_kwargs=solver_settings,\n",
        "        mu_tilde=mu_tilde # <--  mu_tilde \n",
        "    )\n",
        "\n",
        "    # ---  & CSV & KKT ---\n",
        "    if PANDAS_AVAILABLE and isinstance(results_df, pd.DataFrame) and not results_df.empty:\n",
        "        print(\"\\n--- Analysis Results Summary (DataFrame) ---\")\n",
        "        # Select and order columns for display\n",
        "        cols_display = ['alpha', 'success', 'H_star', 'fw_gap', 'iterations', 'active_set']\n",
        "        cols_w = [f'w_fw_{m}' for m in range(M) if f'w_fw_{m}' in results_df.columns]\n",
        "        cols_pi = [f'pi_{k}' for k in range(K) if f'pi_{k}' in results_df.columns]\n",
        "        cols_grad = [f'grad_H_{m}' for m in range(M) if f'grad_H_{m}' in results_df.columns]\n",
        "        cols_lam = [f'lambda_{m}' for m in range(M) if f'lambda_{m}' in results_df.columns]\n",
        "        cols_to_show = cols_display + cols_w + cols_grad #+ cols_pi + cols_lam # Keep output concise\n",
        "        cols_to_show = [col for col in cols_to_show if col in results_df.columns]\n",
        "\n",
        "        # Display settings\n",
        "        float_format_func = lambda x: f\"{x:.4e}\" if pd.notna(x) and isinstance(x, (float, np.number)) else x\n",
        "        pd.options.display.float_format = float_format_func\n",
        "        # Print the results DataFrame to console\n",
        "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 200):\n",
        "             print(results_df[cols_to_show].to_string(index=False, na_rep='NaN'))\n",
        "        pd.reset_option('display.float_format') # Reset float format\n",
        "\n",
        "        # --- CSV ---\n",
        "        try:\n",
        "            # Order columns for CSV saving (more comprehensive)\n",
        "            cols_csv_order = ['alpha', 'success', 'message', 'H_star', 'fw_gap', 'iterations', 'active_set'] + \\\n",
        "                             cols_w + cols_pi + cols_grad + cols_lam\n",
        "            cols_csv_order = [col for col in cols_csv_order if col in results_df.columns]\n",
        "            df_to_save = results_df[cols_csv_order]\n",
        "\n",
        "            df_to_save.to_csv(output_csv_filename, index=False, float_format='%.8e')\n",
        "            print(f\"\\nFull detailed results ({len(df_to_save)} points) saved to: {os.path.abspath(output_csv_filename)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError saving results to CSV: {e}\")\n",
        "        # --- End CSV Save ---\n",
        "\n",
        "        # --- KKT () ---\n",
        "        print(\"\\n--- Quick KKT Check (Stationarity w.r.t. w) ---\")\n",
        "        kkt_violations = []\n",
        "        check_tol = solver_settings.get('tolerance', 1e-9) * 1e3 # Tolerance for KKT checks\n",
        "\n",
        "        for index, row in results_df.iterrows():\n",
        "            alpha = row['alpha']\n",
        "            if not row.get('success', False): # Skip failed runs\n",
        "                 kkt_violations.append({\n",
        "                    'alpha': alpha, 'active_set_report': row.get('active_set', 'N/A'), 'active_set_kkt': 'N/A',\n",
        "                    'max_violation': np.nan, 'grad_consistency_violation': np.nan, 'check_result': 'FW Fail'\n",
        "                 })\n",
        "                 continue\n",
        "\n",
        "            w_star = np.array([row.get(f'w_fw_{m}', np.nan) for m in range(M)])\n",
        "            grad_h = np.array([row.get(f'grad_H_{m}', np.nan) for m in range(M)])\n",
        "            active_set_str = row.get('active_set', '()')\n",
        "\n",
        "            # Check if w* or grad_h are valid\n",
        "            if np.isnan(w_star).any() or np.isnan(grad_h).any() or np.isclose(np.sum(w_star), 0):\n",
        "                kkt_violations.append({\n",
        "                    'alpha': alpha, 'active_set_report': active_set_str, 'active_set_kkt': 'NaN',\n",
        "                    'max_violation': np.nan, 'grad_consistency_violation': np.nan, 'check_result': 'Skipped (NaN/zero w/grad)'\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Identify numerically active weights for KKT check\n",
        "            active_indices_kkt = set(m for m in range(M) if w_star[m] > unique_pt_tolerance)\n",
        "\n",
        "            # Check 1: Consistency of gradients for active weights\n",
        "            grad_consistency_violation = 0.0\n",
        "            if len(active_indices_kkt) > 1:\n",
        "                 active_grads = grad_h[list(active_indices_kkt)]\n",
        "                 grad_consistency_violation = np.max(active_grads) - np.min(active_grads)\n",
        "\n",
        "            # Check 2: Optimality condition for inactive weights\n",
        "            # Estimate nu* = max(active_grads) or max(all grads) if none active\n",
        "            nu_estimate = np.max(grad_h[list(active_indices_kkt)]) if active_indices_kkt else np.max(grad_h)\n",
        "            stationarity_violation = grad_consistency_violation # Start with consistency violation\n",
        "\n",
        "            for m in range(M):\n",
        "                if m not in active_indices_kkt:\n",
        "                    # Check if grad_h[m] <= nu_estimate (within tolerance)\n",
        "                    violation = grad_h[m] - nu_estimate\n",
        "                    if violation > check_tol: # If inactive grad is significantly larger than nu_estimate\n",
        "                        stationarity_violation = max(stationarity_violation, violation)\n",
        "\n",
        "            # Check Result based on violations\n",
        "            is_internal = len(active_indices_kkt) == M\n",
        "            result_str = 'OK'\n",
        "            if stationarity_violation > check_tol * 10: result_str = 'WARN (KKT Viol)'\n",
        "            if is_internal: result_str += ' [INTERNAL]'\n",
        "            elif len(active_indices_kkt) == 1: result_str += ' [VERTEX]'\n",
        "            elif len(active_indices_kkt) == 2: result_str += ' [EDGE]'\n",
        "\n",
        "\n",
        "            kkt_violations.append({\n",
        "                'alpha': alpha,\n",
        "                'active_set_report': active_set_str,\n",
        "                'active_set_kkt': str(tuple(sorted(active_indices_kkt))),\n",
        "                'max_violation': stationarity_violation,\n",
        "                'grad_consistency_violation': grad_consistency_violation,\n",
        "                'check_result': result_str\n",
        "            })\n",
        "\n",
        "        # Display KKT Check Results\n",
        "        if PANDAS_AVAILABLE and kkt_violations:\n",
        "            df_kkt = pd.DataFrame(kkt_violations)\n",
        "            kkt_cols_order = ['alpha', 'active_set_kkt', 'max_violation', 'grad_consistency_violation', 'check_result']\n",
        "            #kkt_cols_order = ['alpha', 'active_set_report','active_set_kkt', 'max_violation', 'grad_consistency_violation', 'check_result'] # More verbose option\n",
        "            kkt_cols_order = [col for col in kkt_cols_order if col in df_kkt.columns]\n",
        "            with pd.option_context('display.float_format', '{:.4e}'.format, 'display.max_rows', None): # All rows\n",
        "                print(df_kkt[kkt_cols_order].to_string(index=False, na_rep='NaN'))\n",
        "            print(\"\\nNotes on KKT Check:\")\n",
        "            # print(\" - active_set_report: Active set reported by inner solver for pi*\") # Verbose option\n",
        "            print(\" - active_set_kkt: Active set inferred from w* > tolerance for outer problem\")\n",
        "            print(\" - max_violation: Max KKT violation (max(grad_consistency, max(grad_inactive - nu_est)))\")\n",
        "            print(\" - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\")\n",
        "            print(\" - check_result: OK/WARN indicates KKT satisfaction; [INTERNAL/EDGE/VERTEX] indicates solution type.\")\n",
        "        else:\n",
        "            print(\"Could not perform KKT check or pandas not available.\")\n",
        "        # --- End KKT Check ---\n",
        "\n",
        "    elif isinstance(results_df, list) or (isinstance(results_df, pd.DataFrame) and results_df.empty):\n",
        "        print(\"\\n--- Analysis Results Summary ---\")\n",
        "        print(\"No results generated or DataFrame is empty.\")\n",
        "        if not PANDAS_AVAILABLE: print(\"(Pandas not available, skipping CSV export)\")\n",
        "\n",
        "    end_time_main = time.time()\n",
        "    print(f\"\\nTotal execution time: {end_time_main - start_time_main:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4RZcwU_3qNl",
        "outputId": "63d772b5-df09-42a9-b5ef-1ff7d2929685"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "--- Searching for Internal Solution around alpha=0.75 [0.700, 0.800] ---\n",
            "--- (Using ActiveSet, mu_tilde=0.025, Quasi-Symmetric Params) ---\n",
            "--- Output CSV: alpha_internal_sol_search_0.700_0.800_activeset_mu025_quasi_sym.csv ---\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "--- Starting Full Results Analysis over Alpha Range [0.7000, 0.8000] ---\n",
            "--- (Using Active Set Method, mu_tilde=0.025) ---\n",
            "\rAnalyzing alpha = 0.700000 (1/101) ... \n",
            "  FW failed for alpha=0.700000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "\rAnalyzing alpha = 0.701000 (2/101) ... \n",
            "  FW failed for alpha=0.701000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "\rAnalyzing alpha = 0.702000 (3/101) ... \n",
            "  FW failed for alpha=0.702000: Outer iter 9: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "\rAnalyzing alpha = 0.703000 (4/101) ... \n",
            "  FW failed for alpha=0.703000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "\rAnalyzing alpha = 0.704000 (5/101) ... \n",
            "  FW failed for alpha=0.704000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "\rAnalyzing alpha = 0.705000 (6/101) ... \n",
            "  FW failed for alpha=0.705000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "\rAnalyzing alpha = 0.706000 (7/101) ... \n",
            "  FW failed for alpha=0.706000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "\rAnalyzing alpha = 0.707000 (8/101) ... \n",
            "  FW failed for alpha=0.707000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.708000 (9/101) ... \n",
            "  FW failed for alpha=0.708000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.709000 (10/101) ... \n",
            "  FW failed for alpha=0.709000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.710000 (11/101) ... \n",
            "  FW failed for alpha=0.710000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.711000 (12/101) ... \n",
            "  FW failed for alpha=0.711000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.712000 (13/101) ... \n",
            "  FW failed for alpha=0.712000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.713000 (14/101) ... \n",
            "  FW failed for alpha=0.713000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.714000 (15/101) ... \n",
            "  FW failed for alpha=0.714000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.715000 (16/101) ... \n",
            "  FW failed for alpha=0.715000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.716000 (17/101) ... \n",
            "  FW failed for alpha=0.716000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.717000 (18/101) ... \n",
            "  FW failed for alpha=0.717000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.718000 (19/101) ... \n",
            "  FW failed for alpha=0.718000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.719000 (20/101) ... \n",
            "  FW failed for alpha=0.719000: Outer iter 13: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.720000 (21/101) ... \n",
            "  FW failed for alpha=0.720000: Outer iter 5: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.721000 (22/101) ... \n",
            "  FW failed for alpha=0.721000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.722000 (23/101) ... \n",
            "  FW failed for alpha=0.722000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.723000 (24/101) ... \n",
            "  FW failed for alpha=0.723000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.724000 (25/101) ... \n",
            "  FW failed for alpha=0.724000: Outer iter 9: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.725000 (26/101) ... \n",
            "  FW failed for alpha=0.725000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.726000 (27/101) ... \n",
            "  FW failed for alpha=0.726000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.727000 (28/101) ... \n",
            "  FW failed for alpha=0.727000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.728000 (29/101) ... \n",
            "  FW failed for alpha=0.728000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.729000 (30/101) ... \n",
            "  FW failed for alpha=0.729000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.730000 (31/101) ... \n",
            "  FW failed for alpha=0.730000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.731000 (32/101) ... \n",
            "  FW failed for alpha=0.731000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.732000 (33/101) ... \n",
            "  FW failed for alpha=0.732000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.733000 (34/101) ... \n",
            "  FW failed for alpha=0.733000: Outer iter 7: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.734000 (35/101) ... \n",
            "  FW failed for alpha=0.734000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.735000 (36/101) ... \n",
            "  FW failed for alpha=0.735000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.736000 (37/101) ... \n",
            "  FW failed for alpha=0.736000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.737000 (38/101) ... \n",
            "  FW failed for alpha=0.737000: Outer iter 6: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.738000 (39/101) ... \n",
            "  FW failed for alpha=0.738000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.739000 (40/101) ... \n",
            "  FW failed for alpha=0.739000: Outer iter 6: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.740000 (41/101) ... \n",
            "  FW failed for alpha=0.740000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.741000 (42/101) ... \n",
            "  FW failed for alpha=0.741000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.742000 (43/101) ... \n",
            "  FW failed for alpha=0.742000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.743000 (44/101) ... \n",
            "  FW failed for alpha=0.743000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.744000 (45/101) ... \n",
            "  FW failed for alpha=0.744000: Outer iter 5: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.745000 (46/101) ... \n",
            "  FW failed for alpha=0.745000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.746000 (47/101) ... \n",
            "  FW failed for alpha=0.746000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.747000 (48/101) ... \n",
            "  FW failed for alpha=0.747000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.748000 (49/101) ... \n",
            "  FW failed for alpha=0.748000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.749000 (50/101) ... \n",
            "  FW failed for alpha=0.749000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.750000 (51/101) ... \n",
            "  FW failed for alpha=0.750000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.751000 (52/101) ... \n",
            "  FW failed for alpha=0.751000: Outer iter 11: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.752000 (53/101) ... \n",
            "  FW failed for alpha=0.752000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.753000 (54/101) ... \n",
            "  FW failed for alpha=0.753000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.754000 (55/101) ... \n",
            "  FW failed for alpha=0.754000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.755000 (56/101) ... \n",
            "  FW failed for alpha=0.755000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.756000 (57/101) ... \n",
            "  FW failed for alpha=0.756000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.757000 (58/101) ... \n",
            "  FW failed for alpha=0.757000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.758000 (59/101) ... \n",
            "  FW failed for alpha=0.758000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.759000 (60/101) ... \n",
            "  FW failed for alpha=0.759000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.760000 (61/101) ... \n",
            "  FW failed for alpha=0.760000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.761000 (62/101) ... \n",
            "  FW failed for alpha=0.761000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.762000 (63/101) ... \n",
            "  FW failed for alpha=0.762000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.763000 (64/101) ... \n",
            "  FW failed for alpha=0.763000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.764000 (65/101) ... \n",
            "  FW failed for alpha=0.764000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.765000 (66/101) ... \n",
            "  FW failed for alpha=0.765000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.766000 (67/101) ... \n",
            "  FW failed for alpha=0.766000: Outer iter 6: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.767000 (68/101) ... \n",
            "  FW failed for alpha=0.767000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.768000 (69/101) ... \n",
            "  FW failed for alpha=0.768000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.769000 (70/101) ... \n",
            "  FW failed for alpha=0.769000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.770000 (71/101) ... \n",
            "  FW failed for alpha=0.770000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.771000 (72/101) ... \n",
            "  FW failed for alpha=0.771000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.772000 (73/101) ... \n",
            "  FW failed for alpha=0.772000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.773000 (74/101) ... \n",
            "  FW failed for alpha=0.773000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.774000 (75/101) ... \n",
            "  FW failed for alpha=0.774000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.775000 (76/101) ... \n",
            "  FW failed for alpha=0.775000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.776000 (77/101) ... \n",
            "  FW failed for alpha=0.776000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.777000 (78/101) ... \n",
            "  FW failed for alpha=0.777000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.778000 (79/101) ... \n",
            "  FW failed for alpha=0.778000: Outer iter 5: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.779000 (80/101) ... \n",
            "  FW failed for alpha=0.779000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.780000 (81/101) ... \n",
            "  FW failed for alpha=0.780000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.781000 (82/101) ... \n",
            "  FW failed for alpha=0.781000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.782000 (83/101) ... \n",
            "  FW failed for alpha=0.782000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.783000 (84/101) ... \n",
            "  FW failed for alpha=0.783000: Outer iter 5: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.784000 (85/101) ... \n",
            "  FW failed for alpha=0.784000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.785000 (86/101) ... \n",
            "  FW failed for alpha=0.785000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.786000 (87/101) ... \n",
            "  FW failed for alpha=0.786000: Outer iter 6: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.787000 (88/101) ... \n",
            "  FW failed for alpha=0.787000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.788000 (89/101) ... \n",
            "  FW failed for alpha=0.788000: Outer iter 3: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.789000 (90/101) ... \n",
            "  FW failed for alpha=0.789000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.790000 (91/101) ... \n",
            "  FW failed for alpha=0.790000: Outer iter 4: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.791000 (92/101) ... \n",
            "  FW failed for alpha=0.791000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.792000 (93/101) ... \n",
            "  FW failed for alpha=0.792000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.793000 (94/101) ... \n",
            "  FW failed for alpha=0.793000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.794000 (95/101) ... \n",
            "  FW failed for alpha=0.794000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.795000 (96/101) ... \n",
            "  FW failed for alpha=0.795000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.796000 (97/101) ... \n",
            "  FW failed for alpha=0.796000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.797000 (98/101) ... \n",
            "  FW failed for alpha=0.797000: Outer iter 5: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.798000 (99/101) ... \n",
            "  FW failed for alpha=0.798000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.799000 (100/101) ... \n",
            "  FW failed for alpha=0.799000: Outer iter 9: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "Analyzing alpha = 0.800000 (101/101) ... \n",
            "  FW failed for alpha=0.800000: Outer iter 2: Inner QP failed: QP fail:Iter 1: KKT solve failed. ActiveSet=[0, 1, 2]. Cond(Q)=1.2e+01. Stopping.\n",
            "\n",
            "--- Finished Full Results Analysis (101 points) ---\n",
            "Total time: 1.51 seconds\n",
            "Successful runs: 0/101\n",
            "Failed alphas: [np.float64(0.7), np.float64(0.701), np.float64(0.702), np.float64(0.703), np.float64(0.704), np.float64(0.705), np.float64(0.706), np.float64(0.707), np.float64(0.708), np.float64(0.709), np.float64(0.71), np.float64(0.711), np.float64(0.712), np.float64(0.713), np.float64(0.714), np.float64(0.715), np.float64(0.716), np.float64(0.717), np.float64(0.718), np.float64(0.719), np.float64(0.72), np.float64(0.721), np.float64(0.722), np.float64(0.723), np.float64(0.724), np.float64(0.725), np.float64(0.726), np.float64(0.727), np.float64(0.728), np.float64(0.729), np.float64(0.73), np.float64(0.731), np.float64(0.732), np.float64(0.733), np.float64(0.734), np.float64(0.735), np.float64(0.736), np.float64(0.737), np.float64(0.738), np.float64(0.739), np.float64(0.74), np.float64(0.741), np.float64(0.742), np.float64(0.743), np.float64(0.744), np.float64(0.745), np.float64(0.746), np.float64(0.747), np.float64(0.748), np.float64(0.749), np.float64(0.75), np.float64(0.751), np.float64(0.752), np.float64(0.753), np.float64(0.754), np.float64(0.755), np.float64(0.756), np.float64(0.757), np.float64(0.758), np.float64(0.759), np.float64(0.76), np.float64(0.761), np.float64(0.762), np.float64(0.763), np.float64(0.764), np.float64(0.765), np.float64(0.766), np.float64(0.767), np.float64(0.768), np.float64(0.769), np.float64(0.77), np.float64(0.771), np.float64(0.772), np.float64(0.773), np.float64(0.774), np.float64(0.775), np.float64(0.776), np.float64(0.777), np.float64(0.778), np.float64(0.779), np.float64(0.78), np.float64(0.781), np.float64(0.782), np.float64(0.783), np.float64(0.784), np.float64(0.785), np.float64(0.786), np.float64(0.787), np.float64(0.788), np.float64(0.789), np.float64(0.79), np.float64(0.791), np.float64(0.792), np.float64(0.793), np.float64(0.794), np.float64(0.795), np.float64(0.796), np.float64(0.797), np.float64(0.798), np.float64(0.799), np.float64(0.8)]\n",
            "\n",
            "--- Analysis Results Summary (DataFrame) ---\n",
            "     alpha  success  H_star  fw_gap  iterations active_set\n",
            "7.0000e-01    False     NaN     NaN           1         ()\n",
            "7.0100e-01    False     NaN     NaN           1         ()\n",
            "7.0200e-01    False     NaN     NaN           8         ()\n",
            "7.0300e-01    False     NaN     NaN           2         ()\n",
            "7.0400e-01    False     NaN     NaN           1         ()\n",
            "7.0500e-01    False     NaN     NaN           1         ()\n",
            "7.0600e-01    False     NaN     NaN           3         ()\n",
            "7.0700e-01    False     NaN     NaN           1         ()\n",
            "7.0800e-01    False     NaN     NaN           1         ()\n",
            "7.0900e-01    False     NaN     NaN           2         ()\n",
            "7.1000e-01    False     NaN     NaN           1         ()\n",
            "7.1100e-01    False     NaN     NaN           3         ()\n",
            "7.1200e-01    False     NaN     NaN           2         ()\n",
            "7.1300e-01    False     NaN     NaN           2         ()\n",
            "7.1400e-01    False     NaN     NaN           2         ()\n",
            "7.1500e-01    False     NaN     NaN           2         ()\n",
            "7.1600e-01    False     NaN     NaN           1         ()\n",
            "7.1700e-01    False     NaN     NaN           2         ()\n",
            "7.1800e-01    False     NaN     NaN           1         ()\n",
            "7.1900e-01    False     NaN     NaN          12         ()\n",
            "7.2000e-01    False     NaN     NaN           4         ()\n",
            "7.2100e-01    False     NaN     NaN           2         ()\n",
            "7.2200e-01    False     NaN     NaN           1         ()\n",
            "7.2300e-01    False     NaN     NaN           1         ()\n",
            "7.2400e-01    False     NaN     NaN           8         ()\n",
            "7.2500e-01    False     NaN     NaN           1         ()\n",
            "7.2600e-01    False     NaN     NaN           2         ()\n",
            "7.2700e-01    False     NaN     NaN           1         ()\n",
            "7.2800e-01    False     NaN     NaN           1         ()\n",
            "7.2900e-01    False     NaN     NaN           3         ()\n",
            "7.3000e-01    False     NaN     NaN           1         ()\n",
            "7.3100e-01    False     NaN     NaN           1         ()\n",
            "7.3200e-01    False     NaN     NaN           2         ()\n",
            "7.3300e-01    False     NaN     NaN           6         ()\n",
            "7.3400e-01    False     NaN     NaN           1         ()\n",
            "7.3500e-01    False     NaN     NaN           3         ()\n",
            "7.3600e-01    False     NaN     NaN           2         ()\n",
            "7.3700e-01    False     NaN     NaN           5         ()\n",
            "7.3800e-01    False     NaN     NaN           2         ()\n",
            "7.3900e-01    False     NaN     NaN           5         ()\n",
            "7.4000e-01    False     NaN     NaN           3         ()\n",
            "7.4100e-01    False     NaN     NaN           3         ()\n",
            "7.4200e-01    False     NaN     NaN           1         ()\n",
            "7.4300e-01    False     NaN     NaN           1         ()\n",
            "7.4400e-01    False     NaN     NaN           4         ()\n",
            "7.4500e-01    False     NaN     NaN           3         ()\n",
            "7.4600e-01    False     NaN     NaN           3         ()\n",
            "7.4700e-01    False     NaN     NaN           1         ()\n",
            "7.4800e-01    False     NaN     NaN           2         ()\n",
            "7.4900e-01    False     NaN     NaN           2         ()\n",
            "7.5000e-01    False     NaN     NaN           1         ()\n",
            "7.5100e-01    False     NaN     NaN          10         ()\n",
            "7.5200e-01    False     NaN     NaN           1         ()\n",
            "7.5300e-01    False     NaN     NaN           1         ()\n",
            "7.5400e-01    False     NaN     NaN           1         ()\n",
            "7.5500e-01    False     NaN     NaN           1         ()\n",
            "7.5600e-01    False     NaN     NaN           3         ()\n",
            "7.5700e-01    False     NaN     NaN           3         ()\n",
            "7.5800e-01    False     NaN     NaN           2         ()\n",
            "7.5900e-01    False     NaN     NaN           1         ()\n",
            "7.6000e-01    False     NaN     NaN           3         ()\n",
            "7.6100e-01    False     NaN     NaN           1         ()\n",
            "7.6200e-01    False     NaN     NaN           2         ()\n",
            "7.6300e-01    False     NaN     NaN           1         ()\n",
            "7.6400e-01    False     NaN     NaN           2         ()\n",
            "7.6500e-01    False     NaN     NaN           2         ()\n",
            "7.6600e-01    False     NaN     NaN           5         ()\n",
            "7.6700e-01    False     NaN     NaN           1         ()\n",
            "7.6800e-01    False     NaN     NaN           2         ()\n",
            "7.6900e-01    False     NaN     NaN           2         ()\n",
            "7.7000e-01    False     NaN     NaN           3         ()\n",
            "7.7100e-01    False     NaN     NaN           1         ()\n",
            "7.7200e-01    False     NaN     NaN           2         ()\n",
            "7.7300e-01    False     NaN     NaN           1         ()\n",
            "7.7400e-01    False     NaN     NaN           1         ()\n",
            "7.7500e-01    False     NaN     NaN           1         ()\n",
            "7.7600e-01    False     NaN     NaN           1         ()\n",
            "7.7700e-01    False     NaN     NaN           1         ()\n",
            "7.7800e-01    False     NaN     NaN           4         ()\n",
            "7.7900e-01    False     NaN     NaN           1         ()\n",
            "7.8000e-01    False     NaN     NaN           3         ()\n",
            "7.8100e-01    False     NaN     NaN           3         ()\n",
            "7.8200e-01    False     NaN     NaN           1         ()\n",
            "7.8300e-01    False     NaN     NaN           4         ()\n",
            "7.8400e-01    False     NaN     NaN           2         ()\n",
            "7.8500e-01    False     NaN     NaN           1         ()\n",
            "7.8600e-01    False     NaN     NaN           5         ()\n",
            "7.8700e-01    False     NaN     NaN           1         ()\n",
            "7.8800e-01    False     NaN     NaN           2         ()\n",
            "7.8900e-01    False     NaN     NaN           1         ()\n",
            "7.9000e-01    False     NaN     NaN           3         ()\n",
            "7.9100e-01    False     NaN     NaN           1         ()\n",
            "7.9200e-01    False     NaN     NaN           1         ()\n",
            "7.9300e-01    False     NaN     NaN           1         ()\n",
            "7.9400e-01    False     NaN     NaN           1         ()\n",
            "7.9500e-01    False     NaN     NaN           1         ()\n",
            "7.9600e-01    False     NaN     NaN           1         ()\n",
            "7.9700e-01    False     NaN     NaN           4         ()\n",
            "7.9800e-01    False     NaN     NaN           1         ()\n",
            "7.9900e-01    False     NaN     NaN           8         ()\n",
            "8.0000e-01    False     NaN     NaN           1         ()\n",
            "\n",
            "Full detailed results (101 points) saved to: /content/alpha_internal_sol_search_0.700_0.800_activeset_mu025_quasi_sym.csv\n",
            "\n",
            "--- Quick KKT Check (Stationarity w.r.t. w) ---\n",
            "     alpha active_set_kkt  max_violation  grad_consistency_violation check_result\n",
            "7.0000e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.0100e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.0200e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.0300e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.0400e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.0500e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.0600e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.0700e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.0800e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.0900e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.1000e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.1100e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.1200e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.1300e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.1400e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.1500e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.1600e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.1700e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.1800e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.1900e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.2000e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.2100e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.2200e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.2300e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.2400e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.2500e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.2600e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.2700e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.2800e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.2900e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.3000e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.3100e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.3200e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.3300e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.3400e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.3500e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.3600e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.3700e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.3800e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.3900e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.4000e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.4100e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.4200e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.4300e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.4400e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.4500e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.4600e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.4700e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.4800e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.4900e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.5000e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.5100e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.5200e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.5300e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.5400e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.5500e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.5600e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.5700e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.5800e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.5900e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.6000e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.6100e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.6200e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.6300e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.6400e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.6500e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.6600e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.6700e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.6800e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.6900e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.7000e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.7100e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.7200e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.7300e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.7400e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.7500e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.7600e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.7700e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.7800e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.7900e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.8000e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.8100e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.8200e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.8300e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.8400e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.8500e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.8600e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.8700e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.8800e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.8900e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.9000e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.9100e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.9200e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.9300e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.9400e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.9500e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.9600e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.9700e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.9800e-01            N/A            NaN                         NaN      FW Fail\n",
            "7.9900e-01            N/A            NaN                         NaN      FW Fail\n",
            "8.0000e-01            N/A            NaN                         NaN      FW Fail\n",
            "\n",
            "Notes on KKT Check:\n",
            " - active_set_kkt: Active set inferred from w* > tolerance for outer problem\n",
            " - max_violation: Max KKT violation (max(grad_consistency, max(grad_inactive - nu_est)))\n",
            " - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\n",
            " - check_result: OK/WARN indicates KKT satisfaction; [INTERNAL/EDGE/VERTEX] indicates solution type.\n",
            "\n",
            "Total execution time: 1.54 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "5_2_find_internal_solution_mu022_reg_quasi_symmetric.py\n",
        "\n",
        "Attempts to find an internal solution w* by using quasi-symmetric parameters\n",
        "around alpha=0.75, testing mu_tilde = 0.022 and increased QP regularization (1e-9).\n",
        "Uses the Active Set method for the inner QP solve.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from scipy.linalg import solve, LinAlgError, eigh, inv, qr, lstsq, pinv # Import lstsq, pinv for potential fallback\n",
        "from scipy.optimize import linprog, OptimizeWarning\n",
        "import traceback\n",
        "import time\n",
        "import itertools\n",
        "import os\n",
        "import math\n",
        "from collections import Counter # \n",
        "\n",
        "# pandas \n",
        "try:\n",
        "    import pandas as pd\n",
        "    PANDAS_AVAILABLE = True\n",
        "    pd.set_option('display.width', 180) # \n",
        "    pd.set_option('display.float_format', '{:.6e}'.format) # \n",
        "    pd.set_option('display.max_rows', 300) # \n",
        "except ImportError:\n",
        "    PANDAS_AVAILABLE = False\n",
        "    print(\"Warning: pandas library not found. Output formatting will be basic. CSV export disabled.\")\n",
        "\n",
        "# ===  ===\n",
        "DEFAULT_TOLERANCE = 1e-9 # General tolerance\n",
        "\n",
        "# --- OptimizationResult  () ---\n",
        "class OptimizationResult:\n",
        "    def __init__(self, success, message, w_opt=None, pi_opt=None, lambda_opt=None,\n",
        "                 H_opt=None, grad_H_opt=None, iterations=None, fw_gap=None,\n",
        "                 active_set_opt=None):\n",
        "        self.success = success; self.message = message; self.w_opt = w_opt; self.pi_opt = pi_opt\n",
        "        self.lambda_opt = lambda_opt; self.H_opt = H_opt; self.grad_H_opt = grad_H_opt\n",
        "        self.iterations = iterations; self.fw_gap = fw_gap\n",
        "        self.active_set_opt = active_set_opt\n",
        "\n",
        "# --- find_feasible_initial_pi  () ---\n",
        "def find_feasible_initial_pi(R, mu_tilde, K, tolerance=1e-8):\n",
        "    M = R.shape[1]; c = np.zeros(K + 1); c[K] = 1.0\n",
        "    A_ub = np.hstack((-R.T, -np.ones((M, 1)))); b_ub = -mu_tilde * np.ones(M)\n",
        "    bounds = [(None, None)] * K + [(0, None)]; opts = {'tol': tolerance, 'disp': False, 'presolve': True}\n",
        "    result = None; methods_to_try = ['highs', 'highs-ipm', 'highs-ds', 'simplex']\n",
        "    last_method_tried = 'None'\n",
        "    for method in methods_to_try:\n",
        "        last_method_tried = method\n",
        "        try:\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\", OptimizeWarning)\n",
        "                result = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=method, options=opts)\n",
        "            if result.success: break\n",
        "        except ValueError as e:\n",
        "             # print(f\"DEBUG Phase 1 ({method}) ValueError: {e}\") # Debug print\n",
        "             continue\n",
        "        except Exception as e:\n",
        "            # print(f\"\\n  Phase 1 LP Exception ({method}): {e}\") # Debug print\n",
        "            return None, False, f\"Phase 1 LP failed ({method}): {e}\"\n",
        "    if result is None or not result.success:\n",
        "        msg = result.message if result else \"No solver succeeded\"\n",
        "        status = result.status if result else -1\n",
        "        # Add more context to the failure message\n",
        "        max_expected_return = -np.inf\n",
        "        try: # Estimate max return with equal weights if possible\n",
        "            if K > 0 and M > 0 and R is not None and np.all(np.isfinite(R)):\n",
        "                 equal_pi = np.ones(K) / K\n",
        "                 expected_returns = equal_pi @ R\n",
        "                 if np.all(np.isfinite(expected_returns)):\n",
        "                      max_expected_return = np.max(expected_returns)\n",
        "        except: pass # Ignore errors during estimation\n",
        "\n",
        "        fail_reason = f\"Phase 1 LP solver failed: {msg} (status={status}, method={last_method_tried})\"\n",
        "        if np.isfinite(max_expected_return) and mu_tilde > max_expected_return + tolerance: # Check if mu_tilde might be too high\n",
        "             fail_reason += f\" - mu_tilde ({mu_tilde:.4f}) > max_equal_weight_return ({max_expected_return:.4f})?\"\n",
        "        return None, False, fail_reason\n",
        "\n",
        "    s = result.x[K]; pi = result.x[:K]\n",
        "    if np.isnan(pi).any(): return None, False, \"Phase 1 LP resulted in NaN values for pi.\"\n",
        "    G = -R.T; h = -mu_tilde * np.ones(M); violation = np.max(G @ pi - h)\n",
        "    feasibility_tol = tolerance * 10000\n",
        "    if s <= tolerance * 1000: # Increased tolerance for s slightly\n",
        "        if violation <= feasibility_tol:\n",
        "             return pi, True, f\"Phase 1 OK (s*={s:.1e}, max_viol={violation:.1e})\"\n",
        "        else:\n",
        "             return pi, True, f\"Phase 1 OK (s*={s:.1e}), WARN: Initial violation {violation:.1e} > {feasibility_tol:.1e}\"\n",
        "    else:\n",
        "        return None, False, f\"Phase 1 Likely Infeasible (s* = {s:.1e} > {tolerance*1000:.1e}), max_viol={violation:.1e}\"\n",
        "\n",
        "\n",
        "# --- solve_kkt_system  (KKT) ---\n",
        "def solve_kkt_system(Q, G_W, g, tolerance=DEFAULT_TOLERANCE):\n",
        "    K = Q.shape[0]; n_act = G_W.shape[0] if G_W is not None and G_W.ndim == 2 and G_W.shape[0] > 0 else 0\n",
        "    kkt_cond = np.nan # Initialize KKT condition number\n",
        "\n",
        "    # Input checks\n",
        "    if not np.all(np.isfinite(Q)) or not np.all(np.isfinite(g)): return None, None, False, np.nan\n",
        "    if n_act > 0 and (G_W is None or not np.all(np.isfinite(G_W))): return None, None, False, np.nan\n",
        "\n",
        "    if n_act == 0:\n",
        "        try:\n",
        "            # Calculate condition number before solving\n",
        "            try: kkt_cond = np.linalg.cond(Q)\n",
        "            except LinAlgError: kkt_cond = np.inf\n",
        "\n",
        "            p = solve(Q, -g, assume_a='sym', check_finite=False)\n",
        "            l = np.array([])\n",
        "        except LinAlgError: return None, None, False, kkt_cond # Return condition number even on failure\n",
        "        except ValueError: return None, None, False, kkt_cond\n",
        "        if p is None or np.isnan(p).any() or np.isinf(p).any(): return None, None, False, kkt_cond\n",
        "        res_norm = np.linalg.norm(Q @ p + g); g_norm = np.linalg.norm(g)\n",
        "        solved_ok = res_norm <= tolerance * 1e4 * (1 + g_norm)\n",
        "        return p, l, solved_ok, kkt_cond\n",
        "    else:\n",
        "        kkt_mat = None; rhs = None\n",
        "        try:\n",
        "            if G_W.ndim != 2 or G_W.shape[1] != K: return None, None, False, kkt_cond\n",
        "            kkt_mat = np.block([[Q, G_W.T], [G_W, np.zeros((n_act, n_act))]])\n",
        "            rhs = np.concatenate([-g, np.zeros(n_act)])\n",
        "            # Calculate condition number of KKT matrix (can be expensive)\n",
        "            try: kkt_cond = np.linalg.cond(kkt_mat)\n",
        "            except LinAlgError: kkt_cond = np.inf # Handle case where cond itself fails\n",
        "        except ValueError: return None, None, False, kkt_cond\n",
        "        except Exception as e: # Catch any other error during block creation\n",
        "            print(f\"DEBUG KKT block matrix creation error: {e}\")\n",
        "            return None, None, False, kkt_cond\n",
        "\n",
        "        try:\n",
        "            sol = solve(kkt_mat, rhs, assume_a='sym', check_finite=False)\n",
        "            p = sol[:K]; l = sol[K:]\n",
        "        # Keep returning kkt_cond on failure\n",
        "        except LinAlgError: return None, None, False, kkt_cond\n",
        "        except ValueError: return None, None, False, kkt_cond\n",
        "        except Exception as e:\n",
        "             print(f\"DEBUG KKT solve unexpected error: {e}\")\n",
        "             return None, None, False, kkt_cond\n",
        "\n",
        "        if sol is None or np.isnan(sol).any() or np.isinf(sol).any():\n",
        "             return None, None, False, kkt_cond\n",
        "\n",
        "        # Check residual norm after solve\n",
        "        res_norm = np.linalg.norm(kkt_mat @ sol - rhs); rhs_norm = np.linalg.norm(rhs)\n",
        "        solved_ok = res_norm <= tolerance * 1e4 * (1 + rhs_norm)\n",
        "        return p, l, solved_ok, kkt_cond\n",
        "\n",
        "\n",
        "# --- solve_inner_qp_active_set  (KKT, ActiveSet) ---\n",
        "def solve_inner_qp_active_set(Vw, R, mu_tilde, initial_pi, max_iter=350, tolerance=DEFAULT_TOLERANCE, regularization_epsilon=1e-10):\n",
        "    K = Vw.shape[0]; M = R.shape[1];\n",
        "    if not np.all(np.isfinite(Vw)): return None, None, None, None, None, False, \"QP fail: Vw contains NaN/Inf.\"\n",
        "\n",
        "    # Regularization (using the increased value passed from solver_settings)\n",
        "    Q_reg = 2 * Vw + 2 * regularization_epsilon * np.eye(K)\n",
        "    q_cond = np.inf\n",
        "    try: q_cond = np.linalg.cond(Q_reg)\n",
        "    except LinAlgError: pass # Keep q_cond as inf if cond fails\n",
        "    if q_cond > 1 / tolerance:\n",
        "        warnings.warn(f\"QP Warning: Condition number of Q_reg is high ({q_cond:.2e}). Regularization epsilon={regularization_epsilon:.1e}\")\n",
        "\n",
        "    G = -R.T; h = -mu_tilde * np.ones(M)\n",
        "\n",
        "    if initial_pi is None or not np.all(np.isfinite(initial_pi)):\n",
        "        return None, None, None, None, None, False, \"QP fail: Initial pi is None or contains NaN/Inf.\"\n",
        "\n",
        "    pi_k = np.copy(initial_pi)\n",
        "    lam_opt = np.zeros(M)\n",
        "    W = set() # Active set\n",
        "\n",
        "    active_tol = tolerance * 100 # Tolerance for constraint activation\n",
        "\n",
        "    # --- Initial Active Set Determination ---\n",
        "    initial_violations = G @ pi_k - h\n",
        "    max_initial_violation = np.max(initial_violations)\n",
        "    # If initial pi is significantly infeasible, start with only clearly violated constraints active\n",
        "    if max_initial_violation > active_tol * 10:\n",
        "        warnings.warn(f\"QP Warning: Initial pi infeasible (max viol: {max_initial_violation:.2e} > {active_tol * 10:.1e}).\")\n",
        "        # Only activate constraints violated by more than active_tol\n",
        "        W = set(j for j, viol in enumerate(initial_violations) if viol > active_tol)\n",
        "        # print(f\"DEBUG: Infeasible initial pi. Starting Active Set: {sorted(list(W))}\")\n",
        "    else:\n",
        "        # If initial pi is feasible or near feasible, activate constraints \"close\" to the boundary\n",
        "        W = set(j for j, viol in enumerate(initial_violations) if viol > -active_tol)\n",
        "    # --- End Initial Active Set Determination ---\n",
        "\n",
        "    active_indices_opt = None\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        g_k = Q_reg @ pi_k\n",
        "        if not np.all(np.isfinite(g_k)): return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Gradient g_k contains NaN/Inf.\"\n",
        "\n",
        "        act = sorted(list(W))\n",
        "        n_act = len(act)\n",
        "        G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "\n",
        "        # Solve KKT system\n",
        "        p_k, lam_Wk, solved, kkt_cond = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "\n",
        "        if not solved or p_k is None:\n",
        "            # Include condition numbers in the failure message\n",
        "            return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: KKT solve failed. ActiveSet={act}. Q_cond={q_cond:.1e}, KKT_cond={kkt_cond:.1e}\"\n",
        "\n",
        "        p_norm = np.linalg.norm(p_k); pi_k_norm = np.linalg.norm(pi_k)\n",
        "        # Check convergence: If search direction p_k is close to zero\n",
        "        if p_norm <= tolerance * 100 * (1 + pi_k_norm):\n",
        "            is_optimal_point = True; blocking_constraint_idx = -1; min_negative_lambda = float('inf')\n",
        "            dual_feas_tol = -tolerance * 100 # Allow slightly negative lambda\n",
        "\n",
        "            if W: # Check dual feasibility if constraints are active\n",
        "                if lam_Wk is None or len(lam_Wk) != n_act: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: lam_Wk inconsistent? ActiveSet={act}\"\n",
        "                if not np.all(np.isfinite(lam_Wk)): return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: lam_Wk contains NaN/Inf. ActiveSet={act}\"\n",
        "                lambda_map = dict(zip(act, lam_Wk))\n",
        "                for constraint_idx, lagrange_multiplier in lambda_map.items():\n",
        "                    if lagrange_multiplier < dual_feas_tol:\n",
        "                        is_optimal_point = False\n",
        "                        if lagrange_multiplier < min_negative_lambda:\n",
        "                            min_negative_lambda = lagrange_multiplier; blocking_constraint_idx = constraint_idx\n",
        "\n",
        "            if is_optimal_point: # Optimal solution found\n",
        "                lam_opt.fill(0.0)\n",
        "                if W and lam_Wk is not None:\n",
        "                     try: lam_opt[act] = np.maximum(lam_Wk, 0)\n",
        "                     except IndexError: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Index error assign lambda. ActiveSet={act}\"\n",
        "                final_infeas = np.max(G @ pi_k - h)\n",
        "                msg = f\"Optimal found at iter {i+1}.\"\n",
        "                if final_infeas > active_tol: msg += f\" (WARN: Final violation {final_infeas:.1e})\"\n",
        "                active_indices_opt = act\n",
        "                return pi_k, lam_opt, active_indices_opt, None, Q_reg / 2.0, True, msg\n",
        "            else: # Not optimal: remove constraint with most negative multiplier\n",
        "                if blocking_constraint_idx in W:\n",
        "                    W.remove(blocking_constraint_idx); continue\n",
        "                else: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Neg lambda idx {blocking_constraint_idx}, not in W={act}\"\n",
        "        else: # Non-zero search direction p_k: Move along p_k\n",
        "            alpha_k = 1.0; blocking_constraint_idx = -1; min_step_length = float('inf')\n",
        "            step_tol = tolerance * 100\n",
        "\n",
        "            # Find maximum step length alpha_k before hitting an inactive constraint\n",
        "            for j in range(M):\n",
        "                if j not in W:\n",
        "                    constraint_gradient_dot_p = G[j, :] @ p_k\n",
        "                    if constraint_gradient_dot_p > step_tol: # Potential to violate constraint j\n",
        "                        distance_to_boundary = h[j] - (G[j, :] @ pi_k)\n",
        "                        if abs(constraint_gradient_dot_p) > 1e-15: # Avoid division by zero\n",
        "                             alpha_j = distance_to_boundary / constraint_gradient_dot_p\n",
        "                             if np.isfinite(alpha_j) and alpha_j >= -tolerance:\n",
        "                                 step_j = max(0.0, alpha_j)\n",
        "                                 if step_j < min_step_length:\n",
        "                                     min_step_length = step_j; blocking_constraint_idx = j\n",
        "\n",
        "            alpha_k = min(1.0, min_step_length) # Determine step length\n",
        "\n",
        "            # Safeguard for tiny steps (might indicate cycling or slow progress)\n",
        "            if alpha_k * p_norm < tolerance * (1 + pi_k_norm):\n",
        "                 # warnings.warn(f\"QP Iter {i+1}: Tiny step size ({alpha_k:.1e}). May indicate slow progress or cycling.\")\n",
        "                 # Decide whether to stop or continue if step is negligible\n",
        "                 # Let's continue for now, but monitor if this causes issues.\n",
        "                 pass\n",
        "\n",
        "            # Update solution\n",
        "            pi_k += alpha_k * p_k\n",
        "            if not np.all(np.isfinite(pi_k)):\n",
        "                 return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: pi_k became NaN/Inf after step.\"\n",
        "\n",
        "            # Add blocking constraint to active set if step was blocked\n",
        "            if alpha_k < 1.0 - step_tol and blocking_constraint_idx != -1:\n",
        "                if blocking_constraint_idx not in W: W.add(blocking_constraint_idx)\n",
        "\n",
        "            continue # Continue to next iteration\n",
        "\n",
        "    # Max iterations reached\n",
        "    msg = f\"Max iter ({max_iter}) reached.\"\n",
        "    final_infeas = np.max(G @ pi_k - h)\n",
        "    if final_infeas > active_tol * 100:\n",
        "        return None, None, None, None, None, False, f\"{msg} Final infeasible ({final_infeas:.1e} > {active_tol*100:.1e}). ActiveSet={sorted(list(W))}\"\n",
        "\n",
        "    # Assess optimality at final point\n",
        "    act = sorted(list(W)); n_act = len(act)\n",
        "    G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "    is_likely_optimal = False; active_constraints_opt = act\n",
        "\n",
        "    g_k = Q_reg @ pi_k\n",
        "    if not np.all(np.isfinite(g_k)): return None, None, None, None, None, False, f\"{msg} Final gradient g_k contains NaN/Inf.\"\n",
        "\n",
        "    p_f, lam_f, solved_f, kkt_cond_f = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "    final_lambda_estimate = np.zeros(M)\n",
        "\n",
        "    if solved_f and p_f is not None and np.linalg.norm(p_f) <= tolerance * 1000 * (1 + np.linalg.norm(pi_k)):\n",
        "        if n_act > 0:\n",
        "            if lam_f is not None and len(lam_f) == n_act and np.all(np.isfinite(lam_f)):\n",
        "                 try: final_lambda_estimate[act] = lam_f\n",
        "                 except IndexError: pass\n",
        "                 active_lambdas = final_lambda_estimate[act]\n",
        "                 if np.all(active_lambdas >= -tolerance * 1000):\n",
        "                     is_likely_optimal = True; msg += \" Final KKT check approx OK.\"\n",
        "                 else: msg += f\" Final KKT check fails (dual infeasible, min_lam={np.min(active_lambdas):.1e}).\"\n",
        "            else: msg += \" Final KKT check fails (lam_f invalid).\"\n",
        "        else: is_likely_optimal = True; msg += \" Final KKT check approx OK (unconstrained).\"\n",
        "    else:\n",
        "        p_norm_f = np.linalg.norm(p_f) if p_f is not None else np.nan\n",
        "        msg += f\" Final KKT check fails (stationarity error, p_norm={p_norm_f:.1e} or solve error). KKT_cond={kkt_cond_f:.1e}\"\n",
        "\n",
        "    lam_opt = np.maximum(final_lambda_estimate, 0)\n",
        "    return pi_k, lam_opt, active_constraints_opt, None, Q_reg / 2.0, is_likely_optimal, msg\n",
        "\n",
        "\n",
        "# --- make_psd  () ---\n",
        "def make_psd(matrix, tolerance=1e-8):\n",
        "    if not np.all(np.isfinite(matrix)):\n",
        "         warnings.warn(\"make_psd: Input matrix contains NaN/Inf. Returning as is after symmetrization.\")\n",
        "         sym = (matrix + matrix.T) / 2.0\n",
        "         return sym\n",
        "    sym = (matrix + matrix.T) / 2.0\n",
        "    try:\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(sym)\n",
        "        min_eigenvalue = np.min(eigenvalues)\n",
        "        if min_eigenvalue < tolerance:\n",
        "            eigenvalues[eigenvalues < tolerance] = tolerance\n",
        "            psd_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n",
        "            return (psd_matrix + psd_matrix.T) / 2.0\n",
        "        else:\n",
        "            return sym\n",
        "    except LinAlgError:\n",
        "        warnings.warn(\"make_psd: eigh failed. Returning symmetrized input.\")\n",
        "        return sym\n",
        "\n",
        "# --- calculate_Vw  () ---\n",
        "def calculate_Vw(w, R, SecondMoments_a_array, tolerance=DEFAULT_TOLERANCE, psd_tolerance=1e-8):\n",
        "    K, M = R.shape\n",
        "    w_norm = project_to_simplex(w)\n",
        "    if not np.isclose(np.sum(w_norm), 1.0):\n",
        "         raise ValueError(f\"calculate_Vw: w does not sum to 1 after projection: sum(w)={np.sum(w_norm):.4f}\")\n",
        "    if not np.all(np.isfinite(R)): raise ValueError(\"calculate_Vw: R contains NaN/Inf.\")\n",
        "    if not np.all(np.isfinite(SecondMoments_a_array)): raise ValueError(\"calculate_Vw: SecondMoments_a_array contains NaN/Inf.\")\n",
        "    EwX = R @ w_norm\n",
        "    EwXXT = np.zeros((K, K));\n",
        "    for m in range(M): EwXXT += w_norm[m] * SecondMoments_a_array[m]\n",
        "    if not np.all(np.isfinite(EwX)): raise ValueError(\"calculate_Vw: EwX calculation resulted in NaN/Inf.\")\n",
        "    if not np.all(np.isfinite(EwXXT)): raise ValueError(\"calculate_Vw: EwXXT calculation resulted in NaN/Inf.\")\n",
        "    Vw = EwXXT - np.outer(EwX, EwX);\n",
        "    if not np.all(np.isfinite(Vw)): raise ValueError(\"calculate_Vw: Vw calculation resulted in NaN/Inf.\")\n",
        "    Vw_psd = make_psd(Vw, psd_tolerance)\n",
        "    if not np.all(np.isfinite(Vw_psd)): raise ValueError(\"calculate_Vw: Vw_psd after make_psd contains NaN/Inf.\")\n",
        "    return Vw_psd, EwX, EwXXT\n",
        "\n",
        "# --- calculate_H_gradient  () ---\n",
        "def calculate_H_gradient(pi_star, w, R, SecondMoments_a_array, EwX, EwXXT, tolerance=1e-9, debug_print=False):\n",
        "    M = w.shape[0]; K = R.shape[0]; grad = np.zeros(M)\n",
        "    norm_tolerance = 1e-12\n",
        "    if pi_star is None: return np.full(M, np.nan)\n",
        "    if not np.all(np.isfinite(pi_star)): return np.full(M, np.nan)\n",
        "    if not np.all(np.isfinite(w)): return np.full(M, np.nan)\n",
        "    if not np.all(np.isfinite(R)): return np.full(M, np.nan)\n",
        "    if not np.all(np.isfinite(SecondMoments_a_array)): return np.full(M, np.nan)\n",
        "    if EwX is None or not np.all(np.isfinite(EwX)): return np.full(M, np.nan)\n",
        "    if EwXXT is None or not np.all(np.isfinite(EwXXT)): return np.full(M, np.nan)\n",
        "    pi_norm = np.linalg.norm(pi_star)\n",
        "    if pi_norm < norm_tolerance:\n",
        "        if debug_print: print(f\"DEBUG grad_H: Ret NaN pi_norm {pi_norm:.2e} < {norm_tolerance:.1e}\")\n",
        "        return np.full(M, np.nan)\n",
        "    try:\n",
        "        pi_T_EwX = pi_star.T @ EwX\n",
        "        if not np.isfinite(pi_T_EwX):\n",
        "             if debug_print: print(f\"DEBUG grad_H: pi_T_EwX is NaN/Inf: {pi_T_EwX}\")\n",
        "             return np.full(M, np.nan)\n",
        "        for j in range(M):\n",
        "            Sigma_j = SecondMoments_a_array[j]; r_j = R[:, j]\n",
        "            pi_T_Sigma_j_pi = pi_star.T @ Sigma_j @ pi_star\n",
        "            if not np.isfinite(pi_T_Sigma_j_pi):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_Sigma_j_pi is NaN/Inf\")\n",
        "                 grad[j] = np.nan; continue\n",
        "            pi_T_r_j = pi_star.T @ r_j\n",
        "            if not np.isfinite(pi_T_r_j):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_r_j is NaN/Inf\")\n",
        "                 grad[j] = np.nan; continue\n",
        "            term2 = 2 * pi_T_r_j * pi_T_EwX\n",
        "            if not np.isfinite(term2):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): term2 is NaN/Inf\")\n",
        "                 grad[j] = np.nan; continue\n",
        "            grad[j] = pi_T_Sigma_j_pi - term2\n",
        "            if not np.isfinite(grad[j]):\n",
        "                 if debug_print: print(f\"DEBUG grad_H (j={j}): final grad[{j}] is NaN/Inf\")\n",
        "    except Exception as e_calc:\n",
        "         print(f\"\\nERROR in calculate_H_gradient: {e_calc}\\n{traceback.format_exc()}\")\n",
        "         return np.full(M, np.nan)\n",
        "    if np.any(~np.isfinite(grad)):\n",
        "        if debug_print: print(f\"DEBUG grad_H: Final check found NaN/Inf: {grad}\")\n",
        "        return grad\n",
        "    return grad\n",
        "\n",
        "# --- project_to_simplex  () ---\n",
        "def project_to_simplex(v, z=1):\n",
        "    n_features = v.shape[0];\n",
        "    if n_features == 0: return np.array([])\n",
        "    v_arr = np.asarray(v)\n",
        "    if not np.all(np.isfinite(v_arr)):\n",
        "        warnings.warn(\"project_to_simplex: Input contains NaN/Inf. Returning uniform.\")\n",
        "        return np.full(n_features, z / n_features) if n_features > 0 else np.array([])\n",
        "    if np.all(v_arr >= -1e-9) and np.isclose(np.sum(v_arr), z): return np.maximum(v_arr, 0)\n",
        "    u = np.sort(v_arr)[::-1]; cssv = np.cumsum(u) - z; ind = np.arange(n_features) + 1; cond = u - cssv / ind > 0\n",
        "    if np.any(cond): rho = ind[cond][-1]; theta = cssv[rho - 1] / float(rho); w = np.maximum(v_arr - theta, 0)\n",
        "    else:\n",
        "         w = np.zeros(n_features)\n",
        "         if z > 0: w[np.argmax(v_arr)] = z\n",
        "    w_sum = np.sum(w)\n",
        "    if not np.isclose(w_sum, z):\n",
        "        if w_sum > 1e-9: w = w * (z / w_sum)\n",
        "        elif z > 0 : w = np.zeros(n_features); w[np.argmax(v_arr)] = z\n",
        "    return np.maximum(w, 0)\n",
        "\n",
        "\n",
        "# --- frank_wolfe_optimizer  () ---\n",
        "def frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde, initial_w=None, max_outer_iter=250, fw_gap_tol=1e-7, inner_max_iter=350, tolerance=1e-9, psd_make_tolerance=1e-8, qp_regularization=1e-10, debug_print=False, force_iterations=0, return_history=False):\n",
        "    K, M = R_alpha.shape\n",
        "    if initial_w is None: w_k = np.ones(M) / M\n",
        "    else: w_k = project_to_simplex(np.copy(initial_w))\n",
        "    if w_k is None or not np.all(np.isfinite(w_k)):\n",
        "        result = OptimizationResult(False, \"Initial w_k invalid after projection\", w_opt=initial_w)\n",
        "        return (result, []) if return_history else result\n",
        "\n",
        "    pi0, ok, p1msg = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance)\n",
        "    if not ok:\n",
        "        result = OptimizationResult(False, f\"Phase 1 failed: {p1msg}\", w_opt=w_k) # Use w_k here\n",
        "        return (result, []) if return_history else result\n",
        "    if pi0 is None or not np.all(np.isfinite(pi0)):\n",
        "        result = OptimizationResult(False, \"Phase 1 returned invalid pi0\", w_opt=w_k)\n",
        "        return (result, []) if return_history else result\n",
        "\n",
        "    best_w = np.copy(w_k); best_pi = None; best_lam = np.zeros(M); best_H = -float('inf')\n",
        "    best_grad_H = np.full(M, np.nan); best_active_set = None; best_fw_gap = float('inf')\n",
        "    last_successful_pi = np.copy(pi0)\n",
        "    history = []\n",
        "\n",
        "    inner_solver_args_for_loop = {'max_iter': inner_max_iter, 'tolerance': tolerance, 'regularization_epsilon': qp_regularization}\n",
        "    converged = False; k = 0; final_msg = \"\"\n",
        "\n",
        "    for k in range(max_outer_iter):\n",
        "        iter_data = {'k': k + 1, 'w_k': np.copy(w_k)} if return_history else {}\n",
        "\n",
        "        try: # Calculate Vw\n",
        "            Vk, Ex, ExxT = calculate_Vw(w_k, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "            if not np.all(np.isfinite(Vk)) or not np.all(np.isfinite(Ex)): raise ValueError(\"Vk or Ex NaN/Inf\")\n",
        "        except Exception as e:\n",
        "            final_msg = f\"Outer iter {k+1}: Vw calculation failed: {e}. Stopping.\"\n",
        "            warnings.warn(final_msg); break # Exit loop on Vw failure\n",
        "\n",
        "        # Inner QP Solve\n",
        "        pi_init_inner = last_successful_pi if last_successful_pi is not None else pi0\n",
        "        if pi_init_inner is None or not np.all(np.isfinite(pi_init_inner)): pi_init_inner = pi0\n",
        "        if pi_init_inner is None or not np.all(np.isfinite(pi_init_inner)):\n",
        "            final_msg = f\"Outer iter {k+1}: Invalid initial pi for inner QP. Stopping.\"; break\n",
        "\n",
        "        pk, lk, act_idx_k, _, _, inner_ok, inner_msg = solve_inner_qp_active_set(\n",
        "            Vk, R_alpha, mu_tilde, pi_init_inner, **inner_solver_args_for_loop\n",
        "        )\n",
        "\n",
        "        if not inner_ok or pk is None or not np.all(np.isfinite(pk)): # Inner QP Failed\n",
        "            tried_pi0_fallback = False\n",
        "            if pi_init_inner is not pi0:\n",
        "                 tried_pi0_fallback = True\n",
        "                 warnings.warn(f\"Outer iter {k+1}: Inner QP failed ({inner_msg}). Retrying with pi0.\")\n",
        "                 pk_pi0, lk_pi0, act_idx_k_pi0, _, _, inner_ok_pi0, inner_msg_pi0 = solve_inner_qp_active_set(\n",
        "                     Vk, R_alpha, mu_tilde, pi0, **inner_solver_args_for_loop\n",
        "                 )\n",
        "                 if inner_ok_pi0 and pk_pi0 is not None and np.all(np.isfinite(pk_pi0)):\n",
        "                     warnings.warn(f\"Outer iter {k+1}: Inner QP fallback with pi0 succeeded.\")\n",
        "                     pk = pk_pi0; lk = lk_pi0; act_idx_k = act_idx_k_pi0; inner_ok = True\n",
        "                 else:\n",
        "                     warnings.warn(f\"Outer iter {k+1}: Inner QP fallback with pi0 also failed ({inner_msg_pi0}).\")\n",
        "                     inner_ok = False\n",
        "            if not inner_ok:\n",
        "                 final_msg = f\"Outer iter {k+1}: Inner QP failed persistently ({inner_msg}).\"; break\n",
        "\n",
        "        # Inner QP Succeeded\n",
        "        last_successful_pi = np.copy(pk)\n",
        "        Hk = pk.T @ Vk @ pk\n",
        "        current_pi = np.copy(pk)\n",
        "        current_lam = lk if lk is not None and np.all(np.isfinite(lk)) else np.zeros(M)\n",
        "        current_active_set = tuple(sorted(act_idx_k)) if act_idx_k is not None else tuple()\n",
        "        current_H = Hk\n",
        "\n",
        "        try: # Calculate Gradient\n",
        "            gHk = calculate_H_gradient(pk, w_k, R_alpha, SecondMoments_alpha_array, Ex, ExxT, tolerance, debug_print=debug_print)\n",
        "            if gHk is None or np.any(~np.isfinite(gHk)): raise ValueError(\"Grad calc returned None or NaN/Inf\")\n",
        "            current_grad_H = gHk\n",
        "        except Exception as e:\n",
        "            final_msg = f\"Outer iter {k+1}: Gradient calculation failed: {e}. Stopping.\"; break\n",
        "\n",
        "        # Update History\n",
        "        if return_history:\n",
        "             iter_data['H_k'] = Hk; iter_data['grad_H_k_norm'] = np.linalg.norm(current_grad_H)\n",
        "             iter_data['pi_k'] = np.copy(pk); iter_data['lam_k'] = np.copy(current_lam)\n",
        "             iter_data['active_set_k'] = current_active_set\n",
        "\n",
        "        # Update Best Solution Found\n",
        "        if np.isfinite(Hk) and (best_pi is None or Hk > best_H + tolerance * 1e-1): # Added check for best_pi is None\n",
        "             best_H = Hk; best_w = np.copy(w_k); best_pi = np.copy(pk)\n",
        "             best_lam = np.copy(current_lam); best_grad_H = np.copy(current_grad_H)\n",
        "             best_active_set = current_active_set\n",
        "\n",
        "        # Frank-Wolfe Step\n",
        "        grad_norm = np.linalg.norm(current_grad_H)\n",
        "        if grad_norm < tolerance * 100: sk = w_k; sk_idx = -1\n",
        "        else: sk_idx = np.argmax(current_grad_H); sk = np.zeros(M); sk[sk_idx] = 1.0\n",
        "        if return_history: iter_data['s_k_index'] = sk_idx\n",
        "\n",
        "        fw_gap = current_grad_H.T @ (w_k - sk)\n",
        "        current_fw_gap = fw_gap\n",
        "        if return_history: iter_data['fw_gap'] = fw_gap\n",
        "\n",
        "        # Update best FW gap if current H is the best found so far\n",
        "        if best_pi is not None and np.isclose(Hk, best_H, atol=tolerance*1e-1, rtol=tolerance*1e-1) and np.isfinite(fw_gap):\n",
        "             best_fw_gap = fw_gap\n",
        "\n",
        "        # Check Convergence\n",
        "        if k >= force_iterations and np.isfinite(fw_gap):\n",
        "            if abs(fw_gap) <= fw_gap_tol:\n",
        "                 converged = True\n",
        "                 final_msg = f\"Converged (Gap {abs(fw_gap):.2e} <= {fw_gap_tol:.1e})\"\n",
        "                 break # Exit loop\n",
        "\n",
        "        # Prepare for next iteration\n",
        "        gamma = 2.0 / (k + 3.0)\n",
        "        if return_history: iter_data['gamma_k'] = gamma; history.append(iter_data)\n",
        "        w_k_next = (1.0 - gamma) * w_k + gamma * sk\n",
        "        w_k = project_to_simplex(w_k_next)\n",
        "        if w_k is None or not np.all(np.isfinite(w_k)):\n",
        "             final_msg = f\"Outer iter {k+1}: w_k invalid after update. Stopping.\"; break\n",
        "\n",
        "    # Loop Finished (Converged, Max Iter, or Error)\n",
        "    final_iters = k + 1 # Number of iterations completed (or max_iter)\n",
        "    if not final_msg: # If loop finished by max_iter without prior error message\n",
        "         final_msg = f\"Max Iter ({max_outer_iter}) reached\"\n",
        "\n",
        "    # --- Final Result Preparation ---\n",
        "    # Use the 'best' values found during the run\n",
        "    final_w = best_w\n",
        "    final_pi = best_pi\n",
        "    final_lam = best_lam\n",
        "    final_active_set = best_active_set\n",
        "    final_H = best_H\n",
        "    final_grad = best_grad_H\n",
        "    final_gap = best_fw_gap\n",
        "\n",
        "    # --- Optional Final Consistency Check ---\n",
        "    # If a best solution was found (best_pi is not None), run one last inner QP\n",
        "    if final_pi is not None and np.all(np.isfinite(final_w)):\n",
        "        try:\n",
        "            final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "            pi_f, lam_f, act_f, _, _, ok_f, msg_f = solve_inner_qp_active_set(\n",
        "                final_Vk, R_alpha, mu_tilde, pi0, **inner_solver_args_for_loop # Use stable pi0\n",
        "            )\n",
        "            if ok_f and pi_f is not None and np.all(np.isfinite(pi_f)):\n",
        "                 # Update results with the consistent final values\n",
        "                 final_pi = pi_f\n",
        "                 final_lam = lam_f if lam_f is not None and np.all(np.isfinite(lam_f)) else np.zeros(M)\n",
        "                 final_active_set = tuple(sorted(act_f)) if act_f is not None else tuple()\n",
        "                 final_H = final_pi.T @ final_Vk @ final_pi\n",
        "                 try: # Recalculate final gradient and gap\n",
        "                     final_grad = calculate_H_gradient(final_pi, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance)\n",
        "                     if final_grad is None or np.any(~np.isfinite(final_grad)): final_grad = np.full(M, np.nan)\n",
        "                     if np.any(np.isfinite(final_grad)):\n",
        "                          grad_norm_f = np.linalg.norm(final_grad[np.isfinite(final_grad)])\n",
        "                          if grad_norm_f < tolerance * 100: sk_f = final_w\n",
        "                          else: sk_idx_f = np.nanargmax(np.nan_to_num(final_grad, nan=-np.inf)); sk_f = np.zeros(M); sk_f[sk_idx_f] = 1.0\n",
        "                          final_gap = final_grad.T @ (final_w - sk_f) if np.all(np.isfinite(final_grad)) else np.nan\n",
        "                     else: final_gap = np.nan\n",
        "                 except Exception as e_grad:\n",
        "                      warnings.warn(f\"Final gradient calculation failed after loop: {e_grad}\")\n",
        "                      final_grad = np.full(M, np.nan); final_gap = np.nan\n",
        "            else:\n",
        "                warnings.warn(f\"Warning: Final inner QP solve failed ({msg_f}). Using best values from iterations.\")\n",
        "        except Exception as e_final:\n",
        "            warnings.warn(f\"Error during final consistency check: {e_final}. Using best values from iterations.\")\n",
        "    # --- End Final Check ---\n",
        "\n",
        "    # Determine success based on whether a valid H was ever found\n",
        "    success_flag = best_pi is not None and np.isfinite(best_H)\n",
        "\n",
        "    if return_history and success_flag: # Add final state if successful run\n",
        "         grad_norm_final = np.linalg.norm(final_grad[np.isfinite(final_grad)]) if np.any(np.isfinite(final_grad)) else np.nan\n",
        "         if grad_norm_final < tolerance * 100: sk_idx_final = -1\n",
        "         else: sk_idx_final = np.nanargmax(np.nan_to_num(final_grad, nan=-np.inf)) if np.any(np.isfinite(final_grad)) else -1\n",
        "         history.append({\n",
        "            'k': final_iters + 1, 'w_k': np.copy(final_w), 'H_k': final_H,\n",
        "            'grad_H_k_norm': grad_norm_final, 's_k_index': sk_idx_final, 'fw_gap': final_gap,\n",
        "            'gamma_k': np.nan, 'pi_k': np.copy(final_pi), 'lam_k': np.copy(final_lam), 'active_set_k': final_active_set\n",
        "         })\n",
        "\n",
        "    # Create result object using the best values found\n",
        "    result = OptimizationResult(success_flag, final_msg, w_opt=final_w, pi_opt=final_pi, lambda_opt=final_lam,\n",
        "                                H_opt=final_H, grad_H_opt=final_grad, iterations=final_iters,\n",
        "                                fw_gap=final_gap, active_set_opt=final_active_set)\n",
        "\n",
        "    return (result, history) if return_history else result\n",
        "\n",
        "\n",
        "# --- generate_params_profile_switching_symmetric  () ---\n",
        "def generate_params_profile_switching_symmetric(alpha, alpha_max, K=5, M=3,\n",
        "                                      R_base_sym=np.array([0.02, 0.01, 0.0, -0.01, -0.02]),\n",
        "                                      sigma_base=np.array([0.18, 0.15, 0.20, 0.12, 0.10]),\n",
        "                                      Corr_base=np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]),\n",
        "                                      r_offset=0.03,\n",
        "                                      s_factor_g = 0.001, s_factor_b = -0.001,\n",
        "                                      sigma_min_epsilon=1e-4, psd_tolerance=1e-9):\n",
        "    assert K == len(R_base_sym) and K == len(sigma_base) and K == Corr_base.shape[0] and M == 3, \"Dimension mismatch\"\n",
        "    R_neutral = R_base_sym; R_good = R_base_sym + r_offset; R_bad = R_base_sym - r_offset\n",
        "    Corr_neutral_psd = make_psd(Corr_base, psd_tolerance)\n",
        "    sigma_neutral_eff = np.maximum(sigma_min_epsilon, sigma_base)\n",
        "    V_neutral = np.diag(sigma_neutral_eff) @ Corr_neutral_psd @ np.diag(sigma_neutral_eff)\n",
        "    sigma_good_adj = np.maximum(sigma_min_epsilon, sigma_base * (1 + s_factor_g))\n",
        "    V_good = np.diag(sigma_good_adj) @ Corr_neutral_psd @ np.diag(sigma_good_adj)\n",
        "    sigma_bad_adj = np.maximum(sigma_min_epsilon, sigma_base * (1 + s_factor_b))\n",
        "    V_bad = np.diag(sigma_bad_adj) @ Corr_neutral_psd @ np.diag(sigma_bad_adj)\n",
        "    Sigma_neutral = V_neutral + np.outer(R_neutral, R_neutral)\n",
        "    Sigma_good = V_good + np.outer(R_good, R_good)\n",
        "    Sigma_bad = V_bad + np.outer(R_bad, R_bad)\n",
        "    beta = np.clip(alpha / alpha_max if alpha_max > 0 else (1.0 if alpha > 0 else 0.0), 0.0, 1.0)\n",
        "    R_alpha = np.zeros((K, M)); SecondMoments_a_array = np.zeros((M, K, K))\n",
        "    R_alpha[:, 0] = (1 - beta) * R_good + beta * R_neutral\n",
        "    SecondMoments_a_array[0, :, :] = (1 - beta) * Sigma_good + beta * Sigma_neutral\n",
        "    R_alpha[:, 1] = (1 - beta) * R_bad + beta * R_good\n",
        "    SecondMoments_a_array[1, :, :] = (1 - beta) * Sigma_bad + beta * Sigma_good\n",
        "    R_alpha[:, 2] = (1 - beta) * R_neutral + beta * R_bad\n",
        "    SecondMoments_a_array[2, :, :] = (1 - beta) * Sigma_neutral + beta * Sigma_bad\n",
        "    if not np.all(np.isfinite(R_alpha)): raise ValueError(\"Generated R_alpha contains NaN/Inf.\")\n",
        "    if not np.all(np.isfinite(SecondMoments_a_array)): raise ValueError(\"Generated SecondMoments_a_array contains NaN/Inf.\")\n",
        "    return R_alpha, SecondMoments_a_array\n",
        "\n",
        "# ===  () ===\n",
        "def analyze_alpha_full_results(alpha_range, param_gen_kwargs, optimizer_kwargs, mu_tilde):\n",
        "    results_over_alpha = []\n",
        "    K = param_gen_kwargs.get('K', 5)\n",
        "    M = param_gen_kwargs.get('M', 3)\n",
        "    if K is None or M is None: raise ValueError(\"K and M must be specified\")\n",
        "\n",
        "    print(f\"\\n--- Starting Full Results Analysis over Alpha Range [{alpha_range[0]:.4f}, {alpha_range[-1]:.4f}] ---\")\n",
        "    print(f\"--- (Using Active Set Method, mu_tilde={mu_tilde}) ---\")\n",
        "    total_alphas = len(alpha_range); start_loop_time = time.time()\n",
        "    successful_runs = 0; failed_alphas = []\n",
        "\n",
        "    for idx, alpha in enumerate(alpha_range):\n",
        "        loop_start_time = time.time()\n",
        "        print(f\"\\rAnalyzing alpha = {alpha:.6f} ({idx+1}/{total_alphas}) ... \", end=\"\")\n",
        "        alpha_result = {'alpha': alpha}\n",
        "        fw_success = False; fw_message = \"Analysis not run\"; w_fw_default = np.full(M, np.nan)\n",
        "        H_star_fw = np.nan; pi_opt = np.full(K, np.nan); grad_H_opt = np.full(M, np.nan)\n",
        "        lambda_opt = np.full(M, np.nan); active_set_opt = tuple(); fw_gap_final = np.nan\n",
        "        iterations_final = 0\n",
        "\n",
        "        try:\n",
        "            R_alpha, SecondMoments_alpha_array = generate_params_profile_switching_symmetric(alpha, **param_gen_kwargs)\n",
        "            if not np.all(np.isfinite(R_alpha)) or not np.all(np.isfinite(SecondMoments_alpha_array)):\n",
        "                 raise ValueError(\"Parameter generation resulted in NaN/Inf.\")\n",
        "\n",
        "            fw_result = frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde,\n",
        "                                              initial_w=None, return_history=False,\n",
        "                                              debug_print=False, **optimizer_kwargs)\n",
        "            fw_success = fw_result.success; fw_message = fw_result.message\n",
        "            iterations_final = fw_result.iterations if fw_result.iterations is not None else 0\n",
        "\n",
        "            # Store results regardless of success, using NaNs for failed runs\n",
        "            w_fw_default = fw_result.w_opt if fw_result.w_opt is not None else np.full(M, np.nan)\n",
        "            H_star_fw = fw_result.H_opt if fw_result.H_opt is not None else np.nan\n",
        "            pi_opt = fw_result.pi_opt if fw_result.pi_opt is not None else np.full(K, np.nan)\n",
        "            grad_H_opt = fw_result.grad_H_opt if fw_result.grad_H_opt is not None else np.full(M, np.nan)\n",
        "            lambda_opt = fw_result.lambda_opt if fw_result.lambda_opt is not None else np.full(M, np.nan)\n",
        "            active_set_opt = fw_result.active_set_opt if fw_result.active_set_opt is not None else tuple()\n",
        "            fw_gap_final = fw_result.fw_gap if fw_result.fw_gap is not None else np.nan\n",
        "\n",
        "            if fw_success: successful_runs += 1\n",
        "            else:\n",
        "                 failed_alphas.append(alpha)\n",
        "                 print(f\"\\n  FW failed for alpha={alpha:.6f}: {fw_result.message}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n  Error during analysis for alpha={alpha:.6f}: {e}\")\n",
        "            fw_success = False; fw_message = f\"Exception: {e}\"; w_fw_default = np.full(M, np.nan)\n",
        "            H_star_fw = np.nan; pi_opt = np.full(K, np.nan); grad_H_opt = np.full(M, np.nan)\n",
        "            lambda_opt = np.full(M, np.nan); active_set_opt = tuple(); fw_gap_final = np.nan\n",
        "            failed_alphas.append(alpha)\n",
        "\n",
        "        alpha_result['success'] = fw_success; alpha_result['message'] = fw_message\n",
        "        alpha_result['w_fw_default'] = w_fw_default; alpha_result['H_star'] = H_star_fw\n",
        "        alpha_result['pi_opt'] = pi_opt; alpha_result['grad_H_opt'] = grad_H_opt\n",
        "        alpha_result['lambda_opt'] = lambda_opt; alpha_result['active_set_opt'] = active_set_opt\n",
        "        alpha_result['fw_gap'] = fw_gap_final; alpha_result['iterations'] = iterations_final\n",
        "        results_over_alpha.append(alpha_result)\n",
        "\n",
        "    end_loop_time = time.time()\n",
        "    print(f\"\\n--- Finished Full Results Analysis ({total_alphas} points) ---\")\n",
        "    print(f\"Total time: {end_loop_time - start_loop_time:.2f} seconds\")\n",
        "    print(f\"Successful runs: {successful_runs}/{total_alphas}\")\n",
        "    if failed_alphas: print(f\"Failed alphas (count: {len(failed_alphas)}): {failed_alphas[:5]}...\") # Show only first few failed\n",
        "\n",
        "    if not PANDAS_AVAILABLE: return results_over_alpha\n",
        "    df_results = pd.DataFrame(results_over_alpha)\n",
        "    if df_results.empty: return df_results # Return empty DataFrame if no results\n",
        "\n",
        "    # Expand vector columns carefully, checking for existence first\n",
        "    if 'w_fw_default' in df_results.columns:\n",
        "        w_vecs = np.stack(df_results['w_fw_default'].apply(lambda x: x if isinstance(x, np.ndarray) and x.shape == (M,) else np.full(M, np.nan)).values)\n",
        "        if np.any(np.isfinite(w_vecs)):\n",
        "            for m in range(M): df_results[f'w_fw_{m}'] = w_vecs[:, m]\n",
        "    if 'pi_opt' in df_results.columns:\n",
        "        pi_vecs = np.stack(df_results['pi_opt'].apply(lambda x: x if isinstance(x, np.ndarray) and x.shape == (K,) else np.full(K, np.nan)).values)\n",
        "        if np.any(np.isfinite(pi_vecs)):\n",
        "            for k in range(K): df_results[f'pi_{k}'] = pi_vecs[:, k]\n",
        "    if 'grad_H_opt' in df_results.columns:\n",
        "        grad_vecs = np.stack(df_results['grad_H_opt'].apply(lambda x: x if isinstance(x, np.ndarray) and x.shape == (M,) else np.full(M, np.nan)).values)\n",
        "        if np.any(np.isfinite(grad_vecs)):\n",
        "            for m in range(M): df_results[f'grad_H_{m}'] = grad_vecs[:, m]\n",
        "    if 'lambda_opt' in df_results.columns:\n",
        "        lambda_vecs = np.stack(df_results['lambda_opt'].apply(lambda x: x if isinstance(x, np.ndarray) and x.shape == (M,) else np.full(M, np.nan)).values)\n",
        "        if np.any(np.isfinite(lambda_vecs)):\n",
        "            for m in range(M): df_results[f'lambda_{m}'] = lambda_vecs[:, m]\n",
        "    if 'active_set_opt' in df_results.columns:\n",
        "        df_results['active_set'] = df_results['active_set_opt'].apply(lambda x: str(x) if x is not None else '()')\n",
        "\n",
        "    cols_to_drop = ['w_fw_default', 'pi_opt', 'grad_H_opt', 'lambda_opt', 'active_set_opt']\n",
        "    df_results = df_results.drop(columns=[col for col in cols_to_drop if col in df_results.columns], errors='ignore')\n",
        "    return df_results\n",
        "\n",
        "\n",
        "# ===  ===\n",
        "if __name__ == '__main__':\n",
        "    start_time_main = time.time()\n",
        "    warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered')\n",
        "    warnings.filterwarnings('ignore', category=UserWarning)\n",
        "    warnings.filterwarnings('ignore', category=OptimizeWarning)\n",
        "\n",
        "    # ---  ---\n",
        "    K = 5; M = 3;\n",
        "    #  mu_tilde  0.022  \n",
        "    mu_tilde = 0.022\n",
        "    alpha_max = 1.5\n",
        "\n",
        "    #  ()\n",
        "    param_gen_kwargs_quasi_symmetric = {\n",
        "        'K': K, 'M': M, 'alpha_max': alpha_max,\n",
        "        'R_base_sym': np.array([0.02, 0.01, 0.0, -0.01, -0.02]),\n",
        "        'sigma_base': np.array([0.18, 0.15, 0.20, 0.12, 0.10]),\n",
        "        'Corr_base': np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]),\n",
        "        'r_offset': 0.03, 's_factor_g': 0.001, 's_factor_b': -0.001,\n",
        "        'sigma_min_epsilon': 1e-4, 'psd_tolerance': 1e-9\n",
        "    }\n",
        "\n",
        "    # FW Optimizer  Inner QP \n",
        "    solver_settings = {\n",
        "        'max_outer_iter': 500, 'fw_gap_tol': 1e-9,\n",
        "        'inner_max_iter': 600,\n",
        "        'tolerance': 1e-11,\n",
        "        'psd_make_tolerance': 1e-9,\n",
        "        #   \n",
        "        'qp_regularization': 1e-9, # Increased from 1e-10 to 1e-9\n",
        "        'force_iterations': 50\n",
        "    }\n",
        "\n",
        "    #  alpha  ()\n",
        "    alpha_start = 0.700\n",
        "    alpha_end = 0.800\n",
        "    num_alpha_steps = 101\n",
        "    alpha_range_analyze = np.linspace(alpha_start, alpha_end, num_alpha_steps)\n",
        "    unique_pt_tolerance = 1e-4\n",
        "\n",
        "    #  (mu_tilde )\n",
        "    output_csv_filename = f\"alpha_internal_sol_search_{alpha_start:.3f}_{alpha_end:.3f}_activeset_mu{int(mu_tilde*1000):03d}_quasi_sym_reg{abs(int(math.log10(solver_settings['qp_regularization'])))}.csv\"\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"--- Searching for Internal Solution around alpha=0.75 [{alpha_start:.3f}, {alpha_end:.3f}] ---\")\n",
        "    print(f\"--- (Using ActiveSet, mu_tilde={mu_tilde}, Quasi-Symmetric Params, QP Reg={solver_settings['qp_regularization']:.1e}) ---\") # Print regularization\n",
        "    print(f\"--- Output CSV: {output_csv_filename} ---\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # ---  ---\n",
        "    results_df = analyze_alpha_full_results(\n",
        "        alpha_range=alpha_range_analyze,\n",
        "        param_gen_kwargs=param_gen_kwargs_quasi_symmetric,\n",
        "        optimizer_kwargs=solver_settings,\n",
        "        mu_tilde=mu_tilde\n",
        "    )\n",
        "\n",
        "    # ---  & CSV & KKT ---\n",
        "    if PANDAS_AVAILABLE and isinstance(results_df, pd.DataFrame) and not results_df.empty:\n",
        "        print(\"\\n--- Analysis Results Summary (DataFrame) ---\")\n",
        "        cols_display = ['alpha', 'success', 'H_star', 'fw_gap', 'iterations', 'active_set']\n",
        "        cols_w = [f'w_fw_{m}' for m in range(M) if f'w_fw_{m}' in results_df.columns]\n",
        "        cols_grad = [f'grad_H_{m}' for m in range(M) if f'grad_H_{m}' in results_df.columns]\n",
        "        cols_to_show = cols_display + cols_w + cols_grad\n",
        "        cols_to_show = [col for col in cols_to_show if col in results_df.columns]\n",
        "        float_format_func = lambda x: f\"{x:.4e}\" if pd.notna(x) and isinstance(x, (float, np.number)) else x\n",
        "        pd.options.display.float_format = float_format_func\n",
        "        with pd.option_context('display.max_rows', 100, 'display.max_columns', None, 'display.width', 200): # Limit rows for console\n",
        "             print(results_df[cols_to_show].to_string(index=False, na_rep='NaN'))\n",
        "        pd.reset_option('display.float_format')\n",
        "\n",
        "        # --- CSV ---\n",
        "        try:\n",
        "            cols_pi = [f'pi_{k}' for k in range(K) if f'pi_{k}' in results_df.columns]\n",
        "            cols_lam = [f'lambda_{m}' for m in range(M) if f'lambda_{m}' in results_df.columns]\n",
        "            cols_csv_order = ['alpha', 'success', 'message', 'H_star', 'fw_gap', 'iterations', 'active_set'] + \\\n",
        "                             cols_w + cols_pi + cols_grad + cols_lam\n",
        "            cols_csv_order = [col for col in cols_csv_order if col in results_df.columns]\n",
        "            df_to_save = results_df[cols_csv_order]\n",
        "            df_to_save.to_csv(output_csv_filename, index=False, float_format='%.8e')\n",
        "            print(f\"\\nFull detailed results ({len(df_to_save)} points) saved to: {os.path.abspath(output_csv_filename)}\")\n",
        "        except Exception as e: print(f\"\\nError saving results to CSV: {e}\")\n",
        "\n",
        "        # --- KKT ---\n",
        "        print(\"\\n--- Quick KKT Check (Stationarity w.r.t. w) ---\")\n",
        "        kkt_violations = []\n",
        "        check_tol = solver_settings.get('tolerance', 1e-9) * 1e4 # KKT tolerance\n",
        "\n",
        "        for index, row in results_df.iterrows():\n",
        "            alpha = row['alpha']\n",
        "            # Use the 'success' flag from the DataFrame\n",
        "            if not row.get('success', False):\n",
        "                 kkt_violations.append({\n",
        "                    'alpha': alpha, 'active_set_kkt': 'N/A',\n",
        "                    'max_violation': np.nan, 'grad_consistency_violation': np.nan, 'check_result': 'FW Fail'\n",
        "                 })\n",
        "                 continue\n",
        "            w_star = np.array([row.get(f'w_fw_{m}', np.nan) for m in range(M)])\n",
        "            grad_h = np.array([row.get(f'grad_H_{m}', np.nan) for m in range(M)])\n",
        "            if np.isnan(w_star).any() or np.isnan(grad_h).any() or np.isclose(np.sum(w_star), 0):\n",
        "                kkt_violations.append({\n",
        "                    'alpha': alpha, 'active_set_kkt': 'NaN',\n",
        "                    'max_violation': np.nan, 'grad_consistency_violation': np.nan, 'check_result': 'Skipped (NaN)'\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            active_indices_kkt = set(m for m in range(M) if w_star[m] > unique_pt_tolerance)\n",
        "            grad_consistency_violation = 0.0\n",
        "            if len(active_indices_kkt) > 1:\n",
        "                 active_grads = grad_h[list(active_indices_kkt)]\n",
        "                 if np.all(np.isfinite(active_grads)):\n",
        "                      grad_consistency_violation = np.max(active_grads) - np.min(active_grads)\n",
        "                 else: grad_consistency_violation = np.nan\n",
        "\n",
        "            stationarity_violation = grad_consistency_violation if np.isfinite(grad_consistency_violation) else 0.0\n",
        "            nu_estimate = np.nan\n",
        "            if active_indices_kkt:\n",
        "                 valid_active_grads = grad_h[list(active_indices_kkt)]\n",
        "                 valid_active_grads = valid_active_grads[np.isfinite(valid_active_grads)]\n",
        "                 if len(valid_active_grads) > 0: nu_estimate = np.max(valid_active_grads)\n",
        "            if not np.isfinite(nu_estimate):\n",
        "                 valid_grads = grad_h[np.isfinite(grad_h)]\n",
        "                 if len(valid_grads) > 0: nu_estimate = np.max(valid_grads)\n",
        "\n",
        "            if np.isfinite(nu_estimate):\n",
        "                for m in range(M):\n",
        "                    if m not in active_indices_kkt and np.isfinite(grad_h[m]):\n",
        "                        violation = grad_h[m] - nu_estimate\n",
        "                        if violation > check_tol:\n",
        "                            stationarity_violation = max(stationarity_violation, violation)\n",
        "            else: stationarity_violation = np.nan\n",
        "\n",
        "            is_internal = len(active_indices_kkt) == M\n",
        "            result_str = ''\n",
        "            if not np.isfinite(stationarity_violation): result_str = 'WARN (NaN KKT)'\n",
        "            elif stationarity_violation > check_tol * 10: result_str = 'WARN (KKT Viol)'\n",
        "            else: result_str = 'OK'\n",
        "            if is_internal: result_str += ' [INTERNAL]'\n",
        "            elif len(active_indices_kkt) == 2: result_str += ' [EDGE]'\n",
        "            elif len(active_indices_kkt) == 1: result_str += ' [VERTEX]'\n",
        "            elif len(active_indices_kkt) == 0: result_str += ' [EMPTY?]' # Should not happen if w sums to 1\n",
        "\n",
        "            kkt_violations.append({\n",
        "                'alpha': alpha, 'active_set_kkt': str(tuple(sorted(active_indices_kkt))),\n",
        "                'max_violation': stationarity_violation, 'grad_consistency_violation': grad_consistency_violation,\n",
        "                'check_result': result_str\n",
        "            })\n",
        "\n",
        "        if PANDAS_AVAILABLE and kkt_violations:\n",
        "            df_kkt = pd.DataFrame(kkt_violations)\n",
        "            kkt_cols_order = ['alpha', 'active_set_kkt', 'max_violation', 'grad_consistency_violation', 'check_result']\n",
        "            kkt_cols_order = [col for col in kkt_cols_order if col in df_kkt.columns]\n",
        "            with pd.option_context('display.float_format', '{:.4e}'.format, 'display.max_rows', None):\n",
        "                print(df_kkt[kkt_cols_order].to_string(index=False, na_rep='NaN'))\n",
        "            print(\"\\nNotes on KKT Check:\")\n",
        "            print(\" - active_set_kkt: Active set inferred from w* > tolerance for outer problem\")\n",
        "            print(\" - max_violation: Max KKT violation (max(grad_consistency, max(grad_inactive - nu_est)))\")\n",
        "            print(\" - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\")\n",
        "            print(\" - check_result: OK/WARN indicates KKT satisfaction; [...] indicates solution type.\")\n",
        "        else: print(\"Could not perform KKT check or pandas not available.\")\n",
        "\n",
        "    elif isinstance(results_df, list) or (isinstance(results_df, pd.DataFrame) and results_df.empty):\n",
        "        print(\"\\n--- Analysis Results Summary ---\")\n",
        "        print(\"No results generated or DataFrame is empty.\")\n",
        "        if not PANDAS_AVAILABLE: print(\"(Pandas not available, skipping CSV export)\")\n",
        "\n",
        "    end_time_main = time.time()\n",
        "    print(f\"\\nTotal execution time: {end_time_main - start_time_main:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFVTr2pr5ZOH",
        "outputId": "cfe71524-c2d3-4654-ee97-91130bb4f0a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "--- Searching for Internal Solution around alpha=0.75 [0.700, 0.800] ---\n",
            "--- (Using ActiveSet, mu_tilde=0.022, Quasi-Symmetric Params, QP Reg=1.0e-09) ---\n",
            "--- Output CSV: alpha_internal_sol_search_0.700_0.800_activeset_mu022_quasi_sym_reg9.csv ---\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "--- Starting Full Results Analysis over Alpha Range [0.7000, 0.8000] ---\n",
            "--- (Using Active Set Method, mu_tilde=0.022) ---\n",
            "Analyzing alpha = 0.800000 (101/101) ... \n",
            "--- Finished Full Results Analysis (101 points) ---\n",
            "Total time: 32.79 seconds\n",
            "Successful runs: 101/101\n",
            "\n",
            "--- Analysis Results Summary (DataFrame) ---\n",
            "     alpha  success     H_star      fw_gap  iterations active_set     w_fw_0     w_fw_1     w_fw_2   grad_H_0   grad_H_1   grad_H_2\n",
            "7.0000e-01     True 9.5764e-03 -9.9093e-10         143     (0, 1) 9.9994e-01 3.2375e-05 3.2375e-05 9.0924e-03 9.0809e-03 9.0732e-03\n",
            "7.0100e-01     True 9.5763e-03 -9.8969e-10         143     (0, 2) 9.9994e-01 3.2375e-05 3.2375e-05 9.0923e-03 9.0809e-03 9.0732e-03\n",
            "7.0200e-01     True 9.5763e-03 -9.8845e-10         143     (0, 2) 9.9994e-01 3.2375e-05 3.2375e-05 9.0923e-03 9.0809e-03 9.0732e-03\n",
            "7.0300e-01     True 9.5763e-03 -9.8721e-10         143     (0, 1) 9.9994e-01 3.2375e-05 3.2375e-05 9.0923e-03 9.0810e-03 9.0732e-03\n",
            "7.0400e-01     True 9.5763e-03 -9.9986e-10         142     (0, 2) 9.9993e-01 3.2831e-05 3.2831e-05 9.0923e-03 9.0810e-03 9.0732e-03\n",
            "7.0500e-01     True 9.5763e-03 -9.9860e-10         142     (0, 2) 9.9993e-01 3.2831e-05 3.2831e-05 9.0923e-03 9.0810e-03 9.0732e-03\n",
            "7.0600e-01     True 9.5763e-03 -9.9735e-10         142     (0, 1) 9.9993e-01 3.2831e-05 3.2831e-05 9.0923e-03 9.0810e-03 9.0731e-03\n",
            "7.0700e-01     True 9.5763e-03 -9.9609e-10         142     (0, 2) 9.9993e-01 3.2831e-05 3.2831e-05 9.0923e-03 9.0811e-03 9.0731e-03\n",
            "7.0800e-01     True 9.5763e-03 -9.9483e-10         142     (0, 1) 9.9993e-01 3.2831e-05 3.2831e-05 9.0923e-03 9.0811e-03 9.0731e-03\n",
            "7.0900e-01     True 9.5762e-03 -9.9357e-10         142     (0, 2) 9.9993e-01 3.2831e-05 3.2831e-05 9.0922e-03 9.0811e-03 9.0731e-03\n",
            "7.1000e-01     True 9.5762e-03 -9.9232e-10         142     (0, 2) 9.9993e-01 3.2831e-05 3.2831e-05 9.0922e-03 9.0811e-03 9.0731e-03\n",
            "7.1100e-01     True 9.5762e-03 -9.9106e-10         142     (0, 2) 9.9993e-01 3.2831e-05 3.2831e-05 9.0922e-03 9.0812e-03 9.0731e-03\n",
            "7.1200e-01     True 9.5762e-03 -9.8980e-10         142     (0, 1) 9.9993e-01 3.2831e-05 3.2831e-05 9.0922e-03 9.0812e-03 9.0731e-03\n",
            "7.1300e-01     True 9.5762e-03 -9.8855e-10         142     (0, 1) 9.9993e-01 3.2831e-05 3.2831e-05 9.0922e-03 9.0812e-03 9.0731e-03\n",
            "7.1400e-01     True 9.5762e-03 -9.8729e-10         142     (0, 1) 9.9993e-01 3.2831e-05 3.2831e-05 9.0922e-03 9.0812e-03 9.0730e-03\n",
            "7.1500e-01     True 9.5762e-03 -9.8603e-10         142     (0, 2) 9.9993e-01 3.2831e-05 3.2831e-05 9.0922e-03 9.0813e-03 9.0730e-03\n",
            "7.1600e-01     True 9.5761e-03 -9.9874e-10         141     (0, 1) 9.9993e-01 3.3297e-05 3.3297e-05 9.0921e-03 9.0813e-03 9.0730e-03\n",
            "7.1700e-01     True 9.5761e-03 -9.9747e-10         141     (0, 2) 9.9993e-01 3.3297e-05 3.3297e-05 9.0921e-03 9.0813e-03 9.0730e-03\n",
            "7.1800e-01     True 9.5761e-03 -9.9620e-10         141     (0, 1) 9.9993e-01 3.3297e-05 3.3297e-05 9.0921e-03 9.0813e-03 9.0730e-03\n",
            "7.1900e-01     True 9.5761e-03 -9.9492e-10         141     (0, 1) 9.9993e-01 3.3297e-05 3.3297e-05 9.0921e-03 9.0814e-03 9.0730e-03\n",
            "7.2000e-01     True 9.5761e-03 -9.9365e-10         141     (0, 2) 9.9993e-01 3.3297e-05 3.3297e-05 9.0921e-03 9.0814e-03 9.0730e-03\n",
            "7.2100e-01     True 9.5761e-03 -9.9237e-10         141     (0, 1) 9.9993e-01 3.3297e-05 3.3297e-05 9.0921e-03 9.0814e-03 9.0730e-03\n",
            "7.2200e-01     True 9.5761e-03 -9.9110e-10         141     (0, 1) 9.9993e-01 3.3297e-05 3.3297e-05 9.0921e-03 9.0814e-03 9.0729e-03\n",
            "7.2300e-01     True 9.5761e-03 -9.8982e-10         141     (0, 2) 9.9993e-01 3.3297e-05 3.3297e-05 9.0921e-03 9.0815e-03 9.0729e-03\n",
            "7.2400e-01     True 9.5760e-03 -9.8855e-10         141     (0, 1) 9.9993e-01 3.3297e-05 3.3297e-05 9.0920e-03 9.0815e-03 9.0729e-03\n",
            "7.2500e-01     True 9.5760e-03 -9.8727e-10         141     (0, 2) 9.9993e-01 3.3297e-05 3.3297e-05 9.0920e-03 9.0815e-03 9.0729e-03\n",
            "7.2600e-01     True 9.5760e-03 -9.8600e-10         141     (0, 2) 9.9993e-01 3.3297e-05 3.3297e-05 9.0920e-03 9.0815e-03 9.0729e-03\n",
            "7.2700e-01     True 9.5760e-03 -9.9879e-10         140     (0, 1) 9.9993e-01 3.3772e-05 3.3772e-05 9.0920e-03 9.0816e-03 9.0729e-03\n",
            "7.2800e-01     True 9.5760e-03 -9.9750e-10         140     (0, 1) 9.9993e-01 3.3772e-05 3.3772e-05 9.0920e-03 9.0816e-03 9.0729e-03\n",
            "7.2900e-01     True 9.5760e-03 -9.9620e-10         140     (0, 2) 9.9993e-01 3.3772e-05 3.3772e-05 9.0920e-03 9.0816e-03 9.0729e-03\n",
            "7.3000e-01     True 9.5760e-03 -9.9491e-10         140     (0, 2) 9.9993e-01 3.3772e-05 3.3772e-05 9.0920e-03 9.0816e-03 9.0728e-03\n",
            "7.3100e-01     True 9.5760e-03 -9.9362e-10         140     (0, 2) 9.9993e-01 3.3772e-05 3.3772e-05 9.0920e-03 9.0817e-03 9.0728e-03\n",
            "7.3200e-01     True 9.5759e-03 -9.9233e-10         140     (0, 1) 9.9993e-01 3.3772e-05 3.3772e-05 9.0919e-03 9.0817e-03 9.0728e-03\n",
            "7.3300e-01     True 9.5759e-03 -9.9103e-10         140     (0, 2) 9.9993e-01 3.3772e-05 3.3772e-05 9.0919e-03 9.0817e-03 9.0728e-03\n",
            "7.3400e-01     True 9.5759e-03 -9.8974e-10         140     (0, 1) 9.9993e-01 3.3772e-05 3.3772e-05 9.0919e-03 9.0817e-03 9.0728e-03\n",
            "7.3500e-01     True 9.5759e-03 -9.8845e-10         140     (0, 2) 9.9993e-01 3.3772e-05 3.3772e-05 9.0919e-03 9.0818e-03 9.0728e-03\n",
            "7.3600e-01     True 9.5759e-03 -9.8715e-10         140     (0, 1) 9.9993e-01 3.3772e-05 3.3772e-05 9.0919e-03 9.0818e-03 9.0728e-03\n",
            "7.3700e-01     True 9.5759e-03 -9.8586e-10         140     (0, 1) 9.9993e-01 3.3772e-05 3.3772e-05 9.0919e-03 9.0818e-03 9.0727e-03\n",
            "7.3800e-01     True 9.5759e-03 -9.9873e-10         139     (0, 2) 9.9993e-01 3.4258e-05 3.4258e-05 9.0919e-03 9.0818e-03 9.0727e-03\n",
            "7.3900e-01     True 9.5759e-03 -9.9742e-10         139     (0, 2) 9.9993e-01 3.4258e-05 3.4258e-05 9.0919e-03 9.0819e-03 9.0727e-03\n",
            "7.4000e-01     True 9.5758e-03 -9.9611e-10         139     (0, 2) 9.9993e-01 3.4258e-05 3.4258e-05 9.0918e-03 9.0819e-03 9.0727e-03\n",
            "7.4100e-01     True 9.5758e-03 -9.9480e-10         139     (0, 2) 9.9993e-01 3.4258e-05 3.4258e-05 9.0918e-03 9.0819e-03 9.0727e-03\n",
            "7.4200e-01     True 9.5758e-03 -9.9349e-10         139     (0, 1) 9.9993e-01 3.4258e-05 3.4258e-05 9.0918e-03 9.0820e-03 9.0727e-03\n",
            "7.4300e-01     True 9.5758e-03 -9.9218e-10         139     (0, 1) 9.9993e-01 3.4258e-05 3.4258e-05 9.0918e-03 9.0820e-03 9.0727e-03\n",
            "7.4400e-01     True 9.5758e-03 -9.9087e-10         139     (0, 1) 9.9993e-01 3.4258e-05 3.4258e-05 9.0918e-03 9.0820e-03 9.0727e-03\n",
            "7.4500e-01     True 9.5758e-03 -9.8955e-10         139     (0, 2) 9.9993e-01 3.4258e-05 3.4258e-05 9.0918e-03 9.0820e-03 9.0726e-03\n",
            "7.4600e-01     True 9.5758e-03 -9.8824e-10         139     (0, 1) 9.9993e-01 3.4258e-05 3.4258e-05 9.0918e-03 9.0821e-03 9.0726e-03\n",
            "7.4700e-01     True 9.5758e-03 -9.8693e-10         139     (0, 2) 9.9993e-01 3.4258e-05 3.4258e-05 9.0918e-03 9.0821e-03 9.0726e-03\n",
            "7.4800e-01     True 9.5757e-03 -9.9990e-10         138     (0, 2) 9.9993e-01 3.4755e-05 3.4755e-05 9.0917e-03 9.0821e-03 9.0726e-03\n",
            "7.4900e-01     True 9.5757e-03 -9.9857e-10         138     (0, 2) 9.9993e-01 3.4755e-05 3.4755e-05 9.0917e-03 9.0821e-03 9.0726e-03\n",
            "7.5000e-01     True 9.5757e-03 -9.9724e-10         138     (0, 2) 9.9993e-01 3.4755e-05 3.4755e-05 9.0917e-03 9.0822e-03 9.0726e-03\n",
            "7.5100e-01     True 9.5757e-03 -9.9591e-10         138     (0, 2) 9.9993e-01 3.4755e-05 3.4755e-05 9.0917e-03 9.0822e-03 9.0726e-03\n",
            "7.5200e-01     True 9.5757e-03 -9.9458e-10         138     (0, 1) 9.9993e-01 3.4755e-05 3.4755e-05 9.0917e-03 9.0822e-03 9.0726e-03\n",
            "7.5300e-01     True 9.5757e-03 -9.9325e-10         138     (0, 2) 9.9993e-01 3.4755e-05 3.4755e-05 9.0917e-03 9.0822e-03 9.0725e-03\n",
            "7.5400e-01     True 9.5757e-03 -9.9192e-10         138     (0, 2) 9.9993e-01 3.4755e-05 3.4755e-05 9.0917e-03 9.0823e-03 9.0725e-03\n",
            "7.5500e-01     True 9.5757e-03 -9.9059e-10         138     (0, 1) 9.9993e-01 3.4755e-05 3.4755e-05 9.0917e-03 9.0823e-03 9.0725e-03\n",
            "7.5600e-01     True 9.5756e-03 -9.8926e-10         138     (0, 1) 9.9993e-01 3.4755e-05 3.4755e-05 9.0916e-03 9.0823e-03 9.0725e-03\n",
            "7.5700e-01     True 9.5756e-03 -9.8793e-10         138     (0, 2) 9.9993e-01 3.4755e-05 3.4755e-05 9.0916e-03 9.0823e-03 9.0725e-03\n",
            "7.5800e-01     True 9.5756e-03 -9.8660e-10         138     (0, 2) 9.9993e-01 3.4755e-05 3.4755e-05 9.0916e-03 9.0824e-03 9.0725e-03\n",
            "7.5900e-01     True 9.5756e-03 -9.9965e-10         137     (0, 1) 9.9993e-01 3.5262e-05 3.5262e-05 9.0916e-03 9.0824e-03 9.0725e-03\n",
            "7.6000e-01     True 9.5756e-03 -9.9830e-10         137     (0, 2) 9.9993e-01 3.5262e-05 3.5262e-05 9.0916e-03 9.0824e-03 9.0725e-03\n",
            "7.6100e-01     True 9.5756e-03 -9.9695e-10         137     (0, 1) 9.9993e-01 3.5262e-05 3.5262e-05 9.0916e-03 9.0824e-03 9.0724e-03\n",
            "7.6200e-01     True 9.5756e-03 -9.9560e-10         137     (0, 1) 9.9993e-01 3.5262e-05 3.5262e-05 9.0916e-03 9.0825e-03 9.0724e-03\n",
            "7.6300e-01     True 9.5755e-03 -9.9425e-10         137     (0, 2) 9.9993e-01 3.5262e-05 3.5262e-05 9.0915e-03 9.0825e-03 9.0724e-03\n",
            "7.6400e-01     True 9.5755e-03 -9.9290e-10         137     (0, 2) 9.9993e-01 3.5262e-05 3.5262e-05 9.0915e-03 9.0825e-03 9.0724e-03\n",
            "7.6500e-01     True 9.5755e-03 -9.9155e-10         137     (0, 2) 9.9993e-01 3.5262e-05 3.5262e-05 9.0915e-03 9.0825e-03 9.0724e-03\n",
            "7.6600e-01     True 9.5755e-03 -9.9020e-10         137     (0, 2) 9.9993e-01 3.5262e-05 3.5262e-05 9.0915e-03 9.0826e-03 9.0724e-03\n",
            "7.6700e-01     True 9.5755e-03 -9.8885e-10         137     (0, 2) 9.9993e-01 3.5262e-05 3.5262e-05 9.0915e-03 9.0826e-03 9.0724e-03\n",
            "7.6800e-01     True 9.5755e-03 -9.8750e-10         137     (0, 1) 9.9993e-01 3.5262e-05 3.5262e-05 9.0915e-03 9.0826e-03 9.0724e-03\n",
            "7.6900e-01     True 9.5755e-03 -9.8615e-10         137     (0, 2) 9.9993e-01 3.5262e-05 3.5262e-05 9.0915e-03 9.0826e-03 9.0723e-03\n",
            "7.7000e-01     True 9.5755e-03 -9.9928e-10         136     (0, 1) 9.9993e-01 3.5781e-05 3.5781e-05 9.0915e-03 9.0827e-03 9.0723e-03\n",
            "7.7100e-01     True 9.5754e-03 -9.9791e-10         136     (0, 2) 9.9993e-01 3.5781e-05 3.5781e-05 9.0914e-03 9.0827e-03 9.0723e-03\n",
            "7.7200e-01     True 9.5754e-03 -9.9654e-10         136     (0, 1) 9.9993e-01 3.5781e-05 3.5781e-05 9.0914e-03 9.0827e-03 9.0723e-03\n",
            "7.7300e-01     True 9.5754e-03 -9.9517e-10         136     (0, 1) 9.9993e-01 3.5781e-05 3.5781e-05 9.0914e-03 9.0827e-03 9.0723e-03\n",
            "7.7400e-01     True 9.5754e-03 -9.9380e-10         136     (0, 2) 9.9993e-01 3.5781e-05 3.5781e-05 9.0914e-03 9.0828e-03 9.0723e-03\n",
            "7.7500e-01     True 9.5754e-03 -9.9243e-10         136     (0, 1) 9.9993e-01 3.5781e-05 3.5781e-05 9.0914e-03 9.0828e-03 9.0723e-03\n",
            "7.7600e-01     True 9.5754e-03 -9.9106e-10         136     (0, 2) 9.9993e-01 3.5781e-05 3.5781e-05 9.0914e-03 9.0828e-03 9.0723e-03\n",
            "7.7700e-01     True 9.5754e-03 -9.8969e-10         136     (0, 1) 9.9993e-01 3.5781e-05 3.5781e-05 9.0914e-03 9.0828e-03 9.0722e-03\n",
            "7.7800e-01     True 9.5754e-03 -9.8833e-10         136     (0, 1) 9.9993e-01 3.5781e-05 3.5781e-05 9.0914e-03 9.0829e-03 9.0722e-03\n",
            "7.7900e-01     True 9.5753e-03 -9.8696e-10         136     (0, 2) 9.9993e-01 3.5781e-05 3.5781e-05 9.0913e-03 9.0829e-03 9.0722e-03\n",
            "7.8000e-01     True 9.5753e-03 -9.8559e-10         136     (0, 2) 9.9993e-01 3.5781e-05 3.5781e-05 9.0913e-03 9.0829e-03 9.0722e-03\n",
            "7.8100e-01     True 9.5753e-03 -9.9880e-10         135     (0, 1) 9.9993e-01 3.6311e-05 3.6311e-05 9.0913e-03 9.0829e-03 9.0722e-03\n",
            "7.8200e-01     True 9.5753e-03 -9.9741e-10         135     (0, 1) 9.9993e-01 3.6311e-05 3.6311e-05 9.0913e-03 9.0830e-03 9.0722e-03\n",
            "7.8300e-01     True 9.5753e-03 -9.9602e-10         135     (0, 1) 9.9993e-01 3.6311e-05 3.6311e-05 9.0913e-03 9.0830e-03 9.0722e-03\n",
            "7.8400e-01     True 9.5753e-03 -9.9463e-10         135     (0, 1) 9.9993e-01 3.6311e-05 3.6311e-05 9.0913e-03 9.0830e-03 9.0721e-03\n",
            "7.8500e-01     True 9.5753e-03 -9.9324e-10         135     (0, 2) 9.9993e-01 3.6311e-05 3.6311e-05 9.0913e-03 9.0830e-03 9.0721e-03\n",
            "7.8600e-01     True 9.5753e-03 -9.9185e-10         135     (0, 1) 9.9993e-01 3.6311e-05 3.6311e-05 9.0913e-03 9.0831e-03 9.0721e-03\n",
            "7.8700e-01     True 9.5752e-03 -9.9046e-10         135     (0, 1) 9.9993e-01 3.6311e-05 3.6311e-05 9.0912e-03 9.0831e-03 9.0721e-03\n",
            "7.8800e-01     True 9.5752e-03 -9.8907e-10         135     (0, 2) 9.9993e-01 3.6311e-05 3.6311e-05 9.0912e-03 9.0831e-03 9.0721e-03\n",
            "7.8900e-01     True 9.5752e-03 -9.8768e-10         135     (0, 2) 9.9993e-01 3.6311e-05 3.6311e-05 9.0912e-03 9.0831e-03 9.0721e-03\n",
            "7.9000e-01     True 9.5752e-03 -9.8629e-10         135     (0, 2) 9.9993e-01 3.6311e-05 3.6311e-05 9.0912e-03 9.0832e-03 9.0721e-03\n",
            "7.9100e-01     True 9.5752e-03 -9.9960e-10         134     (0, 2) 9.9993e-01 3.6853e-05 3.6853e-05 9.0912e-03 9.0832e-03 9.0721e-03\n",
            "7.9200e-01     True 9.5752e-03 -9.9818e-10         134     (0, 2) 9.9993e-01 3.6853e-05 3.6853e-05 9.0912e-03 9.0832e-03 9.0720e-03\n",
            "7.9300e-01     True 9.5752e-03 -9.9677e-10         134     (0, 1) 9.9993e-01 3.6853e-05 3.6853e-05 9.0912e-03 9.0833e-03 9.0720e-03\n",
            "7.9400e-01     True 9.5752e-03 -9.9536e-10         134     (0, 1) 9.9993e-01 3.6853e-05 3.6853e-05 9.0912e-03 9.0833e-03 9.0720e-03\n",
            "7.9500e-01     True 9.5751e-03 -9.9395e-10         134     (0, 1) 9.9993e-01 3.6853e-05 3.6853e-05 9.0911e-03 9.0833e-03 9.0720e-03\n",
            "7.9600e-01     True 9.5751e-03 -9.9254e-10         134     (0, 1) 9.9993e-01 3.6853e-05 3.6853e-05 9.0911e-03 9.0833e-03 9.0720e-03\n",
            "7.9700e-01     True 9.5751e-03 -9.9113e-10         134     (0, 2) 9.9993e-01 3.6853e-05 3.6853e-05 9.0911e-03 9.0834e-03 9.0720e-03\n",
            "7.9800e-01     True 9.5751e-03 -9.8972e-10         134     (0, 1) 9.9993e-01 3.6853e-05 3.6853e-05 9.0911e-03 9.0834e-03 9.0720e-03\n",
            "7.9900e-01     True 9.5751e-03 -9.8831e-10         134     (0, 2) 9.9993e-01 3.6853e-05 3.6853e-05 9.0911e-03 9.0834e-03 9.0720e-03\n",
            "8.0000e-01     True 9.5751e-03 -9.8690e-10         134     (0, 2) 9.9993e-01 3.6853e-05 3.6853e-05 9.0911e-03 9.0834e-03 9.0719e-03\n",
            "\n",
            "Full detailed results (101 points) saved to: /content/alpha_internal_sol_search_0.700_0.800_activeset_mu022_quasi_sym_reg9.csv\n",
            "\n",
            "--- Quick KKT Check (Stationarity w.r.t. w) ---\n",
            "     alpha active_set_kkt  max_violation  grad_consistency_violation check_result\n",
            "7.0000e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.0100e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.0200e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.0300e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.0400e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.0500e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.0600e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.0700e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.0800e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.0900e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.1000e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.1100e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.1200e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.1300e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.1400e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.1500e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.1600e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.1700e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.1800e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.1900e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.2000e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.2100e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.2200e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.2300e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.2400e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.2500e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.2600e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.2700e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.2800e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.2900e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.3000e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.3100e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.3200e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.3300e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.3400e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.3500e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.3600e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.3700e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.3800e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.3900e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.4000e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.4100e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.4200e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.4300e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.4400e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.4500e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.4600e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.4700e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.4800e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.4900e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.5000e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.5100e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.5200e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.5300e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.5400e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.5500e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.5600e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.5700e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.5800e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.5900e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.6000e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.6100e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.6200e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.6300e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.6400e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.6500e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.6600e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.6700e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.6800e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.6900e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.7000e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.7100e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.7200e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.7300e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.7400e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.7500e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.7600e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.7700e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.7800e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.7900e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.8000e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.8100e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.8200e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.8300e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.8400e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.8500e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.8600e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.8700e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.8800e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.8900e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.9000e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.9100e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.9200e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.9300e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.9400e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.9500e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.9600e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.9700e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.9800e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "7.9900e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "8.0000e-01           (0,)     0.0000e+00                  0.0000e+00  OK [VERTEX]\n",
            "\n",
            "Notes on KKT Check:\n",
            " - active_set_kkt: Active set inferred from w* > tolerance for outer problem\n",
            " - max_violation: Max KKT violation (max(grad_consistency, max(grad_inactive - nu_est)))\n",
            " - grad_consistency_violation: max(active_grads) - min(active_grads) for w_m > tol\n",
            " - check_result: OK/WARN indicates KKT satisfaction; [...] indicates solution type.\n",
            "\n",
            "Total execution time: 32.84 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "5_3_find_internal_solution_mu022_reg_quasi_sym_larger_diff.py\n",
        "\n",
        "Attempts to find an internal solution w* by using quasi-symmetric parameters\n",
        "with LARGER differences in Vm (s_factor = +/- 0.01) around alpha=0.75.\n",
        "Uses mu_tilde = 0.022 and increased QP regularization (1e-9).\n",
        "Uses the Active Set method for the inner QP solve.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from scipy.linalg import solve, LinAlgError, eigh, inv, qr, lstsq, pinv\n",
        "from scipy.optimize import linprog, OptimizeWarning\n",
        "import traceback\n",
        "import time\n",
        "import itertools\n",
        "import os\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# pandas \n",
        "try:\n",
        "    import pandas as pd\n",
        "    PANDAS_AVAILABLE = True\n",
        "    pd.set_option('display.width', 180)\n",
        "    pd.set_option('display.float_format', '{:.6e}'.format)\n",
        "    pd.set_option('display.max_rows', 300)\n",
        "except ImportError:\n",
        "    PANDAS_AVAILABLE = False\n",
        "    print(\"Warning: pandas library not found. Output formatting will be basic. CSV export disabled.\")\n",
        "\n",
        "# ===  () ===\n",
        "DEFAULT_TOLERANCE = 1e-9\n",
        "\n",
        "# --- OptimizationResult  ---\n",
        "class OptimizationResult:\n",
        "    def __init__(self, success, message, w_opt=None, pi_opt=None, lambda_opt=None,\n",
        "                 H_opt=None, grad_H_opt=None, iterations=None, fw_gap=None,\n",
        "                 active_set_opt=None):\n",
        "        self.success = success; self.message = message; self.w_opt = w_opt; self.pi_opt = pi_opt\n",
        "        self.lambda_opt = lambda_opt; self.H_opt = H_opt; self.grad_H_opt = grad_H_opt\n",
        "        self.iterations = iterations; self.fw_gap = fw_gap\n",
        "        self.active_set_opt = active_set_opt\n",
        "\n",
        "# --- find_feasible_initial_pi  ---\n",
        "def find_feasible_initial_pi(R, mu_tilde, K, tolerance=1e-8):\n",
        "    M = R.shape[1]; c = np.zeros(K + 1); c[K] = 1.0\n",
        "    A_ub = np.hstack((-R.T, -np.ones((M, 1)))); b_ub = -mu_tilde * np.ones(M)\n",
        "    bounds = [(None, None)] * K + [(0, None)]; opts = {'tol': tolerance, 'disp': False, 'presolve': True}\n",
        "    result = None; methods_to_try = ['highs', 'highs-ipm', 'highs-ds', 'simplex']\n",
        "    last_method_tried = 'None'\n",
        "    for method in methods_to_try:\n",
        "        last_method_tried = method\n",
        "        try:\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\", OptimizeWarning)\n",
        "                result = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=method, options=opts)\n",
        "            if result.success: break\n",
        "        except ValueError: continue\n",
        "        except Exception as e: return None, False, f\"Phase 1 LP failed ({method}): {e}\"\n",
        "    if result is None or not result.success:\n",
        "        msg = result.message if result else \"No solver succeeded\"; status = result.status if result else -1\n",
        "        max_expected_return = -np.inf\n",
        "        try:\n",
        "             if K > 0 and M > 0 and R is not None and np.all(np.isfinite(R)):\n",
        "                  equal_pi = np.ones(K) / K; expected_returns = equal_pi @ R\n",
        "                  if np.all(np.isfinite(expected_returns)): max_expected_return = np.max(expected_returns)\n",
        "        except: pass\n",
        "        fail_reason = f\"Phase 1 LP solver failed: {msg} (status={status}, method={last_method_tried})\"\n",
        "        if np.isfinite(max_expected_return) and mu_tilde > max_expected_return + tolerance:\n",
        "             fail_reason += f\" - mu_tilde ({mu_tilde:.4f}) may be too high?\"\n",
        "        return None, False, fail_reason\n",
        "    s = result.x[K]; pi = result.x[:K]\n",
        "    if np.isnan(pi).any(): return None, False, \"Phase 1 LP resulted in NaN values for pi.\"\n",
        "    G = -R.T; h = -mu_tilde * np.ones(M); violation = np.max(G @ pi - h)\n",
        "    feasibility_tol = tolerance * 10000\n",
        "    if s <= tolerance * 1000:\n",
        "        if violation <= feasibility_tol: return pi, True, f\"Phase 1 OK (s*={s:.1e}, max_viol={violation:.1e})\"\n",
        "        else: return pi, True, f\"Phase 1 OK (s*={s:.1e}), WARN: Initial violation {violation:.1e}\"\n",
        "    else: return None, False, f\"Phase 1 Likely Infeasible (s*={s:.1e}), max_viol={violation:.1e}\"\n",
        "\n",
        "# --- solve_kkt_system  ---\n",
        "def solve_kkt_system(Q, G_W, g, tolerance=DEFAULT_TOLERANCE):\n",
        "    K = Q.shape[0]; n_act = G_W.shape[0] if G_W is not None and G_W.ndim == 2 and G_W.shape[0] > 0 else 0\n",
        "    kkt_cond = np.nan\n",
        "    if not np.all(np.isfinite(Q)) or not np.all(np.isfinite(g)): return None, None, False, np.nan\n",
        "    if n_act > 0 and (G_W is None or not np.all(np.isfinite(G_W))): return None, None, False, np.nan\n",
        "    if n_act == 0:\n",
        "        try:\n",
        "            try: kkt_cond = np.linalg.cond(Q)\n",
        "            except LinAlgError: kkt_cond = np.inf\n",
        "            p = solve(Q, -g, assume_a='sym', check_finite=False); l = np.array([])\n",
        "        except LinAlgError: return None, None, False, kkt_cond\n",
        "        except ValueError: return None, None, False, kkt_cond\n",
        "        if p is None or np.isnan(p).any() or np.isinf(p).any(): return None, None, False, kkt_cond\n",
        "        res_norm = np.linalg.norm(Q @ p + g); g_norm = np.linalg.norm(g)\n",
        "        solved_ok = res_norm <= tolerance * 1e4 * (1 + g_norm)\n",
        "        return p, l, solved_ok, kkt_cond\n",
        "    else:\n",
        "        kkt_mat = None; rhs = None\n",
        "        try:\n",
        "            if G_W.ndim != 2 or G_W.shape[1] != K: return None, None, False, kkt_cond\n",
        "            kkt_mat = np.block([[Q, G_W.T], [G_W, np.zeros((n_act, n_act))]])\n",
        "            rhs = np.concatenate([-g, np.zeros(n_act)])\n",
        "            try: kkt_cond = np.linalg.cond(kkt_mat)\n",
        "            except LinAlgError: kkt_cond = np.inf\n",
        "        except ValueError: return None, None, False, kkt_cond\n",
        "        except Exception as e: return None, None, False, kkt_cond\n",
        "        try: sol = solve(kkt_mat, rhs, assume_a='sym', check_finite=False); p = sol[:K]; l = sol[K:]\n",
        "        except LinAlgError: return None, None, False, kkt_cond\n",
        "        except ValueError: return None, None, False, kkt_cond\n",
        "        except Exception: return None, None, False, kkt_cond\n",
        "        if sol is None or np.isnan(sol).any() or np.isinf(sol).any(): return None, None, False, kkt_cond\n",
        "        res_norm = np.linalg.norm(kkt_mat @ sol - rhs); rhs_norm = np.linalg.norm(rhs)\n",
        "        solved_ok = res_norm <= tolerance * 1e4 * (1 + rhs_norm)\n",
        "        return p, l, solved_ok, kkt_cond\n",
        "\n",
        "# --- solve_inner_qp_active_set  ---\n",
        "def solve_inner_qp_active_set(Vw, R, mu_tilde, initial_pi, max_iter=350, tolerance=DEFAULT_TOLERANCE, regularization_epsilon=1e-10):\n",
        "    K = Vw.shape[0]; M = R.shape[1];\n",
        "    if not np.all(np.isfinite(Vw)): return None, None, None, None, None, False, \"QP fail: Vw NaN/Inf.\"\n",
        "    Q_reg = 2 * Vw + 2 * regularization_epsilon * np.eye(K); q_cond = np.inf\n",
        "    try: q_cond = np.linalg.cond(Q_reg)\n",
        "    except LinAlgError: pass\n",
        "    if q_cond > 1 / tolerance: warnings.warn(f\"QP Warning: Q_reg cond high ({q_cond:.2e}). Reg={regularization_epsilon:.1e}\")\n",
        "    G = -R.T; h = -mu_tilde * np.ones(M)\n",
        "    if initial_pi is None or not np.all(np.isfinite(initial_pi)): return None, None, None, None, None, False, \"QP fail: Initial pi invalid.\"\n",
        "    pi_k = np.copy(initial_pi); lam_opt = np.zeros(M); W = set(); active_tol = tolerance * 100\n",
        "    initial_violations = G @ pi_k - h; max_initial_violation = np.max(initial_violations)\n",
        "    if max_initial_violation > active_tol * 10:\n",
        "        warnings.warn(f\"QP Warning: Initial pi infeasible (max viol: {max_initial_violation:.2e}).\")\n",
        "        W = set(j for j, viol in enumerate(initial_violations) if viol > active_tol)\n",
        "    else: W = set(j for j, viol in enumerate(initial_violations) if viol > -active_tol)\n",
        "    active_indices_opt = None\n",
        "    for i in range(max_iter):\n",
        "        g_k = Q_reg @ pi_k\n",
        "        if not np.all(np.isfinite(g_k)): return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: g_k NaN/Inf.\"\n",
        "        act = sorted(list(W)); n_act = len(act)\n",
        "        G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "        p_k, lam_Wk, solved, kkt_cond = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "        if not solved or p_k is None: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: KKT solve failed. ActSet={act}. Q_cond={q_cond:.1e}, KKT_cond={kkt_cond:.1e}\"\n",
        "        p_norm = np.linalg.norm(p_k); pi_k_norm = np.linalg.norm(pi_k)\n",
        "        if p_norm <= tolerance * 100 * (1 + pi_k_norm):\n",
        "            is_optimal_point = True; blocking_constraint_idx = -1; min_negative_lambda = float('inf'); dual_feas_tol = -tolerance * 100\n",
        "            if W:\n",
        "                if lam_Wk is None or len(lam_Wk) != n_act: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: lam_Wk inconsistent? ActSet={act}\"\n",
        "                if not np.all(np.isfinite(lam_Wk)): return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: lam_Wk NaN/Inf. ActSet={act}\"\n",
        "                lambda_map = dict(zip(act, lam_Wk))\n",
        "                for constr_idx, lam_val in lambda_map.items():\n",
        "                    if lam_val < dual_feas_tol: is_optimal_point = False\n",
        "                    if lam_val < min_negative_lambda: min_negative_lambda = lam_val; blocking_constraint_idx = constr_idx\n",
        "            if is_optimal_point:\n",
        "                lam_opt.fill(0.0)\n",
        "                if W and lam_Wk is not None:\n",
        "                     try: lam_opt[act] = np.maximum(lam_Wk, 0)\n",
        "                     except IndexError: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Index err assign lambda. ActSet={act}\"\n",
        "                final_infeas = np.max(G @ pi_k - h); msg = f\"Optimal found iter {i+1}.\"\n",
        "                if final_infeas > active_tol: msg += f\" (WARN: Final viol {final_infeas:.1e})\"\n",
        "                active_indices_opt = act\n",
        "                return pi_k, lam_opt, active_indices_opt, None, Q_reg / 2.0, True, msg\n",
        "            else:\n",
        "                if blocking_constraint_idx in W: W.remove(blocking_constraint_idx); continue\n",
        "                else: return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: Neg lam idx {blocking_constraint_idx}, not in W={act}\"\n",
        "        else:\n",
        "            alpha_k = 1.0; blocking_constraint_idx = -1; min_step_length = float('inf'); step_tol = tolerance * 100\n",
        "            for j in range(M):\n",
        "                if j not in W:\n",
        "                    constr_grad_dot_p = G[j, :] @ p_k\n",
        "                    if constr_grad_dot_p > step_tol:\n",
        "                        dist_to_bound = h[j] - (G[j, :] @ pi_k)\n",
        "                        if abs(constr_grad_dot_p) > 1e-15:\n",
        "                             alpha_j = dist_to_bound / constr_grad_dot_p\n",
        "                             if np.isfinite(alpha_j) and alpha_j >= -tolerance:\n",
        "                                 step_j = max(0.0, alpha_j)\n",
        "                                 if step_j < min_step_length: min_step_length = step_j; blocking_constraint_idx = j\n",
        "            alpha_k = min(1.0, min_step_length)\n",
        "            pi_k += alpha_k * p_k\n",
        "            if not np.all(np.isfinite(pi_k)): return None, None, None, None, None, False, f\"QP fail:Iter {i+1}: pi_k NaN/Inf after step.\"\n",
        "            if alpha_k < 1.0 - step_tol and blocking_constraint_idx != -1:\n",
        "                if blocking_constraint_idx not in W: W.add(blocking_constraint_idx)\n",
        "            continue\n",
        "    msg = f\"Max iter ({max_iter}) reached.\"; final_infeas = np.max(G @ pi_k - h)\n",
        "    if final_infeas > active_tol * 100: return None, None, None, None, None, False, f\"{msg} Final infeasible ({final_infeas:.1e}). ActSet={sorted(list(W))}\"\n",
        "    act = sorted(list(W)); n_act = len(act); G_Wk = G[act, :] if n_act > 0 else np.empty((0, K))\n",
        "    is_likely_optimal = False; active_constraints_opt = act; g_k = Q_reg @ pi_k\n",
        "    if not np.all(np.isfinite(g_k)): return None, None, None, None, None, False, f\"{msg} Final g_k NaN/Inf.\"\n",
        "    p_f, lam_f, solved_f, kkt_cond_f = solve_kkt_system(Q_reg, G_Wk, g_k, tolerance)\n",
        "    final_lambda_estimate = np.zeros(M)\n",
        "    if solved_f and p_f is not None and np.linalg.norm(p_f) <= tolerance * 1000 * (1 + np.linalg.norm(pi_k)):\n",
        "        if n_act > 0:\n",
        "            if lam_f is not None and len(lam_f) == n_act and np.all(np.isfinite(lam_f)):\n",
        "                 try: final_lambda_estimate[act] = lam_f\n",
        "                 except IndexError: pass\n",
        "                 active_lambdas = final_lambda_estimate[act]\n",
        "                 if np.all(active_lambdas >= -tolerance * 1000): is_likely_optimal = True; msg += \" Final KKT approx OK.\"\n",
        "                 else: msg += f\" Final KKT fails (dual infeas, min_lam={np.min(active_lambdas):.1e}).\"\n",
        "            else: msg += \" Final KKT fails (lam_f invalid).\"\n",
        "        else: is_likely_optimal = True; msg += \" Final KKT approx OK (unconstrained).\"\n",
        "    else: p_norm_f = np.linalg.norm(p_f) if p_f is not None else np.nan; msg += f\" Final KKT fails (stationarity p_norm={p_norm_f:.1e} or solve). KKT_cond={kkt_cond_f:.1e}\"\n",
        "    lam_opt = np.maximum(final_lambda_estimate, 0)\n",
        "    return pi_k, lam_opt, active_constraints_opt, None, Q_reg / 2.0, is_likely_optimal, msg\n",
        "\n",
        "# --- make_psd  ---\n",
        "def make_psd(matrix, tolerance=1e-8):\n",
        "    if not np.all(np.isfinite(matrix)): warnings.warn(\"make_psd: Input NaN/Inf.\"); sym = (matrix + matrix.T) / 2.0; return sym\n",
        "    sym = (matrix + matrix.T) / 2.0\n",
        "    try:\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(sym); min_eigenvalue = np.min(eigenvalues)\n",
        "        if min_eigenvalue < tolerance: eigenvalues[eigenvalues < tolerance] = tolerance; psd_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T; return (psd_matrix + psd_matrix.T) / 2.0\n",
        "        else: return sym\n",
        "    except LinAlgError: warnings.warn(\"make_psd: eigh failed.\"); return sym\n",
        "\n",
        "# --- calculate_Vw  ---\n",
        "def calculate_Vw(w, R, SecondMoments_a_array, tolerance=DEFAULT_TOLERANCE, psd_tolerance=1e-8):\n",
        "    K, M = R.shape; w_norm = project_to_simplex(w)\n",
        "    if not np.isclose(np.sum(w_norm), 1.0): raise ValueError(f\"calculate_Vw: w sum err: {np.sum(w_norm):.4f}\")\n",
        "    if not np.all(np.isfinite(R)): raise ValueError(\"calculate_Vw: R NaN/Inf.\")\n",
        "    if not np.all(np.isfinite(SecondMoments_a_array)): raise ValueError(\"calculate_Vw: SecondMoments NaN/Inf.\")\n",
        "    EwX = R @ w_norm; EwXXT = np.zeros((K, K));\n",
        "    for m in range(M): EwXXT += w_norm[m] * SecondMoments_a_array[m]\n",
        "    if not np.all(np.isfinite(EwX)): raise ValueError(\"calculate_Vw: EwX NaN/Inf.\")\n",
        "    if not np.all(np.isfinite(EwXXT)): raise ValueError(\"calculate_Vw: EwXXT NaN/Inf.\")\n",
        "    Vw = EwXXT - np.outer(EwX, EwX);\n",
        "    if not np.all(np.isfinite(Vw)): raise ValueError(\"calculate_Vw: Vw NaN/Inf.\")\n",
        "    Vw_psd = make_psd(Vw, psd_tolerance)\n",
        "    if not np.all(np.isfinite(Vw_psd)): raise ValueError(\"calculate_Vw: Vw_psd NaN/Inf.\")\n",
        "    return Vw_psd, EwX, EwXXT\n",
        "\n",
        "# --- calculate_H_gradient  ---\n",
        "def calculate_H_gradient(pi_star, w, R, SecondMoments_a_array, EwX, EwXXT, tolerance=1e-9, debug_print=False):\n",
        "    M = w.shape[0]; K = R.shape[0]; grad = np.zeros(M); norm_tolerance = 1e-12\n",
        "    if pi_star is None or not np.all(np.isfinite(pi_star)): return np.full(M, np.nan)\n",
        "    if not np.all(np.isfinite(w)): return np.full(M, np.nan)\n",
        "    if not np.all(np.isfinite(R)): return np.full(M, np.nan)\n",
        "    if not np.all(np.isfinite(SecondMoments_a_array)): return np.full(M, np.nan)\n",
        "    if EwX is None or not np.all(np.isfinite(EwX)): return np.full(M, np.nan)\n",
        "    if EwXXT is None or not np.all(np.isfinite(EwXXT)): return np.full(M, np.nan)\n",
        "    pi_norm = np.linalg.norm(pi_star)\n",
        "    if pi_norm < norm_tolerance:\n",
        "      if debug_print: print(f\"DEBUG grad_H: Ret NaN pi_norm {pi_norm:.2e}\"); return np.full(M, np.nan)\n",
        "    try:\n",
        "        pi_T_EwX = pi_star.T @ EwX\n",
        "        if not np.isfinite(pi_T_EwX): if debug_print: print(f\"DEBUG grad_H: pi_T_EwX NaN/Inf\"); return np.full(M, np.nan)\n",
        "        for j in range(M):\n",
        "            Sigma_j = SecondMoments_a_array[j]; r_j = R[:, j]\n",
        "            pi_T_Sigma_j_pi = pi_star.T @ Sigma_j @ pi_star\n",
        "            if not np.isfinite(pi_T_Sigma_j_pi): if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_Sigma_j_pi NaN/Inf\"); grad[j] = np.nan; continue\n",
        "            pi_T_r_j = pi_star.T @ r_j\n",
        "            if not np.isfinite(pi_T_r_j): if debug_print: print(f\"DEBUG grad_H (j={j}): pi_T_r_j NaN/Inf\"); grad[j] = np.nan; continue\n",
        "            term2 = 2 * pi_T_r_j * pi_T_EwX\n",
        "            if not np.isfinite(term2): if debug_print: print(f\"DEBUG grad_H (j={j}): term2 NaN/Inf\"); grad[j] = np.nan; continue\n",
        "            grad[j] = pi_T_Sigma_j_pi - term2\n",
        "            if not np.isfinite(grad[j]): if debug_print: print(f\"DEBUG grad_H (j={j}): final grad[{j}] NaN/Inf\")\n",
        "    except Exception as e_calc: print(f\"\\nERROR in calculate_H_gradient: {e_calc}\\n{traceback.format_exc()}\"); return np.full(M, np.nan)\n",
        "    if np.any(~np.isfinite(grad)): if debug_print: print(f\"DEBUG grad_H: Final check NaN/Inf: {grad}\"); return grad\n",
        "    return grad\n",
        "\n",
        "# --- project_to_simplex  ---\n",
        "def project_to_simplex(v, z=1):\n",
        "    n_features = v.shape[0];\n",
        "    if n_features == 0: return np.array([])\n",
        "    v_arr = np.asarray(v)\n",
        "    if not np.all(np.isfinite(v_arr)): warnings.warn(\"project_to_simplex: Input NaN/Inf.\"); return np.full(n_features, z / n_features) if n_features > 0 else np.array([])\n",
        "    if np.all(v_arr >= -1e-9) and np.isclose(np.sum(v_arr), z): return np.maximum(v_arr, 0)\n",
        "    u = np.sort(v_arr)[::-1]; cssv = np.cumsum(u) - z; ind = np.arange(n_features) + 1; cond = u - cssv / ind > 0\n",
        "    if np.any(cond): rho = ind[cond][-1]; theta = cssv[rho - 1] / float(rho); w = np.maximum(v_arr - theta, 0)\n",
        "    else: w = np.zeros(n_features);\n",
        "    if z > 0 and not np.any(cond): w[np.argmax(v_arr)] = z # Fallback if needed\n",
        "    w_sum = np.sum(w)\n",
        "    if not np.isclose(w_sum, z):\n",
        "        if w_sum > 1e-9: w = w * (z / w_sum)\n",
        "        elif z > 0 : w = np.zeros(n_features); w[np.argmax(v_arr)] = z\n",
        "    return np.maximum(w, 0)\n",
        "\n",
        "# --- frank_wolfe_optimizer  ---\n",
        "def frank_wolfe_optimizer(R_alpha, SecondMoments_alpha_array, mu_tilde, initial_w=None, max_outer_iter=250, fw_gap_tol=1e-7, inner_max_iter=350, tolerance=1e-9, psd_make_tolerance=1e-8, qp_regularization=1e-10, debug_print=False, force_iterations=0, return_history=False):\n",
        "    K, M = R_alpha.shape\n",
        "    if initial_w is None: w_k = np.ones(M) / M\n",
        "    else: w_k = project_to_simplex(np.copy(initial_w))\n",
        "    if w_k is None or not np.all(np.isfinite(w_k)): return (OptimizationResult(False, \"Initial w_k invalid\", w_opt=initial_w), []) if return_history else OptimizationResult(False, \"Initial w_k invalid\", w_opt=initial_w)\n",
        "    pi0, ok, p1msg = find_feasible_initial_pi(R_alpha, mu_tilde, K, tolerance)\n",
        "    if not ok: return (OptimizationResult(False, f\"Phase 1 failed: {p1msg}\", w_opt=w_k), []) if return_history else OptimizationResult(False, f\"Phase 1 failed: {p1msg}\", w_opt=w_k)\n",
        "    if pi0 is None or not np.all(np.isfinite(pi0)): return (OptimizationResult(False, \"Phase 1 invalid pi0\", w_opt=w_k), []) if return_history else OptimizationResult(False, \"Phase 1 invalid pi0\", w_opt=w_k)\n",
        "    best_w = np.copy(w_k); best_pi = None; best_lam = np.zeros(M); best_H = -float('inf')\n",
        "    best_grad_H = np.full(M, np.nan); best_active_set = None; best_fw_gap = float('inf')\n",
        "    last_successful_pi = np.copy(pi0); history = []; converged = False; k = 0; final_msg = \"\"\n",
        "    inner_solver_args_for_loop = {'max_iter': inner_max_iter, 'tolerance': tolerance, 'regularization_epsilon': qp_regularization}\n",
        "    for k in range(max_outer_iter):\n",
        "        iter_data = {'k': k + 1, 'w_k': np.copy(w_k)} if return_history else {}\n",
        "        try: Vk, Ex, ExxT = calculate_Vw(w_k, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "        except Exception as e: final_msg = f\"Outer iter {k+1}: Vw calc failed: {e}.\"; warnings.warn(final_msg); break\n",
        "        pi_init_inner = last_successful_pi if last_successful_pi is not None else pi0\n",
        "        if pi_init_inner is None or not np.all(np.isfinite(pi_init_inner)): pi_init_inner = pi0\n",
        "        if pi_init_inner is None or not np.all(np.isfinite(pi_init_inner)): final_msg = f\"Outer iter {k+1}: Invalid inner pi init.\"; break\n",
        "        pk, lk, act_idx_k, _, _, inner_ok, inner_msg = solve_inner_qp_active_set(Vk, R_alpha, mu_tilde, pi_init_inner, **inner_solver_args_for_loop)\n",
        "        if not inner_ok or pk is None or not np.all(np.isfinite(pk)):\n",
        "            tried_pi0_fallback = False\n",
        "            if pi_init_inner is not pi0:\n",
        "                 tried_pi0_fallback = True; warnings.warn(f\"Outer iter {k+1}: Inner QP failed ({inner_msg}). Retry w/ pi0.\")\n",
        "                 pk_pi0, lk_pi0, act_idx_k_pi0, _, _, inner_ok_pi0, inner_msg_pi0 = solve_inner_qp_active_set(Vk, R_alpha, mu_tilde, pi0, **inner_solver_args_for_loop)\n",
        "                 if inner_ok_pi0 and pk_pi0 is not None and np.all(np.isfinite(pk_pi0)): warnings.warn(f\"Outer iter {k+1}: Inner QP fallback OK.\"); pk = pk_pi0; lk = lk_pi0; act_idx_k = act_idx_k_pi0; inner_ok = True\n",
        "                 else: warnings.warn(f\"Outer iter {k+1}: Inner QP fallback failed ({inner_msg_pi0}).\"); inner_ok = False\n",
        "            if not inner_ok: final_msg = f\"Outer iter {k+1}: Inner QP failed persist ({inner_msg}).\"; break\n",
        "        last_successful_pi = np.copy(pk); Hk = pk.T @ Vk @ pk\n",
        "        current_pi = np.copy(pk); current_lam = lk if lk is not None and np.all(np.isfinite(lk)) else np.zeros(M)\n",
        "        current_active_set = tuple(sorted(act_idx_k)) if act_idx_k is not None else tuple(); current_H = Hk\n",
        "        try:\n",
        "            gHk = calculate_H_gradient(pk, w_k, R_alpha, SecondMoments_alpha_array, Ex, ExxT, tolerance, debug_print=debug_print)\n",
        "            if gHk is None or np.any(~np.isfinite(gHk)): raise ValueError(\"Grad calc None/NaN/Inf\")\n",
        "            current_grad_H = gHk\n",
        "        except Exception as e: final_msg = f\"Outer iter {k+1}: Grad calc failed: {e}.\"; break\n",
        "        if return_history: iter_data['H_k'] = Hk; iter_data['grad_H_k_norm'] = np.linalg.norm(current_grad_H); iter_data['pi_k'] = np.copy(pk); iter_data['lam_k'] = np.copy(current_lam); iter_data['active_set_k'] = current_active_set\n",
        "        if np.isfinite(Hk) and (best_pi is None or Hk > best_H + tolerance * 1e-1): best_H = Hk; best_w = np.copy(w_k); best_pi = np.copy(pk); best_lam = np.copy(current_lam); best_grad_H = np.copy(current_grad_H); best_active_set = current_active_set\n",
        "        grad_norm = np.linalg.norm(current_grad_H);\n",
        "        if grad_norm < tolerance * 100: sk = w_k; sk_idx = -1\n",
        "        else: sk_idx = np.argmax(current_grad_H); sk = np.zeros(M); sk[sk_idx] = 1.0\n",
        "        if return_history: iter_data['s_k_index'] = sk_idx\n",
        "        fw_gap = current_grad_H.T @ (w_k - sk); current_fw_gap = fw_gap\n",
        "        if return_history: iter_data['fw_gap'] = fw_gap\n",
        "        if best_pi is not None and np.isclose(Hk, best_H, atol=tolerance*1e-1, rtol=tolerance*1e-1) and np.isfinite(fw_gap): best_fw_gap = fw_gap\n",
        "        if k >= force_iterations and np.isfinite(fw_gap):\n",
        "            if abs(fw_gap) <= fw_gap_tol: converged = True; final_msg = f\"Converged (Gap {abs(fw_gap):.2e})\"; break\n",
        "        gamma = 2.0 / (k + 3.0)\n",
        "        if return_history: iter_data['gamma_k'] = gamma; history.append(iter_data)\n",
        "        w_k_next = (1.0 - gamma) * w_k + gamma * sk; w_k = project_to_simplex(w_k_next)\n",
        "        if w_k is None or not np.all(np.isfinite(w_k)): final_msg = f\"Outer iter {k+1}: w_k invalid after update.\"; break\n",
        "    final_iters = k + 1;\n",
        "    if not final_msg: final_msg = f\"Max Iter ({max_outer_iter}) reached\"\n",
        "    final_w = best_w; final_pi = best_pi; final_lam = best_lam; final_active_set = best_active_set\n",
        "    final_H = best_H; final_grad = best_grad_H; final_gap = best_fw_gap\n",
        "    if final_pi is not None and np.all(np.isfinite(final_w)):\n",
        "        try:\n",
        "            final_Vk, final_Ex, final_ExxT = calculate_Vw(final_w, R_alpha, SecondMoments_alpha_array, tolerance=tolerance, psd_tolerance=psd_make_tolerance)\n",
        "            pi_f, lam_f, act_f, _, _, ok_f, msg_f = solve_inner_qp_active_set(final_Vk, R_alpha, mu_tilde, pi0, **inner_solver_args_for_loop)\n",
        "            if ok_f and pi_f is not None and np.all(np.isfinite(pi_f)):\n",
        "                 final_pi = pi_f; final_lam = lam_f if lam_f is not None and np.all(np.isfinite(lam_f)) else np.zeros(M)\n",
        "                 final_active_set = tuple(sorted(act_f)) if act_f is not None else tuple(); final_H = final_pi.T @ final_Vk @ final_pi\n",
        "                 try:\n",
        "                     final_grad = calculate_H_gradient(final_pi, final_w, R_alpha, SecondMoments_alpha_array, final_Ex, final_ExxT, tolerance)\n",
        "                     if final_grad is None or np.any(~np.isfinite(final_grad)): final_grad = np.full(M, np.nan)\n",
        "                     if np.any(np.isfinite(final_grad)):\n",
        "                          grad_norm_f = np.linalg.norm(final_grad[np.isfinite(final_grad)])\n",
        "                          if grad_norm_f < tolerance * 100: sk_f = final_w\n",
        "                          else: sk_idx_f = np.nanargmax(np.nan_to_num(final_grad, nan=-np.inf)); sk_f = np.zeros(M); sk_f[sk_idx_f] = 1.0\n",
        "                          final_gap = final_grad.T @ (final_w - sk_f) if np.all(np.isfinite(final_grad)) else np.nan\n",
        "                     else: final_gap = np.nan\n",
        "                 except Exception as e_grad: warnings.warn(f\"Final grad calc failed: {e_grad}\"); final_grad = np.full(M, np.nan); final_gap = np.nan\n",
        "            else: warnings.warn(f\"Warning: Final inner QP solve failed ({msg_f}). Using best iter values.\")\n",
        "        except Exception as e_final: warnings.warn(f\"Error during final check: {e_final}. Using best iter values.\")\n",
        "    success_flag = best_pi is not None and np.isfinite(best_H)\n",
        "    if return_history and success_flag:\n",
        "         grad_norm_final = np.linalg.norm(final_grad[np.isfinite(final_grad)]) if np.any(np.isfinite(final_grad)) else np.nan\n",
        "         if grad_norm_final < tolerance * 100: sk_idx_final = -1\n",
        "         else: sk_idx_final = np.nanargmax(np.nan_to_num(final_grad, nan=-np.inf)) if np.any(np.isfinite(final_grad)) else -1\n",
        "         history.append({'k': final_iters + 1, 'w_k': np.copy(final_w), 'H_k': final_H, 'grad_H_k_norm': grad_norm_final, 's_k_index': sk_idx_final, 'fw_gap': final_gap, 'gamma_k': np.nan, 'pi_k': np.copy(final_pi), 'lam_k': np.copy(final_lam), 'active_set_k': final_active_set})\n",
        "    result = OptimizationResult(success_flag, final_msg, w_opt=final_w, pi_opt=final_pi, lambda_opt=final_lam, H_opt=final_H, grad_H_opt=final_grad, iterations=final_iters, fw_gap=final_gap, active_set_opt=final_active_set)\n",
        "    return (result, history) if return_history else result\n",
        "\n",
        "# --- generate_params_profile_switching_symmetric  ---\n",
        "# (s_factor_g, s_factor_b  kwargs )\n",
        "def generate_params_profile_switching_symmetric(alpha, alpha_max, K=5, M=3,\n",
        "                                      R_base_sym=np.array([0.02, 0.01, 0.0, -0.01, -0.02]),\n",
        "                                      sigma_base=np.array([0.18, 0.15, 0.20, 0.12, 0.10]),\n",
        "                                      Corr_base=np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]),\n",
        "                                      r_offset=0.03,\n",
        "                                      #  s_factor  kwargs  \n",
        "                                      s_factor_g = 0.001, # Default value if not provided\n",
        "                                      s_factor_b = -0.001,# Default value if not provided\n",
        "                                      # \n",
        "                                      sigma_min_epsilon=1e-4, psd_tolerance=1e-9):\n",
        "    \"\"\" Generates quasi-symmetric parameters with controllable differences in Vm \"\"\"\n",
        "    assert K == len(R_base_sym) and K == len(sigma_base) and K == Corr_base.shape[0] and M == 3, \"Dimension mismatch\"\n",
        "    R_neutral = R_base_sym; R_good = R_base_sym + r_offset; R_bad = R_base_sym - r_offset\n",
        "    Corr_neutral_psd = make_psd(Corr_base, psd_tolerance)\n",
        "    sigma_neutral_eff = np.maximum(sigma_min_epsilon, sigma_base)\n",
        "    V_neutral = np.diag(sigma_neutral_eff) @ Corr_neutral_psd @ np.diag(sigma_neutral_eff)\n",
        "    # Use s_factor_g, s_factor_b passed via kwargs\n",
        "    sigma_good_adj = np.maximum(sigma_min_epsilon, sigma_base * (1 + s_factor_g))\n",
        "    V_good = np.diag(sigma_good_adj) @ Corr_neutral_psd @ np.diag(sigma_good_adj)\n",
        "    sigma_bad_adj = np.maximum(sigma_min_epsilon, sigma_base * (1 + s_factor_b))\n",
        "    V_bad = np.diag(sigma_bad_adj) @ Corr_neutral_psd @ np.diag(sigma_bad_adj)\n",
        "    Sigma_neutral = V_neutral + np.outer(R_neutral, R_neutral)\n",
        "    Sigma_good = V_good + np.outer(R_good, R_good); Sigma_bad = V_bad + np.outer(R_bad, R_bad)\n",
        "    beta = np.clip(alpha / alpha_max if alpha_max > 0 else (1.0 if alpha > 0 else 0.0), 0.0, 1.0)\n",
        "    R_alpha = np.zeros((K, M)); SecondMoments_a_array = np.zeros((M, K, K))\n",
        "    R_alpha[:, 0] = (1 - beta) * R_good + beta * R_neutral\n",
        "    SecondMoments_a_array[0, :, :] = (1 - beta) * Sigma_good + beta * Sigma_neutral\n",
        "    R_alpha[:, 1] = (1 - beta) * R_bad + beta * R_good\n",
        "    SecondMoments_a_array[1, :, :] = (1 - beta) * Sigma_bad + beta * Sigma_good\n",
        "    R_alpha[:, 2] = (1 - beta) * R_neutral + beta * R_bad\n",
        "    SecondMoments_a_array[2, :, :] = (1 - beta) * Sigma_neutral + beta * Sigma_bad\n",
        "    if not np.all(np.isfinite(R_alpha)): raise ValueError(\"Gen R_alpha NaN/Inf.\")\n",
        "    if not np.all(np.isfinite(SecondMoments_a_array)): raise ValueError(\"Gen SecondMoments NaN/Inf.\")\n",
        "    return R_alpha, SecondMoments_a_array\n",
        "\n",
        "# === analyze_alpha_full_results  () ===\n",
        "def analyze_alpha_full_results(alpha_range, param_gen_kwargs, optimizer_kwargs, mu_tilde):\n",
        "    results_over_alpha = []\n",
        "    K = param_gen_kwargs.get('K', 5); M = param_gen_kwargs.get('M', 3)\n",
        "    if K is None or M is None: raise ValueError(\"K and M must be specified\")\n",
        "    print(f\"\\n--- Starting Full Results Analysis [{alpha_range[0]:.4f}, {alpha_range[-1]:.4f}] (mu={mu_tilde:.4f}) ---\")\n",
        "    total_alphas = len(alpha_range); start_loop_time = time.time(); successful_runs = 0; failed_alphas = []\n",
        "    for idx, alpha in enumerate(alpha_range):\n",
        "        print(f\"\\rAnalyzing alpha = {alpha:.6f} ({idx+1}/{total_alphas}) ... \", end=\"\")\n",
        "        alpha_result = {'alpha': alpha}; fw_success = False; fw_message = \"Not run\"; w_fw = np.full(M, np.nan)\n",
        "        H_star = np.nan; pi_opt = np.full(K, np.nan); grad_H = np.full(M, np.nan); lam_opt = np.full(M, np.nan)\n",
        "        act_set = tuple(); fw_gap = np.nan; iters = 0\n",
        "        try:\n",
        "            R_alpha, SecMoments = generate_params_profile_switching_symmetric(alpha, **param_gen_kwargs)\n",
        "            fw_result = frank_wolfe_optimizer(R_alpha, SecMoments, mu_tilde, initial_w=None, return_history=False, debug_print=False, **optimizer_kwargs)\n",
        "            fw_success = fw_result.success; fw_message = fw_result.message; iters = fw_result.iterations\n",
        "            w_fw = fw_result.w_opt if fw_result.w_opt is not None else np.full(M, np.nan)\n",
        "            H_star = fw_result.H_opt if fw_result.H_opt is not None else np.nan\n",
        "            pi_opt = fw_result.pi_opt if fw_result.pi_opt is not None else np.full(K, np.nan)\n",
        "            grad_H = fw_result.grad_H_opt if fw_result.grad_H_opt is not None else np.full(M, np.nan)\n",
        "            lam_opt = fw_result.lambda_opt if fw_result.lambda_opt is not None else np.full(M, np.nan)\n",
        "            act_set = fw_result.active_set_opt if fw_result.active_set_opt is not None else tuple()\n",
        "            fw_gap = fw_result.fw_gap if fw_result.fw_gap is not None else np.nan\n",
        "            if fw_success: successful_runs += 1\n",
        "            else: failed_alphas.append(alpha); print(f\"\\n  FW fail alpha={alpha:.6f}: {fw_result.message}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n  Error alpha={alpha:.6f}: {e}\"); fw_success = False; fw_message = f\"Exception: {e}\"; failed_alphas.append(alpha)\n",
        "        alpha_result['success'] = fw_success; alpha_result['message'] = fw_message; alpha_result['w_fw_default'] = w_fw\n",
        "        alpha_result['H_star'] = H_star; alpha_result['pi_opt'] = pi_opt; alpha_result['grad_H_opt'] = grad_H\n",
        "        alpha_result['lambda_opt'] = lam_opt; alpha_result['active_set_opt'] = act_set; alpha_result['fw_gap'] = fw_gap; alpha_result['iterations'] = iters\n",
        "        results_over_alpha.append(alpha_result)\n",
        "    end_loop_time = time.time(); print(f\"\\n--- Finished Analysis ({total_alphas} pts) --- Time: {end_loop_time - start_loop_time:.2f}s\")\n",
        "    print(f\"Successful runs: {successful_runs}/{total_alphas}\")\n",
        "    if failed_alphas: print(f\"Failed alphas ({len(failed_alphas)}): {failed_alphas[:5]}...\")\n",
        "    if not PANDAS_AVAILABLE: return results_over_alpha\n",
        "    df_results = pd.DataFrame(results_over_alpha)\n",
        "    if df_results.empty: return df_results\n",
        "    if 'w_fw_default' in df_results.columns:\n",
        "        w_vecs = np.stack(df_results['w_fw_default'].apply(lambda x: x if isinstance(x, np.ndarray) and x.shape == (M,) else np.full(M, np.nan)).values)\n",
        "        if np.any(np.isfinite(w_vecs)):\n",
        "            for m in range(M): df_results[f'w_fw_{m}'] = w_vecs[:, m]\n",
        "    if 'pi_opt' in df_results.columns:\n",
        "        pi_vecs = np.stack(df_results['pi_opt'].apply(lambda x: x if isinstance(x, np.ndarray) and x.shape == (K,) else np.full(K, np.nan)).values)\n",
        "        if np.any(np.isfinite(pi_vecs)):\n",
        "            for k in range(K): df_results[f'pi_{k}'] = pi_vecs[:, k]\n",
        "    if 'grad_H_opt' in df_results.columns:\n",
        "        grad_vecs = np.stack(df_results['grad_H_opt'].apply(lambda x: x if isinstance(x, np.ndarray) and x.shape == (M,) else np.full(M, np.nan)).values)\n",
        "        if np.any(np.isfinite(grad_vecs)):\n",
        "            for m in range(M): df_results[f'grad_H_{m}'] = grad_vecs[:, m]\n",
        "    if 'lambda_opt' in df_results.columns:\n",
        "        lambda_vecs = np.stack(df_results['lambda_opt'].apply(lambda x: x if isinstance(x, np.ndarray) and x.shape == (M,) else np.full(M, np.nan)).values)\n",
        "        if np.any(np.isfinite(lambda_vecs)):\n",
        "            for m in range(M): df_results[f'lambda_{m}'] = lambda_vecs[:, m]\n",
        "    if 'active_set_opt' in df_results.columns:\n",
        "        df_results['active_set'] = df_results['active_set_opt'].apply(lambda x: str(x) if x is not None else '()')\n",
        "    cols_to_drop = ['w_fw_default', 'pi_opt', 'grad_H_opt', 'lambda_opt', 'active_set_opt']\n",
        "    df_results = df_results.drop(columns=[col for col in cols_to_drop if col in df_results.columns], errors='ignore')\n",
        "    return df_results\n",
        "\n",
        "# ===  ===\n",
        "if __name__ == '__main__':\n",
        "    start_time_main = time.time()\n",
        "    warnings.filterwarnings('ignore', category=RuntimeWarning, message='invalid value encountered')\n",
        "    warnings.filterwarnings('ignore', category=UserWarning); warnings.filterwarnings('ignore', category=OptimizeWarning)\n",
        "\n",
        "    # ---  ---\n",
        "    K = 5; M = 3;\n",
        "    mu_tilde = 0.022 # \n",
        "    alpha_max = 1.5\n",
        "\n",
        "    #   () \n",
        "    param_gen_kwargs_quasi_symmetric_larger_diff = {\n",
        "        'K': K, 'M': M, 'alpha_max': alpha_max,\n",
        "        'R_base_sym': np.array([0.02, 0.01, 0.0, -0.01, -0.02]),\n",
        "        'sigma_base': np.array([0.18, 0.15, 0.20, 0.12, 0.10]),\n",
        "        'Corr_base': np.array([[1.0, 0.4, 0.3, 0.2, 0.1], [0.4, 1.0, 0.5, 0.3, 0.2], [0.3, 0.5, 1.0, 0.6, 0.4], [0.2, 0.3, 0.6, 1.0, 0.5], [0.1, 0.2, 0.4, 0.5, 1.0]]),\n",
        "        'r_offset': 0.03, # \n",
        "        #  Vm  \n",
        "        's_factor_g': 0.01,  # 0.001 -> 0.01\n",
        "        's_factor_b': -0.01, # -0.001 -> -0.01\n",
        "        # \n",
        "        'sigma_min_epsilon': 1e-4, 'psd_tolerance': 1e-9\n",
        "    }\n",
        "\n",
        "    #  ( 1e-9 )\n",
        "    solver_settings = {\n",
        "        'max_outer_iter': 500, 'fw_gap_tol': 1e-9, 'inner_max_iter': 600,\n",
        "        'tolerance': 1e-11, 'psd_make_tolerance': 1e-9,\n",
        "        'qp_regularization': 1e-9, # \n",
        "        'force_iterations': 50\n",
        "    }\n",
        "\n",
        "    # alpha  ()\n",
        "    alpha_start = 0.700; alpha_end = 0.800; num_alpha_steps = 101\n",
        "    alpha_range_analyze = np.linspace(alpha_start, alpha_end, num_alpha_steps)\n",
        "    unique_pt_tolerance = 1e-4\n",
        "\n",
        "    #  ()\n",
        "    s_factor_abs_int = abs(int(math.log10(param_gen_kwargs_quasi_symmetric_larger_diff['s_factor_g']))) if param_gen_kwargs_quasi_symmetric_larger_diff['s_factor_g'] != 0 else 0\n",
        "    output_csv_filename = f\"alpha_internal_sol_search_{alpha_start:.3f}_{alpha_end:.3f}_activeset_mu{int(mu_tilde*1000):03d}_diff{s_factor_abs_int}_reg9.csv\"\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"--- Searching Internal Solution [{alpha_start:.3f}, {alpha_end:.3f}] ---\")\n",
        "    print(f\"--- (mu={mu_tilde}, Larger Vm Diff: s_factor=+/-{param_gen_kwargs_quasi_symmetric_larger_diff['s_factor_g']:.2f}, Reg={solver_settings['qp_regularization']:.1e}) ---\")\n",
        "    print(f\"--- Output CSV: {output_csv_filename} ---\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # ---  ---\n",
        "    results_df = analyze_alpha_full_results(\n",
        "        alpha_range=alpha_range_analyze,\n",
        "        param_gen_kwargs=param_gen_kwargs_quasi_symmetric_larger_diff, # <-- \n",
        "        optimizer_kwargs=solver_settings,\n",
        "        mu_tilde=mu_tilde\n",
        "    )\n",
        "\n",
        "    # ---  & CSV & KKT ---\n",
        "    if PANDAS_AVAILABLE and isinstance(results_df, pd.DataFrame) and not results_df.empty:\n",
        "        print(\"\\n--- Analysis Results Summary (DataFrame) ---\")\n",
        "        cols_display = ['alpha', 'success', 'H_star', 'fw_gap', 'iterations', 'active_set']\n",
        "        cols_w = [f'w_fw_{m}' for m in range(M) if f'w_fw_{m}' in results_df.columns]\n",
        "        cols_grad = [f'grad_H_{m}' for m in range(M) if f'grad_H_{m}' in results_df.columns]\n",
        "        cols_to_show = cols_display + cols_w + cols_grad\n",
        "        cols_to_show = [col for col in cols_to_show if col in results_df.columns]\n",
        "        float_format_func = lambda x: f\"{x:.4e}\" if pd.notna(x) and isinstance(x, (float, np.number)) else x\n",
        "        pd.options.display.float_format = float_format_func\n",
        "        with pd.option_context('display.max_rows', 100, 'display.max_columns', None, 'display.width', 200):\n",
        "             print(results_df[cols_to_show].to_string(index=False, na_rep='NaN'))\n",
        "        pd.reset_option('display.float_format')\n",
        "\n",
        "        # --- CSV ---\n",
        "        try:\n",
        "            cols_pi = [f'pi_{k}' for k in range(K) if f'pi_{k}' in results_df.columns]\n",
        "            cols_lam = [f'lambda_{m}' for m in range(M) if f'lambda_{m}' in results_df.columns]\n",
        "            cols_csv_order = ['alpha', 'success', 'message', 'H_star', 'fw_gap', 'iterations', 'active_set'] + cols_w + cols_pi + cols_grad + cols_lam\n",
        "            cols_csv_order = [col for col in cols_csv_order if col in results_df.columns]\n",
        "            df_to_save = results_df[cols_csv_order]; df_to_save.to_csv(output_csv_filename, index=False, float_format='%.8e')\n",
        "            print(f\"\\nFull detailed results saved to: {os.path.abspath(output_csv_filename)}\")\n",
        "        except Exception as e: print(f\"\\nError saving CSV: {e}\")\n",
        "\n",
        "        # --- KKT ---\n",
        "        print(\"\\n--- Quick KKT Check (Stationarity w.r.t. w) ---\")\n",
        "        kkt_violations = []\n",
        "        check_tol = solver_settings.get('tolerance', 1e-9) * 1e4\n",
        "        for index, row in results_df.iterrows():\n",
        "            alpha = row['alpha']\n",
        "            if not row.get('success', False):\n",
        "                 kkt_violations.append({'alpha': alpha, 'active_set_kkt': 'N/A', 'max_violation': np.nan, 'grad_consistency_violation': np.nan, 'check_result': 'FW Fail'})\n",
        "                 continue\n",
        "            w_star = np.array([row.get(f'w_fw_{m}', np.nan) for m in range(M)])\n",
        "            grad_h = np.array([row.get(f'grad_H_{m}', np.nan) for m in range(M)])\n",
        "            if np.isnan(w_star).any() or np.isnan(grad_h).any() or np.isclose(np.sum(w_star), 0):\n",
        "                kkt_violations.append({'alpha': alpha, 'active_set_kkt': 'NaN', 'max_violation': np.nan, 'grad_consistency_violation': np.nan, 'check_result': 'Skipped (NaN)'})\n",
        "                continue\n",
        "            active_indices_kkt = set(m for m in range(M) if w_star[m] > unique_pt_tolerance)\n",
        "            grad_consistency_violation = 0.0\n",
        "            if len(active_indices_kkt) > 1:\n",
        "                 active_grads = grad_h[list(active_indices_kkt)]; valid_grads = active_grads[np.isfinite(active_grads)]\n",
        "                 if len(valid_grads) > 1: grad_consistency_violation = np.max(valid_grads) - np.min(valid_grads)\n",
        "                 else: grad_consistency_violation = 0.0 if len(valid_grads)==1 else np.nan\n",
        "            stationarity_violation = grad_consistency_violation if np.isfinite(grad_consistency_violation) else 0.0\n",
        "            nu_estimate = np.nan\n",
        "            if active_indices_kkt:\n",
        "                 valid_active_grads = grad_h[list(active_indices_kkt)]; valid_active_grads = valid_active_grads[np.isfinite(valid_active_grads)]\n",
        "                 if len(valid_active_grads) > 0: nu_estimate = np.max(valid_active_grads)\n",
        "            if not np.isfinite(nu_estimate): valid_grads = grad_h[np.isfinite(grad_h)];\n",
        "            if len(valid_grads)>0: nu_estimate = np.max(valid_grads)\n",
        "            if np.isfinite(nu_estimate):\n",
        "                for m in range(M):\n",
        "                    if m not in active_indices_kkt and np.isfinite(grad_h[m]):\n",
        "                        violation = grad_h[m] - nu_estimate\n",
        "                        if violation > check_tol: stationarity_violation = max(stationarity_violation, violation)\n",
        "            else: stationarity_violation = np.nan\n",
        "            is_internal = len(active_indices_kkt) == M; result_str = ''\n",
        "            if not np.isfinite(stationarity_violation): result_str = 'WARN (NaN KKT)'\n",
        "            elif stationarity_violation > check_tol * 10: result_str = 'WARN (KKT Viol)'\n",
        "            else: result_str = 'OK'\n",
        "            if is_internal: result_str += ' [INTERNAL]'\n",
        "            elif len(active_indices_kkt) == 2: result_str += ' [EDGE]'\n",
        "            elif len(active_indices_kkt) == 1: result_str += ' [VERTEX]'\n",
        "            elif len(active_indices_kkt) == 0: result_str += ' [EMPTY?]'\n",
        "            kkt_violations.append({'alpha': alpha, 'active_set_kkt': str(tuple(sorted(active_indices_kkt))), 'max_violation': stationarity_violation, 'grad_consistency_violation': grad_consistency_violation, 'check_result': result_str})\n",
        "        if PANDAS_AVAILABLE and kkt_violations:\n",
        "            df_kkt = pd.DataFrame(kkt_violations)\n",
        "            kkt_cols_order = ['alpha', 'active_set_kkt', 'max_violation', 'grad_consistency_violation', 'check_result']\n",
        "            kkt_cols_order = [col for col in kkt_cols_order if col in df_kkt.columns]\n",
        "            with pd.option_context('display.float_format', '{:.4e}'.format, 'display.max_rows', None): print(df_kkt[kkt_cols_order].to_string(index=False, na_rep='NaN'))\n",
        "            print(\"\\nNotes on KKT Check:\")\n",
        "            print(\" - active_set_kkt: Active set inferred from w* > tolerance\")\n",
        "            print(\" - max_violation: Max KKT violation\")\n",
        "            print(\" - grad_consistency_violation: max(active_grads) - min(active_grads)\")\n",
        "            print(\" - check_result: OK/WARN indicates KKT satisfaction; [...] solution type.\")\n",
        "        else: print(\"Could not perform KKT check or pandas not available.\")\n",
        "\n",
        "    elif isinstance(results_df, list) or (isinstance(results_df, pd.DataFrame) and results_df.empty):\n",
        "        print(\"\\n--- Analysis Results Summary ---\")\n",
        "        print(\"No results generated or DataFrame is empty.\")\n",
        "        if not PANDAS_AVAILABLE: print(\"(Pandas not available, skipping CSV export)\")\n",
        "\n",
        "    end_time_main = time.time()\n",
        "    print(f\"\\nTotal execution time: {end_time_main - start_time_main:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "lNik7O3R1hBV",
        "outputId": "d2cb53c5-e3ea-4a9d-9622-0a2bc08f57e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-7-262d1fa57b1d>, line 241)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-262d1fa57b1d>\"\u001b[0;36m, line \u001b[0;32m241\u001b[0m\n\u001b[0;31m    if pi_norm < norm_tolerance: if debug_print: print(f\"DEBUG grad_H: Ret NaN pi_norm {pi_norm:.2e}\"); return np.full(M, np.nan)\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zCF9iSdi63Fx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}